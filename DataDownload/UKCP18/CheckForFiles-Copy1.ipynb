{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8264a98",
   "metadata": {},
   "source": [
    "### Check for files and initialise download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4fb7293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp masher92@xfer1.jasmin.ac.uk:/gws/nopw/j04/icasp_swf/masher/bc013/bc013a.pr20180209.pp \"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_first30mins/2002_2020/bc013/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "            Access to this system is monitored and restricted to\n",
      "            authorised users.   If you do not have authorisation\n",
      "            to use  this system,  you should not  proceed beyond\n",
      "            this point and should disconnect immediately.\n",
      "\n",
      "            Unauthorised use could lead to prosecution.\n",
      "\n",
      "    (See also - http://www.stfc.ac.uk/aup)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded bc013a.pr20180209.pp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Configuration\n",
    "em_code = \"bc013\"  # Example EM code, replace with your actual value\n",
    "base_path = \"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_first30mins/2002_2020/\"\n",
    "remote_user = \"masher92\"\n",
    "remote_host = \"xfer1.jasmin.ac.uk\"\n",
    "remote_base_dir = \"/gws/nopw/j04/icasp_swf/masher/\"\n",
    "\n",
    "\n",
    "# Function to check file existence and download if absent\n",
    "def check_and_download(year, month, day):\n",
    "    file_pattern = f\"{em_code}a.pr{year}{month}{day}.pp\"\n",
    "    local_file_path = os.path.join(base_path, em_code, file_pattern)\n",
    "    remote_file_path = os.path.join(remote_base_dir, em_code, file_pattern)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(local_file_path):\n",
    "        #print(f\"{file_pattern}\")\n",
    "        print(f'scp masher92@xfer1.jasmin.ac.uk:/gws/nopw/j04/icasp_swf/masher/{em_code}/{file_pattern} \"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_first30mins/2002_2020/{em_code}/\"')\n",
    "        #print(f\"pp_file={file_pattern}\")\n",
    "        \n",
    "        # Construct the scp command\n",
    "        scp_command = [\n",
    "            \"scp\",\n",
    "            f\"{remote_user}@{remote_host}:{remote_file_path}\",\n",
    "            local_file_path\n",
    "        ]\n",
    "        # Call the scp command\n",
    "        try:\n",
    "            subprocess.run(scp_command, check=True)\n",
    "            print(f\"Successfully downloaded {file_pattern}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"Failed to download {file_pattern}\")\n",
    "    else:\n",
    "        #print(f\"File {file_pattern} already exists, skipping...\")\n",
    "        pass\n",
    "\n",
    "\n",
    "# Example usage for a specific year, month, and day\n",
    "for year in range(2001,2021):\n",
    "    for month in range(1,13):\n",
    "        month_str = f\"{month:02d}\"\n",
    "        for day in range(1, 31):  # Assuming you start from 16th to 30th\n",
    "            day_str = f\"{day:02d}\"  # Zero-padding the day\n",
    "            \n",
    "            check_and_download(year, month_str, day_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acc753",
   "metadata": {},
   "source": [
    "### Check for files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c67157",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'em_matching_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18925/437533723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for em_1hr in ['01', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '15']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mem_1hr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'12'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mem_30mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_matching_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mem_1hr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0myr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmonth_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'02'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'03'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'04'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'05'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'09'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'11'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'12'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'em_matching_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# for em_1hr in ['01', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '15']:\n",
    "for em_1hr in ['12']:\n",
    "    em_30mins = em_matching_dict[em_1hr]\n",
    "    for yr in range(2001,2021):\n",
    "        for month_num in ['01', '02', '03', '04', '05', '09', '10','11','12']:\n",
    "            print(f\"em{em_1hr}, yr {yr}, month {month_num}\")\n",
    "            if (os.path.isfile(f\"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/{resolution}/{em_30mins}/{yrs_range}/{em_30mins}a.pr{yr}{month_num}.nc\")):\n",
    "                print(\"already exists\")\n",
    "            else:\n",
    "                print(f\"Running for month {month_num} in {yr}, for {em_1hr} (which equatees to {em_30mins})\")\n",
    "\n",
    "                ####################################################### \n",
    "                #######################################################\n",
    "                ## Get one month of data - HOURLY\n",
    "                ####################################################### \n",
    "                #######################################################\n",
    "                ### Get a list of filenames for hourly data\n",
    "                general_filename_1hr = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_hourly/2.2km/{em_1hr}/{yrs_range}/pr_rcp85_land-cpm_uk_2.2km_{em_1hr}_1hr_{yr}{month_num}*'\n",
    "                filenames_1hr = []\n",
    "                for filename in glob.glob(general_filename_1hr):\n",
    "                        filenames_1hr.append(filename)\n",
    "                # If don't find any files matching this string in the 2001_2020 folder, then check the 1980_2001\n",
    "                if len(filenames_1hr) == 0:\n",
    "                    general_filename_1hr = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_hourly/2.2km/{em_1hr}/1980_2001/pr_rcp85_land-cpm_uk_2.2km_{em_1hr}_1hr_{yr}{month_num}*'\n",
    "                    for filename in glob.glob(general_filename_1hr):\n",
    "                            filenames_1hr.append(filename)\n",
    "                    print(len(filenames_1hr))\n",
    "\n",
    "                # ### Load in the data and remove the ensemble member dimension\n",
    "                monthly_cubes_list_1hr = iris.load(filenames_1hr)\n",
    "                cube_1hr = monthly_cubes_list_1hr[0]\n",
    "                cube_1hr = cube_1hr[0,:,:,:]\n",
    "\n",
    "                # ### Trim to Leeds\n",
    "                # cube_1hr = trim_to_bbox_of_region(cube_1hr, leeds_at_centre_gdf)\n",
    "\n",
    "                #######################################################\n",
    "                #######################################################\n",
    "                ## Get one month of data - 30mins\n",
    "                #######################################################\n",
    "                #######################################################\n",
    "                # ### Get all files for this ensemble member\n",
    "                general_filename_30mins = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_first30mins/{yrs_range}/{em_30mins}/{em_30mins}a.pr{yr}{month_num}*'\n",
    "                filenames_first30mins = []\n",
    "                for filename_30mins in glob.glob(general_filename_30mins):\n",
    "                    filenames_first30mins.append(filename_30mins)\n",
    "                filenames_first30mins.sort()\n",
    "\n",
    "                # ### Load in the data \n",
    "                monthly_cubes_list_30mins = iris.load(filenames_first30mins)\n",
    "\n",
    "                # Equalise\n",
    "                for cube in monthly_cubes_list_30mins:\n",
    "                    for attr in ['forecast_period', 'forecast_reference_time']:\n",
    "                        if attr in cube.attributes:\n",
    "                            del cube.attributes[attr]\n",
    "\n",
    "\n",
    "                monthly_cube_30mins = monthly_cubes_list_30mins.concatenate_cube()      \n",
    "\n",
    "                # ### Trim to be the same shape as the hourly data\n",
    "                # monthly_cube_30mins_1st = trim_to_bbox_of_region_30mins(monthly_cube_30mins, leeds_at_centre_gdf)\n",
    "                monthly_cube_30mins_1st = monthly_cube_30mins[:,24:-24,24:-24]\n",
    "\n",
    "                # ### Convert units of 30 mins data\n",
    "                # Check current units\n",
    "                # print(monthly_cube_30mins_1st.units)\n",
    "                # Set the units to those of the 1 hr cube\n",
    "                monthly_cube_30mins_1st.units = cube_1hr.units\n",
    "                # print(monthly_cube_30mins_1st.units)\n",
    "\n",
    "                # Convert the data to also be this unit\n",
    "                monthly_cube_30mins_1st_data = monthly_cube_30mins_1st.data\n",
    "                monthly_cube_30mins_1st_data = monthly_cube_30mins_1st_data*3600\n",
    "\n",
    "                monthly_cube_30mins_1st.data = monthly_cube_30mins_1st_data\n",
    "\n",
    "\n",
    "                #######################################################\n",
    "                #######################################################\n",
    "                ## Find the second half of the hour, using the first half of hour and hourly values\n",
    "                #######################################################\n",
    "                #######################################################\n",
    "                # get the hourly data\n",
    "                cube_1hr_data = cube_1hr.data\n",
    "                \n",
    "                if cube_1hr_data.shape != monthly_cube_30mins_1st_data.shape:\n",
    "                    print(\"DIFFERENT SIZEs\")\n",
    "                else:\n",
    "                \n",
    "                    # calculate value for second half of hour\n",
    "                    second_half_of_the_hour_mean_hourly_rainfall_rate_data = 2*cube_1hr_data-monthly_cube_30mins_1st_data\n",
    "                    # Create a new cube for the second half of the hour (start by copying the first half of hour cube)\n",
    "                    monthly_cube_30mins_2nd = monthly_cube_30mins_1st.copy()\n",
    "                    # Set values as calculated\n",
    "                    monthly_cube_30mins_2nd.data = second_half_of_the_hour_mean_hourly_rainfall_rate_data\n",
    "\n",
    "\n",
    "                    # ### Edit the times to be 30 mins later\n",
    "                    # get the times from the first half hour\n",
    "                    first_half_hour_times = monthly_cube_30mins_1st.coord('time').copy()\n",
    "                    # add 30 mins\n",
    "                    second_half_hour_times = first_half_hour_times + 0.5\n",
    "                    # for the second hald hour cube, remove the time dimension and then re-add the edited one\n",
    "                    monthly_cube_30mins_2nd.remove_coord('time')\n",
    "                    monthly_cube_30mins_2nd.add_dim_coord(second_half_hour_times, 0)\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    ## Join first half hour and second half hour into one cube\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    # ### Get a list of all the cubes in each of the monthly cubes\n",
    "                    list_30mins_1st = iris.cube.CubeList(monthly_cube_30mins_1st.slices_over('time'))\n",
    "                    list_30mins_2nd = iris.cube.CubeList(monthly_cube_30mins_2nd.slices_over('time'))\n",
    "                    list_30mins = list_30mins_1st +  list_30mins_2nd\n",
    "\n",
    "                    ### Merge back into one cube\n",
    "                    monthly_cube_30mins = list_30mins.merge_cube()\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    ## Save\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    dir_to_save = f\"datadir/UKCP18_every30mins/{resolution}/{em_30mins}/{yrs_range}/\"\n",
    "\n",
    "                    if os.path.isdir(dir_to_save):\n",
    "                        print(\"Exists\")\n",
    "                    else:\n",
    "                        print(\"Doesn't exist\")\n",
    "                        os.makedirs(dir_to_save)\n",
    "                    fp_to_save = f\"datadir/UKCP18_every30mins/{resolution}/{em_30mins}/{yrs_range}/{em_30mins}a.pr{yr}{month_num}.nc\" \n",
    "                    print(fp_to_save)\n",
    "                    iris.save(monthly_cube_30mins, fp_to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
