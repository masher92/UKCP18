{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0f753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_name = 'LinDyke' #LinDyke\n",
    "methods_key ='Observed'\n",
    "\n",
    "# Something in setting crop to to true for Wyke Beck doesn't work (the land cover and results files have different\n",
    "# values for out_meta so end up different sizes)\n",
    "if catchment_name =='WykeBeck':\n",
    "    crop_or_not = False\n",
    "elif catchment_name == 'LinDyke':\n",
    "    crop_or_not = True    \n",
    "\n",
    "region = '' # 'Kippax' #'' # 'Garforth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up link to model directory and read in catchment shapefile\n",
    "model_directory = '../../../FloodModelling/{}Models/Model_{}Profiles_export/'.format(catchment_name, methods_key)\n",
    "landcover_directory = '../../../FloodModelling/{}Models/LandCoverData/'.format(catchment_name)\n",
    "\n",
    "# Define whether to filter out values <0.1\n",
    "remove_little_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a0df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from my_functions import *\n",
    "\n",
    "# Specify strings relating to catchment\n",
    "if catchment_name == 'LinDyke':\n",
    "    catchment_name_str = \"Resampled.Terrain\" \n",
    "    catchment_gdf = gpd.read_file(model_directory + 'CatchmentLinDyke_exported.shp')\n",
    "    cell_size_in_m2 = 1\n",
    "elif catchment_name == 'WykeBeck':\n",
    "    catchment_name_str = \"Terrain.wykeDEM\" \n",
    "    cell_size_in_m2 = 4\n",
    "    catchment_gdf = gpd.read_file(model_directory + 'WykeBeckCatchment.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ad087",
   "metadata": {},
   "source": [
    "### Define the names of the method (in dictionary for different model runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc18ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {'Idealised': [ '6h_sp_c_0.5','6h_sp_fl_0.1', '6h_sp_fl_0.2', '6h_sp_fl_0.3', '6h_sp_fl_0.4',\n",
    "                    '6h_sp_bl_0.6','6h_sp_bl_0.7','6h_sp_bl_0.8','6h_sp_bl_0.9'],\n",
    "                'Observed':['6h_feh_singlepeak', '6h_c1','6h_c2','6h_c3','6h_c4', '6h_c5', '6h_c6','6h_c7',\n",
    "             '6h_c8','6h_c9','6h_c10', '6h_c11', '6h_c12','6h_c13','6h_c14','6h_c15'], \n",
    "               'SinglePeak_Scaled':['6h_sp_+0%','6h_sp_+5%','6h_sp_+10%','6h_sp_+15%','6h_sp_+20%']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2ae818",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = methods_dict[methods_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29431f3",
   "metadata": {},
   "source": [
    "### Get version of landcover array with just 'urban' and 'rural' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74109dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water landcover classification - 10 is water, 11 is eveyrthing else\n",
    "with rasterio.open(landcover_directory + 'LandCover_notwater_classification.tif', 'r') as ds:\n",
    "    landcover_notwater = ds.read()[0]\n",
    "    out_meta = ds.meta\n",
    "landcover_notwater_flat = landcover_notwater.flatten()\n",
    "\n",
    "# Urban landcover classification - 10 is urban, 1 is everything else\n",
    "with rasterio.open(landcover_directory + 'LandCover_urban_and_suburban_classification.tif', 'r') as ds:\n",
    "    landcover_urban = ds.read()[0]\n",
    "landcover_urban_flat = landcover_urban.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fabc1",
   "metadata": {},
   "source": [
    "### Find maximum intensity for each method and minute in which it occurs (to use in sorting results analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b24edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "min_of_maxs = []\n",
    "\n",
    "for method in methods:\n",
    "    if method == '6h_feh_singlepeak':\n",
    "        precip=pd.read_csv(\"../CreateSyntheticRainfallEvents/FEHProfiles/{}/6hr_100yrRP/PostLossRemoval/6hr_100yrRP_6.01h_1mintimestep.csv\".format(catchment_name))\n",
    "    elif method  == '6h_c12_copy':\n",
    "        method = '6h_c12'\n",
    "        precip=pd.read_csv(\"../CreateSyntheticRainfallEvents/{}Profiles/{}/6hr_100yrRP/PostLossRemoval/{}_urban.csv\".format(methods_key,catchment_name, method))\n",
    "    else:\n",
    "        precip=pd.read_csv(\"../CreateSyntheticRainfallEvents/{}Profiles/{}/6hr_100yrRP/PostLossRemoval/{}_urban.csv\".format(methods_key,catchment_name, method))\n",
    "    # Trim and add minutes column\n",
    "    precip = precip[0:360].copy()\n",
    "    precip['minute']=range(1,361)\n",
    "    # Add max and minutes of max\n",
    "    maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "    min_of_maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea22f7",
   "metadata": {},
   "source": [
    "### Create versions of lists of methods, in order based on max intensity and the the timing of the max intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0354d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ids_by_loading=  pd.DataFrame({\"min\": min_of_maxs, 'method_name': methods}).sort_values('min')[\"method_name\"].tolist()\n",
    "short_ids_by_intensity = pd.DataFrame({\"min\": maxs, 'method_name': methods}).sort_values('min', ascending = False)[\"method_name\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c5273",
   "metadata": {},
   "source": [
    "### Create dataframe of colours for each cluster (based on their loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39ef169",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Observed':\n",
    "    colours_df = create_colours_df_observed(short_ids_by_loading, methods)\n",
    "elif methods_key == 'Idealised':\n",
    "    colours_df = create_colours_df_idealised( short_ids_by_loading, methods)\n",
    "elif methods_key == 'SinglePeak_Scaled':\n",
    "    colours_df = create_colours_df_sp( short_ids_by_loading, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748b25b",
   "metadata": {},
   "source": [
    "### Create list of filepaths, formatted to be used for either depth or velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f847f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = []\n",
    "for method_num, short_id in enumerate(methods):\n",
    "    fp = model_directory + \"{}/{} (Max).{}.tif\".format(short_id, '{}', catchment_name_str)\n",
    "    fps.append(fp)\n",
    "if methods_key == 'Observed':\n",
    "    fps[0] = '../../../FloodModelling/{}Models/Model_FEHProfiles_export/6h_feh_singlepeak/{}/{} (Max).{}.tif'.format(catchment_name, region, '{}', catchment_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf33b1",
   "metadata": {},
   "source": [
    "# Hazard calculations\n",
    "The hazard is calculated based on definition here: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/842485/What-is-the-Risk-of-Flooding-from-Surface-Water-Map.pdf . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8ea2d",
   "metadata": {},
   "source": [
    "### Calculate the hazard categories for the single peak results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2326e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in depth and velocity rasters\n",
    "sp_depth, out_meta = open_and_clip_to_catchment(fps[0].format('Depth'), catchment_gdf, crop_or_not)\n",
    "sp_velocity, out_meta = open_and_clip_to_catchment(fps[0].format('Velocity'),  catchment_gdf, crop_or_not)\n",
    "\n",
    "if remove_little_values == True:\n",
    "    sp_depth = remove_little_values_fxn(sp_depth, fp.format('Depth'), catchment_gdf, crop_or_not) \n",
    "    sp_velocity = remove_little_values_fxn(sp_velocity, fp.format('Velocity'), catchment_gdf, crop_or_not)    \n",
    "    \n",
    "# Create a composite hazard array\n",
    "sp_hazard= np.where((sp_depth.flatten())<0.25, (sp_depth.flatten()* (sp_velocity.flatten()+0.5) + 0.5) , (sp_depth.flatten() * (sp_velocity.flatten()+0.5) + 1))\n",
    "sp_hazard = sp_hazard.reshape(sp_depth.shape)    \n",
    "\n",
    "# Classify this according to bins\n",
    "breaks_hazard = np.array([0, 0.5, 0.75, 1.25,2, 100])  \n",
    "sp_classified_hazard = classify_raster(sp_hazard, breaks_hazard)\n",
    "# Save to file\n",
    "fp_for_hazard = fps[0].replace('{} (Max).{}'.format({}, catchment_name_str),'{}_classified'.format('hazard') )\n",
    "save_array_as_raster(sp_classified_hazard, fp_for_hazard, out_meta) \n",
    "\n",
    "# Plot the classified hazard categories\n",
    "labels_hazard = ['Low hazard', 'Moderate hazard', 'Significant hazard', 'Extreme hazard']\n",
    "classified_colors_list_hazard = [mpl.cm.autumn_r(0.2),mpl.cm.autumn_r(0.5), mpl.cm.autumn_r(0.7),\"darkred\"]\n",
    "plot_classified_raster(fp_for_hazard, labels_hazard, classified_colors_list_hazard, catchment_gdf, catchment_name ,methods_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf0145",
   "metadata": {},
   "source": [
    "### Calculate the hazard categories for the other scenarios, and then find the difference in the categories between those and single peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973f4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c1/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c2/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c3/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c4/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c5/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c6/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c7/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c8/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c9/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c10/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c11/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c12/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c13/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c14/{} (Max).Terrain.wykeDEM.tif\n",
      "../../../FloodModelling/WykeBeckModels/Model_ObservedProfiles_export/6h_c15/{} (Max).Terrain.wykeDEM.tif\n"
     ]
    }
   ],
   "source": [
    "for fp in fps[1:]:\n",
    "    print(fp)\n",
    "\n",
    "    # Read in depth and velocity rasters\n",
    "    depth, out_meta = open_and_clip_to_catchment(fp.format('Depth'), catchment_gdf, crop_or_not)\n",
    "    velocity, out_meta = open_and_clip_to_catchment(fp.format('Velocity'),  catchment_gdf, crop_or_not)\n",
    "\n",
    "    if remove_little_values == True:\n",
    "        depth = remove_little_values_fxn(depth, fp.format('Depth'), catchment_gdf, crop_or_not) \n",
    "        velocity = remove_little_values_fxn(velocity, fp.format('Velocity'), catchment_gdf, crop_or_not)    \n",
    "\n",
    "    # Create a composite hazard array\n",
    "    hazard= np.where((depth.flatten())<0.25, (depth.flatten()* (velocity.flatten()+0.5) + 0.5) , (depth.flatten() * (velocity.flatten()+0.5) + 1))\n",
    "    hazard = hazard.reshape(depth.shape)    \n",
    "    \n",
    "    # Classify this according to bins\n",
    "    classified_hazard = classify_raster(hazard, breaks_hazard)\n",
    "    # Save to file\n",
    "    fp_for_hazard = fp.replace('{} (Max).{}'.format({}, catchment_name_str),'{}_classified'.format('hazard') )\n",
    "    save_array_as_raster(classified_hazard, fp_for_hazard, out_meta) \n",
    "    # Plot the classified hazard categories\n",
    "    labels_hazard = ['Low hazard', 'Moderate hazard', 'Significant hazard', 'Extreme hazard']\n",
    "    classified_colors_list_hazard = [mpl.cm.autumn_r(0.2),mpl.cm.autumn_r(0.5), mpl.cm.autumn_r(0.7)]\n",
    "    plot_classified_raster(fp_for_hazard, labels_hazard, classified_colors_list_hazard, catchment_gdf, catchment_name ,methods_key)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ec44",
   "metadata": {},
   "source": [
    "# <u> Flood extent </u>\n",
    "To examine whether the rainfall's temporal distribution influences the total extent of flooding, the number of flooded cells and the total flooded area in km2 (incl. only cells with depth >0.1m) is compared between the profile with a single peak, and the three methods for producing multi-peaked rainfall events. b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130fbb2",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area in each depth/velocity bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e3818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define breaks to split the depths/velocities on\n",
    "breaks_depths = np.array([0, 0.3, 0.6, 1.2, 100])  \n",
    "labels_depth = ['<=0.3m', '0.3-0.6m', '0.6-1.2m', '>1.2m']\n",
    "breaks_velocity = np.array([0,0.25,0.5,2,100])\n",
    "labels_velocity = [\"<=0.25m/s\", \"0.25-0.5m/s\", \"0.5-2m/s\", \">2m/s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af59db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "velocity_counts, velocity_props = create_binned_counts_and_props(methods, fps, '', 'Velocity',catchment_gdf, crop_or_not=crop_or_not,\n",
    "                                                                remove_little_values = remove_little_values)\n",
    "depth_counts, depth_props  = create_binned_counts_and_props(methods, fps, '', 'Depth',catchment_gdf, crop_or_not = crop_or_not,\n",
    "                                                           remove_little_values = remove_little_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d4f945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>6h_sp_c_0.5</th>\n",
       "      <th>6h_sp_fl_0.1</th>\n",
       "      <th>6h_sp_fl_0.2</th>\n",
       "      <th>6h_sp_fl_0.3</th>\n",
       "      <th>6h_sp_fl_0.4</th>\n",
       "      <th>6h_sp_bl_0.6</th>\n",
       "      <th>6h_sp_bl_0.7</th>\n",
       "      <th>6h_sp_bl_0.8</th>\n",
       "      <th>6h_sp_bl_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=0.3m</td>\n",
       "      <td>925412</td>\n",
       "      <td>878304</td>\n",
       "      <td>891635</td>\n",
       "      <td>897282</td>\n",
       "      <td>910998</td>\n",
       "      <td>937460</td>\n",
       "      <td>950085</td>\n",
       "      <td>958687</td>\n",
       "      <td>965775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3-0.6m</td>\n",
       "      <td>498684</td>\n",
       "      <td>486734</td>\n",
       "      <td>485465</td>\n",
       "      <td>492786</td>\n",
       "      <td>496035</td>\n",
       "      <td>500997</td>\n",
       "      <td>504992</td>\n",
       "      <td>509459</td>\n",
       "      <td>510995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6-1.2m</td>\n",
       "      <td>228410</td>\n",
       "      <td>208058</td>\n",
       "      <td>215755</td>\n",
       "      <td>219627</td>\n",
       "      <td>223882</td>\n",
       "      <td>232830</td>\n",
       "      <td>236482</td>\n",
       "      <td>237719</td>\n",
       "      <td>240640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;1.2m</td>\n",
       "      <td>64460</td>\n",
       "      <td>61001</td>\n",
       "      <td>61846</td>\n",
       "      <td>62547</td>\n",
       "      <td>63458</td>\n",
       "      <td>65270</td>\n",
       "      <td>66155</td>\n",
       "      <td>66878</td>\n",
       "      <td>67262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  6h_sp_c_0.5  6h_sp_fl_0.1  6h_sp_fl_0.2  6h_sp_fl_0.3  \\\n",
       "0    <=0.3m       925412        878304        891635        897282   \n",
       "1  0.3-0.6m       498684        486734        485465        492786   \n",
       "2  0.6-1.2m       228410        208058        215755        219627   \n",
       "3     >1.2m        64460         61001         61846         62547   \n",
       "\n",
       "   6h_sp_fl_0.4  6h_sp_bl_0.6  6h_sp_bl_0.7  6h_sp_bl_0.8  6h_sp_bl_0.9  \n",
       "0        910998        937460        950085        958687        965775  \n",
       "1        496035        500997        504992        509459        510995  \n",
       "2        223882        232830        236482        237719        240640  \n",
       "3         63458         65270         66155         66878         67262  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7241e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_urban, velocity_props_urban = create_binned_counts_and_props(methods, fps, True,'Velocity', catchment_gdf,\n",
    "                                                         crop_or_not, landcover_urban_flat,  remove_little_values = remove_little_values)\n",
    "depth_counts_urban, depth_props_urban= create_binned_counts_and_props(methods, fps, True,'Depth',  catchment_gdf, \n",
    "                                                      crop_or_not, landcover_urban_flat, remove_little_values = remove_little_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e6233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_notwater, velocity_props_notwater = create_binned_counts_and_props(methods, fps, True,'Velocity', catchment_gdf,\n",
    "                                      crop_or_not, landcover_notwater_flat, remove_little_values = remove_little_values)\n",
    "depth_counts_notwater, depth_props_notwater = create_binned_counts_and_props(methods, fps, True,'Depth',catchment_gdf, \n",
    "                                    crop_or_not, landcover_notwater_flat, remove_little_values = remove_little_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ca3a",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e7e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = create_totals_df(velocity_counts, cell_size_in_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d51a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df_urban = create_totals_df(velocity_counts_urban, cell_size_in_m2)  \n",
    "totals_df_notwater = create_totals_df(velocity_counts_notwater, cell_size_in_m2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cf9b7",
   "metadata": {},
   "source": [
    "### Create dataframes containing the % diff in the flooded area between single peak and each other method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd99a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Observed':\n",
    "    column_for_comparison = '6h_feh_singlepeak'\n",
    "elif methods_key == 'Idealised':\n",
    "    column_for_comparison ='6h_sp_c_0.5'    \n",
    "elif methods_key == 'SinglePeak_Scaled':\n",
    "    column_for_comparison ='6h_sp_+0%'        \n",
    "    \n",
    "percent_diffs_df = find_percentage_diff (methods, column_for_comparison, totals_df, fps) \n",
    "percent_diffs_df_urban = find_percentage_diff (methods, column_for_comparison, totals_df_urban, fps)\n",
    "percent_diffs_df_notwater = find_percentage_diff (methods, column_for_comparison, totals_df_notwater, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b76f8",
   "metadata": {},
   "source": [
    "## Find number of cells with each hazard rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8865c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_counts, hazard_props = create_binned_counts_and_props_hazard(methods, fps, '', catchment_name_str,catchment_gdf, \n",
    "                                                                    crop_or_not, landcover_data=False, remove_little_values =remove_little_values )\n",
    "hazard_counts_urban, hazard_props_urban = create_binned_counts_and_props_hazard(methods, fps, 'Urban', catchment_name_str,\n",
    "                                                                catchment_gdf, crop_or_not, landcover_urban, remove_little_values = remove_little_values)\n",
    "hazard_counts_notwater, hazard_props_notwater = create_binned_counts_and_props_hazard(methods, fps, 'Notwater', \n",
    "                                                    catchment_name_str,catchment_gdf, crop_or_not, landcover_notwater, remove_little_values = remove_little_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46068d8",
   "metadata": {},
   "source": [
    "### Create a dataframe containing all the info on each of the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884a8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = pd.DataFrame({'Cluster_num': methods, \"MaxRainfallIntensity\": maxs,  \n",
    "    \"MaxRainfallIntensityMinute\": min_of_maxs,\n",
    "    # All cells\n",
    "   'FloodedArea':totals_df['FloodedArea'],\n",
    "    '%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs'],\n",
    "    '%Diff_FloodedArea_fromSP_formatted':percent_diffs_df['percent_diff_formatted'],\n",
    "    'Abs%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs_abs'],\n",
    "    # Urban cells\n",
    " 'UrbanFloodedArea':totals_df_urban['FloodedArea'],\n",
    " '%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs'] ,\n",
    "  '%Diff_UrbanFloodedArea_fromSP_formatted':percent_diffs_df_urban['percent_diff_formatted'],\n",
    "   'Abs%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs_abs'], \n",
    "    # Not water cells\n",
    " 'NotwaterFloodedArea':totals_df_notwater['FloodedArea'],\n",
    " '%Diff_NotwaterFloodedArea_fromSP':percent_diffs_df_notwater['percent_diffs'] ,\n",
    "  '%Diff_NotwaterFloodedArea_fromSP_formatted':percent_diffs_df_notwater['percent_diff_formatted'],\n",
    "   'Abs%Diff_NotwaterFloodedArea_fromSP':percent_diffs_df_notwater['percent_diffs_abs'],                                          \n",
    "   #'WorstCaseDepth_ncells': worst_case_method_depth['counts'].tolist(),\n",
    "   # 'WorstCaseVelocity_ncells': worst_case_method_velocity['counts'].tolist(), \n",
    "                                'colour':colours_df['colour']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681011fa",
   "metadata": {},
   "source": [
    "### Add the depth/velocity category breakdowns and hazard categories to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d03d47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [velocity_props, depth_props,  velocity_props_urban, depth_props_urban,  velocity_props_notwater, depth_props_notwater,   \n",
    "       velocity_counts, depth_counts, velocity_counts_urban, depth_counts_urban, velocity_counts_notwater, depth_counts_notwater,  \n",
    "       hazard_counts, hazard_props, hazard_counts_urban, hazard_props_urban, hazard_counts_notwater, hazard_props_notwater, ]\n",
    "suffixes = ['_propcells', '_propcells', '_propcells_urban','_propcells_urban','_propcells_notwater','_propcells_notwater',\n",
    "            '_countcells','_countcells','_countcells_urban', '_countcells_urban','_countcells_notwater', '_countcells_notwater', '_countcells_notwater', '_countcells_notwater',\n",
    "            '_countcells', '_propcells',  '_countcells_urban', '_propcells_urban', '_countcells_notwater', '_propcells_notwater', ]\n",
    "\n",
    "for num, df in enumerate(dfs):\n",
    "    # Reformat the dataframe\n",
    "    df = df.set_index('index').T\n",
    "    # Add the correct suffix to the column names\n",
    "    df = df.add_suffix(suffixes[num]) \n",
    "    # Add Cluster_num column for joining\n",
    "    df['Cluster_num'] = df.index#\n",
    "    # Join to cluster results dataframe\n",
    "    cluster_results = pd.merge(cluster_results,  df, how=\"outer\", on = 'Cluster_num')\n",
    "    \n",
    "# cluster_results = pd.merge(cluster_results, hazard_cat_changes,  how=\"outer\", on = 'Cluster_num')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90adfff",
   "metadata": {},
   "source": [
    "### Finding proportion of area/urban area flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2975ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_results['%floodedarea_urban'] = round(cluster_results['UrbanFloodedArea']/cluster_results['FloodedArea']*100,2)\n",
    "# cluster_results['%_of_area_flooded'] =(cluster_results['FloodedArea']/29.589)*100\n",
    "# cluster_results['%_of_urban_area_flooded'] =(cluster_results['UrbanFloodedArea']/7.987)*100\n",
    "# # Add NAs for SP\n",
    "# cluster_results['%Diff_FloodedArea_fromSP_formatted']=cluster_results['%Diff_FloodedArea_fromSP_formatted'].fillna('')\n",
    "# cluster_results['%Diff_UrbanFloodedArea_fromSP_formatted']=cluster_results['%Diff_UrbanFloodedArea_fromSP_formatted'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30496b10",
   "metadata": {},
   "source": [
    "## Summarise the number of cells in different depth/velocity categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1d4c8",
   "metadata": {},
   "source": [
    "#### Get one dataframe containing the values for all methods, one row per cell per method \n",
    "Also including the water class variable in that cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d263c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_cells_value = produce_df_of_cell_by_cell_values(model_directory, catchment_gdf, catchment_name_str, methods, \n",
    "                                                     landcover_notwater_flat,landcover_urban_flat, crop_or_not,\n",
    "                                                    remove_little_values = remove_little_values)\n",
    "# rename for consistency\n",
    "each_cells_value['short_id'] = each_cells_value['short_id'].map({'6h_feh_singlepeak': 'FEH'}).fillna(each_cells_value['short_id'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b701b1",
   "metadata": {},
   "source": [
    "### Rename the profile names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Idealised':\n",
    "    cluster_results['Cluster_num']=['C', 'FL1', 'FL2', 'FL3', 'FL4','BL6', 'BL7', 'BL8','BL9']\n",
    "    ### Reorder to C in middle\n",
    "    cluster_results = cluster_results.reindex([1,2,3,4,0,5,6,7,8])\n",
    "    cluster_results.reset_index(inplace=True, drop=True)\n",
    "if methods_key == 'Observed':\n",
    "    methods = ['6h_feh_singlepeak','6h_c1','6h_c8','6h_c15','6h_c3','6h_c11','6h_c10','6h_c9','6h_c13','6h_c6',\n",
    "                 '6h_c2','6h_c12','6h_c14','6h_c4','6h_c7','6h_c5']\n",
    "    cluster_results = cluster_results.reindex(cluster_results['Cluster_num'].map(dict(zip(methods, range(len(methods))))).sort_values().index)\n",
    "    cluster_results.reset_index(inplace=True, drop=True)\n",
    "    cluster_results['Cluster_num'] = cluster_results['Cluster_num'].map({'6h_feh_singlepeak': 'FEH'}).fillna(cluster_results['Cluster_num'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d6402",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6facf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to the folder\n",
    "path = \"Outputs/Data/{}Profiles/{}/\".format(methods_key, catchment_name)\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "# Create a new directory because it does not exist\n",
    "if not isExist:\n",
    "    os.makedirs(path)\n",
    "# # Save\n",
    "cluster_results.to_csv(path + \"{}allclusters_summary_export.csv\".format(region), index=False)\n",
    "\n",
    "# Save\n",
    "each_cells_value.to_csv(path + \"{}individual_cell_values_export.csv\".format(region), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
