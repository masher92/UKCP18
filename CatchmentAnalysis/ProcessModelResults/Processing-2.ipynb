{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0f753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_name = 'WykeBeck' #LinDyke\n",
    "methods_key ='Observed'\n",
    "region = '' # 'Kippax' #'' # 'Garforth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ink to model directory and read in catchment shapefile\n",
    "model_directory = '../../../FloodModelling/{}Models/Model_{}Profiles/'.format(catchment_name, methods_key)\n",
    "landcover_directory = '../../../FloodModelling/{}Models/LandCoverData/'.format(catchment_name)\n",
    "\n",
    "# Define whether to filter out values <0.1\n",
    "remove_little_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a0df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from my_functions import *\n",
    "\n",
    "# Specify strings relating to catchment\n",
    "if catchment_name == 'LinDyke':\n",
    "    catchment_name_str = \"Resampled.Terrain\" \n",
    "    minx, miny, maxx, maxy = 437000,  426500,  445500, 434300\n",
    "    catchment_gdf = gpd.read_file(model_directory + 'CatchmentLinDyke_exported.shp')\n",
    "    cell_size_in_m2 = 1\n",
    "elif catchment_name == 'WykeBeck':\n",
    "    catchment_name_str = \"Terrain.wykeDEM\" \n",
    "    minx, miny, maxx, maxy = 430004,  429978, 438660, 440996 \n",
    "    cell_size_in_m2 = 4\n",
    "    catchment_gdf = gpd.read_file(model_directory + 'WykeBeckCatchment.shp')\n",
    "    \n",
    "# Create a bounding box (this is used in preparing the rasters)\n",
    "bbox = box(minx, miny, maxx, maxy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8283d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binned_counts_and_props(methods, fps, filter_by_land_cover, variable_name, bbox,catchment_gdf, \n",
    "                                   landcover_data=False, remove_little_values = True,):\n",
    "\n",
    "    if variable_name =='Depth':\n",
    "        breaks = np.array([0, 0.3, 0.6, 1.2, 100])  \n",
    "        labels = ['<=0.3m', '0.3-0.6m', '0.6-1.2m', '>1.2m']\n",
    "    elif variable_name =='Velocity':\n",
    "        breaks = np.array([0,0.25,0.5,2,100])\n",
    "        labels = [\"<=0.25m/s\", \"0.25-0.5m/s\", \"0.5-2m/s\", \">2m/s\"]\n",
    "        \n",
    "    # Create dataframes to populate with values\n",
    "    counts_df = pd.DataFrame()\n",
    "    proportions_df = pd.DataFrame()        \n",
    "\n",
    "    # Loop through each rainfall scenario\n",
    "    # Get the raster containing its values, and count the number of each unique value, and construct into a dataframe\n",
    "    for num, fp in enumerate(fps) :\n",
    "        # Classify depth/velocity rasters into depth/velocity bins\n",
    "        #raster = prepare_rainfall_scenario_raster(fp.format(variable_name), bbox, remove_little_values)[0]\n",
    "\n",
    "        with rasterio.open(fp.format(variable_name)) as src:\n",
    "            catchment_gdf=catchment_gdf.to_crs(src.crs)\n",
    "            # print(Vector.crs)\n",
    "            out_image, out_transform=mask(src,catchment_gdf.geometry,crop=True)\n",
    "            out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "            raster = out_image[0]\n",
    "            raster[raster == -9999.] = np.nan\n",
    "            \n",
    "            if remove_little_values == True:\n",
    "                if \"Depth\" in fp:\n",
    "                    raster = np.where(raster <0.1, np.nan, raster)    \n",
    "                else:\n",
    "                    with rasterio.open(fp.format('Depth')) as src:\n",
    "                        out_image, out_transform=mask(src,catchment_gdf.geometry,crop=True)\n",
    "                        out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "                        depth_raster = out_image[0]\n",
    "                        depth_raster[depth_raster == -9999.] = np.nan\n",
    "                        raster = np.where(depth_raster <0.1, np.nan, raster)            \n",
    "            \n",
    "        # If analysing all cells\n",
    "        if filter_by_land_cover == '':\n",
    "            unique, counts = np.unique(raster, return_counts=True)\n",
    "            df = pd.DataFrame({'values': unique, 'value':counts})\n",
    "\n",
    "            # Add a new column specifying the bin which each value falls within\n",
    "            df['bins']= pd.cut(unique, bins=breaks, right=False)\n",
    "\n",
    "        # If just analysing urban cells\n",
    "        elif filter_by_land_cover == True:\n",
    "            raster_and_landcover = pd.DataFrame({'landcovercategory':  landcover_data, 'value': raster.flatten()})\n",
    "            # Get just the relevant rows\n",
    "            df = raster_and_landcover[raster_and_landcover['landcovercategory']==10].copy()  \n",
    "            # Add a column assigning a bin based on the depth/velocity value\n",
    "            df['bins']= pd.cut(df['value'], bins=breaks, right=False)\n",
    "\n",
    "\n",
    "        # Create a new dataframe showing the number of cells in each of the bins\n",
    "        groups = df.groupby(['bins']).count()\n",
    "        groups  = groups.reset_index()\n",
    "        groups.rename(columns={\"value\": \"Count\"},inplace=True)\n",
    "\n",
    "        # Find the total number of cells\n",
    "        total_n_cells = groups['Count'].sum()\n",
    "        # Find the number of cells in each group as a proportion of the total\n",
    "        groups['Proportion'] = round((groups['Count']/total_n_cells) *100,1)\n",
    "\n",
    "        # Add values to dataframes\n",
    "        method_name = methods[num]\n",
    "        counts_df[method_name] = groups['Count']\n",
    "        proportions_df[method_name] = groups['Proportion']\n",
    "\n",
    "    # Reset index to show the groups\n",
    "    counts_df.reset_index(inplace=True)\n",
    "    proportions_df.reset_index(inplace=True)\n",
    "\n",
    "    # Set index values\n",
    "    counts_df['index'] = labels\n",
    "    proportions_df['index'] = labels\n",
    "    \n",
    "    return counts_df,proportions_df\n",
    "\n",
    "\n",
    "def create_binned_counts_and_props_hazard(methods, fps, filter_by_land_cover, catchment_name_str, catchment_gdf,bbox, landcover_data=False):\n",
    "\n",
    "    # Create dataframes to populate with values\n",
    "    counts_df = pd.DataFrame()\n",
    "    proportions_df = pd.DataFrame()      \n",
    "\n",
    "    for num, fp in enumerate(fps):\n",
    "        # Define filepath\n",
    "        fp = fp.replace('{} (Max).{}'.format({}, catchment_name_str),'hazard_classified')\n",
    "        # Read in data\n",
    "        #hazard = prepare_rainfall_scenario_raster(fp, bbox, remove_little_values)[0]\n",
    "        \n",
    "        with rasterio.open(fp) as src:\n",
    "            catchment_gdf=catchment_gdf.to_crs(src.crs)\n",
    "            # print(Vector.crs)\n",
    "            out_image, out_transform=mask(src,catchment_gdf.geometry,crop=True)\n",
    "            out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "            hazard = out_image[0]\n",
    "            hazard[hazard == -9999.] = np.nan\n",
    "                \n",
    "        # If fdiltering by land cover, then do additional stage of filtering out only cells in that category\n",
    "        if filter_by_land_cover != '':\n",
    "            # Get dataframe of hazard values, alongside land cover class\n",
    "            hazard_and_landcover = pd.DataFrame({'landcovercategory':  landcover_data.flatten(), 'counts': hazard.flatten()})\n",
    "            # Keep just the rows in the relevant landcoverclass\n",
    "            df = hazard_and_landcover[hazard_and_landcover['landcovercategory']==10].copy()  \n",
    "            # remove the NA values (i.e. where there is no flooding)\n",
    "            df=df[df.counts.notnull()]\n",
    "            # Convert the counts back into an array\n",
    "            hazard = np.array(df['counts'])\n",
    "       \n",
    "        # Count number of cells in each hazard category\n",
    "        unique, counts = np.unique(hazard, return_counts=True)\n",
    "        df = pd.DataFrame({'values': unique, 'counts':counts})\n",
    "        # Remove Nan values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Find the total number of cells\n",
    "        total_n_cells = df ['counts'].sum()\n",
    "        # Find the number of cells in each group as a proportion of the total\n",
    "        df['Proportion'] = round((df['counts']/total_n_cells) *100,1)\n",
    "        \n",
    "        # Add values to dataframes\n",
    "        method_name = methods[num]\n",
    "        counts_df[method_name] = df['counts']\n",
    "        proportions_df[method_name] = df['Proportion']\n",
    "\n",
    "    # Reset index to show the groups\n",
    "    counts_df.reset_index(inplace=True)\n",
    "    proportions_df.reset_index(inplace=True)\n",
    "\n",
    "    # Set index values\n",
    "    labels_hazard = ['Low hazard', 'Moderate hazard', 'Significant hazard', 'Extreme hazard']\n",
    "    counts_df['index'] = labels_hazard\n",
    "    proportions_df['index'] = labels_hazard\n",
    "    return counts_df, proportions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ad087",
   "metadata": {},
   "source": [
    "### Define the names of the method (in dictionary for different model runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc18ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {'Idealised': [ '6h_sp_c_0.5','6h_sp_fl_0.1', '6h_sp_fl_0.2', '6h_sp_fl_0.3', '6h_sp_fl_0.4',\n",
    "                    '6h_sp_bl_0.6','6h_sp_bl_0.7','6h_sp_bl_0.8','6h_sp_bl_0.9'],\n",
    "                'Observed':['6h_feh_singlepeak', '6h_c1','6h_c2','6h_c3','6h_c4', '6h_c5', '6h_c6','6h_c7',\n",
    "             '6h_c8','6h_c9','6h_c10', '6h_c11', '6h_c12','6h_c13','6h_c14','6h_c15'], \n",
    "               'SinglePeak_Scaled':['6h_sp_+0%','6h_sp_+5%','6h_sp_+10%','6h_sp_+15%','6h_sp_+20%']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2ae818",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = methods_dict[methods_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29431f3",
   "metadata": {},
   "source": [
    "### Get version of landcover array with just 'urban' and 'rural' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53046143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in the data\n",
    "# landcover, out_meta = prepare_rainfall_scenario_raster(model_directory + \"../LandCoverData/{}/LandCover_clipped.tif\".format(region), bbox, True)\n",
    "# # Convert the 1 and 6 values to 10 (for urban) and the rest to 11 (for non-urban).  \n",
    "# landcover_mod =  np.where(landcover==1, 10, landcover)\n",
    "# landcover_mod =  np.where(landcover_mod==6, 10, landcover_mod)\n",
    "# # Convert the rest of the classes to 11\n",
    "# for i in [1,2,3,4,5,7,8,9]:\n",
    "#     landcover_mod =  np.where(landcover_mod==i, 11, landcover_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1776302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban landcover classification - 10 is urban, 1 is everything else\n",
    "landcover_urban, out_meta = open_and_clip(landcover_directory + 'LandCover_urban_and_suburban_classification.tif', bbox)\n",
    "landcover_urban_flat = landcover_urban.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde2b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water landcover classification - 10 is water, 11 is eveyrthing else\n",
    "landcover_water, out_meta = open_and_clip(landcover_directory + 'LandCover_Freshwater_classification.tif', bbox)\n",
    "landcover_water_flat = landcover_water.flatten()\n",
    "\n",
    "# Water landcover classification - 10 is water, 11 is eveyrthing else\n",
    "landcover_notwater, out_meta = open_and_clip(landcover_directory + 'LandCover_notwater_classification.tif', bbox)\n",
    "landcover_notwater_flat = landcover_notwater.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_urban, out_meta = open_and_clip(landcover_directory + 'LandCover_Urban_classification.tif', bbox)\n",
    "# landcover_urban_flat = landcover_urban.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_suburban, out_meta = open_and_clip(landcover_directory + 'LandCover_SubUrban_classification.tif', bbox)\n",
    "# landcover_suburban_flat = landcover_suburban.flatten()\n",
    "\n",
    "# # Water landcover classification - 10 is water, 11 is eveyrthing else\n",
    "# landcover_arable, out_meta = open_and_clip(landcover_directory + 'LandCover_Arable_classification.tif', bbox)\n",
    "# landcover_arable_flat = landcover_arable.flatten()\n",
    "\n",
    "# # Water landcover classification - 10 is water, 11 is eveyrthing else\n",
    "# landcover_cg, out_meta = open_and_clip(landcover_directory + 'LandCover_CalcareousGrassland_classification.tif', bbox)\n",
    "# landcover_cg_flat = landcover_cg.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_ig, out_meta = open_and_clip(landcover_directory + 'LandCover_ImprovedGrassland_classification.tif', bbox)\n",
    "# landcover_ig_flat = landcover_ig.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_ng, out_meta = open_and_clip(landcover_directory + 'LandCover_NeutralGrassland_classification.tif', bbox)\n",
    "# landcover_ng_flat = landcover_ng.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_hg, out_meta = open_and_clip(landcover_directory + 'LandCover_HeatherGrassland_classification.tif', bbox)\n",
    "# landcover_hg_flat = landcover_hg.flatten()\n",
    "\n",
    "# # Urban landcover classification - 10 is urban, 1 is everything else\n",
    "# landcover_dw, out_meta = open_and_clip(landcover_directory + 'LandCover_DeciduousWoodland_classification.tif', bbox)\n",
    "# landcover_dw_flat = landcover_dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fabc1",
   "metadata": {},
   "source": [
    "### Find maximum intensity for each method and minute in which it occurs (to use in sorting results analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b24edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "min_of_maxs = []\n",
    "\n",
    "for method in methods:\n",
    "    if method == '6h_feh_singlepeak':\n",
    "        precip=pd.read_csv(\"../CreateSyntheticRainfallEvents/FEHProfiles/{}/6hr_100yrRP/PostLossRemoval/6hr_100yrRP_6.01h_1mintimestep.csv\".format(catchment_name))\n",
    "    else:\n",
    "        precip=pd.read_csv(\"../CreateSyntheticRainfallEvents/{}Profiles/{}/6hr_100yrRP/PostLossRemoval/{}_urban.csv\".format(methods_key,catchment_name, method))\n",
    "    # Trim and add minutes column\n",
    "    precip = precip[0:360].copy()\n",
    "    precip['minute']=range(1,361)\n",
    "    # Add max and minutes of max\n",
    "    maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "    min_of_maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea22f7",
   "metadata": {},
   "source": [
    "### Create versions of lists of methods, in order based on max intensity and the the timing of the max intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0354d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ids_by_loading=  pd.DataFrame({\"min\": min_of_maxs, 'method_name': methods}).sort_values('min')[\"method_name\"].tolist()\n",
    "short_ids_by_intensity = pd.DataFrame({\"min\": maxs, 'method_name': methods}).sort_values('min', ascending = False)[\"method_name\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c5273",
   "metadata": {},
   "source": [
    "### Create dataframe of colours for each cluster (based on their loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b39ef169",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Observed':\n",
    "    colours_df = create_colours_df_observed(short_ids_by_loading, methods)\n",
    "elif methods_key == 'Idealised':\n",
    "    colours_df = create_colours_df_idealised( short_ids_by_loading, methods)\n",
    "elif methods_key == 'SinglePeak_Scaled':\n",
    "    colours_df = create_colours_df_sp( short_ids_by_loading, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748b25b",
   "metadata": {},
   "source": [
    "### Create list of filepaths, formatted to be used for either depth or velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f847f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = []\n",
    "for method_num, short_id in enumerate(methods):\n",
    "    fp = model_directory + \"{}/{} (Max).{}.tif\".format(short_id, '{}', catchment_name_str)\n",
    "    fps.append(fp)\n",
    "if methods_key == 'Observed':\n",
    "    fps[0] = '../../../FloodModelling/{}Models/Model_FEHProfiles/6h_feh_singlepeak/{}/{} (Max).{}.tif'.format(catchment_name, region, '{}', catchment_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ec44",
   "metadata": {},
   "source": [
    "# <u> Flood extent </u>\n",
    "To examine whether the rainfall's temporal distribution influences the total extent of flooding, the number of flooded cells and the total flooded area in km2 (incl. only cells with depth >0.1m) is compared between the profile with a single peak, and the three methods for producing multi-peaked rainfall events. b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130fbb2",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area in each depth/velocity bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42e3818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define breaks to split the depths/velocities on\n",
    "breaks_depths = np.array([0, 0.3, 0.6, 1.2, 100])  \n",
    "labels_depth = ['<=0.3m', '0.3-0.6m', '0.6-1.2m', '>1.2m']\n",
    "breaks_velocity = np.array([0,0.25,0.5,2,100])\n",
    "labels_velocity = [\"<=0.25m/s\", \"0.25-0.5m/s\", \"0.5-2m/s\", \">2m/s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af59db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "velocity_counts, velocity_props = create_binned_counts_and_props(methods, fps, '', 'Velocity',bbox,catchment_gdf, )\n",
    "depth_counts, depth_props  = create_binned_counts_and_props(methods, fps, '', 'Depth',bbox,catchment_gdf, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7241e0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m velocity_counts_urban, velocity_props_urban \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_binned_counts_and_props\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVelocity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                                             \u001b[49m\u001b[43mcatchment_gdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlandcover_urban_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m depth_counts_urban, depth_props_urban\u001b[38;5;241m=\u001b[39m create_binned_counts_and_props(methods, fps, \u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepth\u001b[39m\u001b[38;5;124m'\u001b[39m,bbox, \n\u001b[1;32m      4\u001b[0m                                                                        catchment_gdf, landcover_urban_flat)\n",
      "Cell \u001b[0;32mIn [7], line 50\u001b[0m, in \u001b[0;36mcreate_binned_counts_and_props\u001b[0;34m(methods, fps, filter_by_land_cover, variable_name, bbox, catchment_gdf, landcover_data, remove_little_values)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# If just analysing urban cells\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filter_by_land_cover \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     raster_and_landcover \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlandcovercategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[43mlandcover_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Get just the relevant rows\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     df \u001b[38;5;241m=\u001b[39m raster_and_landcover[raster_and_landcover[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcovercategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()  \n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "velocity_counts_urban, velocity_props_urban = create_binned_counts_and_props(methods, fps, True,'Velocity',bbox, \n",
    "                                                                             catchment_gdf,landcover_urban_flat)\n",
    "depth_counts_urban, depth_props_urban= create_binned_counts_and_props(methods, fps, True,'Depth',bbox, \n",
    "                                                                       catchment_gdf, landcover_urban_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f917db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_suburban, velocity_props_suburban = create_binned_counts_and_props(methods, fps, True,'Velocity',bbox, \n",
    "                                                                             catchment_gdf,landcover_suburban_flat)\n",
    "depth_counts_suburban, depth_props_suburban= create_binned_counts_and_props(methods, fps, True,'Depth',bbox, \n",
    "                                                                       catchment_gdf, landcover_suburban_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_notwater, velocity_props_notwater = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_notwater_flat)\n",
    "depth_counts_notwater, depth_props_notwater = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_notwater_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_ng, velocity_props_ng = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_ng_flat)\n",
    "depth_counts_ng, depth_props_ng = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_ng_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_ig, velocity_props_ig = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_ig_flat)\n",
    "depth_counts_ig, depth_props_ig = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_ig_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fde87",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_cg, velocity_props_cg = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_cg_flat)\n",
    "depth_counts_cg, depth_props_cg = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_cg_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_hg, velocity_props_hg = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_hg_flat)\n",
    "depth_counts_hg, depth_props_hg = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_hg_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_dw, velocity_props_dw = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_dw_flat)\n",
    "depth_counts_dw, depth_props_dw = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_dw_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41233789",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_water, velocity_props_water = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_water_flat)\n",
    "depth_counts_water, depth_props_water = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_water_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts_arable, velocity_props_arable = create_binned_counts_and_props(methods, fps, True,'Velocity', bbox, \n",
    "                                                                                   catchment_gdf,landcover_arable_flat)\n",
    "depth_counts_arable, depth_props_arable = create_binned_counts_and_props(methods, fps, True,'Depth',bbox,catchment_gdf, \n",
    "                                                                             landcover_arable_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ca3a",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = create_totals_df(velocity_counts, cell_size_in_m2)\n",
    "totals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df_urban = create_totals_df(velocity_counts_urban, cell_size_in_m2)  \n",
    "totals_df_suburban = create_totals_df(velocity_counts_suburban, cell_size_in_m2)  \n",
    "totals_df_notwater = create_totals_df(velocity_counts_notwater, cell_size_in_m2)  \n",
    "totals_df_water = create_totals_df(velocity_counts_water, cell_size_in_m2)  \n",
    "totals_df_arable= create_totals_df(velocity_counts_arable, cell_size_in_m2)  \n",
    "totals_df_dw = create_totals_df(velocity_counts_dw, cell_size_in_m2)  \n",
    "totals_df_hg = create_totals_df(velocity_counts_hg, cell_size_in_m2)  \n",
    "totals_df_ig = create_totals_df(velocity_counts_ig, cell_size_in_m2)  \n",
    "totals_df_cg = create_totals_df(velocity_counts_cg, cell_size_in_m2)  \n",
    "totals_df_ng = create_totals_df(velocity_counts_ng, cell_size_in_m2)  \n",
    "\n",
    "totals_dfs = [totals_df_urban, totals_df_suburban, totals_df_water, totals_df_arable, totals_df_dw, totals_df_hg,\n",
    "             totals_df_hg, totals_df_ig, totals_df_cg, totals_df_ng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c773dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for totals_df in totals_dfs:\n",
    "    print(totals_df['FloodedArea'][0])\n",
    "    \n",
    "# totals_df_water['FloodedArea'][0] +totals_df_urban['FloodedArea'][0] +totals_df_suburban['FloodedArea'][0] + totals_df_arable['FloodedArea'][0]+ totals_df_dw['FloodedArea'][0]+totals_df_ig['FloodedArea'][0]+totals_df_cg['FloodedArea'][0]+totals_df_hg['FloodedArea'][0]+totals_df_ng['FloodedArea'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878309ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df_urban['FloodedArea'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cf9b7",
   "metadata": {},
   "source": [
    "### Create dataframes containing the % diff in the flooded area between single peak and each other method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Observed':\n",
    "    column_for_comparison = '6h_feh_singlepeak'\n",
    "elif methods_key == 'Idealised':\n",
    "    column_for_comparison ='6h_sp_c_0.5'    \n",
    "elif methods_key == 'SinglePeak_Scaled':\n",
    "    column_for_comparison ='6h_sp_+0%'        \n",
    "    \n",
    "percent_diffs_df = find_percentage_diff (methods, column_for_comparison, totals_df, fps) \n",
    "percent_diffs_df_urban = find_percentage_diff (methods, column_for_comparison, totals_df_urban, fps)\n",
    "percent_diffs_df_notwater = find_percentage_diff (methods, column_for_comparison, totals_df_notwater, fps)\n",
    "percent_diffs_df_water = find_percentage_diff (methods, column_for_comparison, totals_df_water, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change(current, previous):\n",
    "    if current == previous:\n",
    "        return 100.0\n",
    "    try:\n",
    "        return (abs(current - previous) / previous) * 100.0\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65632ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_percentage_diff (methods, reference_method_name, totals_df, fps):\n",
    "    percent_diffs_formatted_for_plot = []\n",
    "    percent_diffs_abs = []\n",
    "    percent_diffs = []\n",
    "\n",
    "    sp_value = totals_df.loc[totals_df['short_id'] == reference_method_name]['FloodedArea'].values[0]\n",
    "\n",
    "    for num, fp in enumerate(fps):\n",
    "        rainfall_scenario_name = methods[num]\n",
    "        if rainfall_scenario_name!= reference_method_name:\n",
    "            # FInd value for this scenario\n",
    "            this_scenario_value = totals_df.loc[totals_df['short_id'] == rainfall_scenario_name]['FloodedArea'].values[0]\n",
    "            # FInd % difference between single peak and this scenario\n",
    "            #percent_diffs.append(round((this_scenario_value/sp_value-1)*100,1)[0])\n",
    "            percent_diffs.append(get_change(this_scenario_value, sp_value))\n",
    "            #percent_diffs.append(round((this_scenario_value/sp_value-1)*100,1))\n",
    "            percent_diffs_abs.append(round(abs((this_scenario_value/sp_value-1))*100,1))\n",
    "            percent_diffs_formatted_for_plot.append(round((this_scenario_value/sp_value-1)*100,1))\n",
    "    # Convert values to strings, and add a + sign for positive values\n",
    "    # Include an empty entry for the single peak scenario\n",
    "    percent_diffs_df = pd.DataFrame({'percent_diff_formatted':[''] +['+' + str(round((list_item),2)) + '%' if list_item > 0 else str(round((list_item),2)) +\n",
    "     '%'  for list_item in percent_diffs_formatted_for_plot] ,\n",
    "             'percent_diffs':[0] + percent_diffs, 'percent_diffs_abs':[0] + percent_diffs_abs })\n",
    "    return percent_diffs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb2a6",
   "metadata": {},
   "source": [
    "## Find number of cells in which each method leads to the worst flooding (depth/velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48368b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the number of flooded cells with the worst flooding for each method\n",
    "# worst_case_method_depth = find_worst_case_method(fps, methods, 'Depth')\n",
    "# worst_case_method_velocity = find_worst_case_method(fps, methods,  'Velocity') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove multiple matches and nan\n",
    "# worst_case_method_depth = worst_case_method_depth[~worst_case_method_depth['values'].isin(['multiple matches','nan'])]\n",
    "# worst_case_method_velocity = worst_case_method_velocity[~worst_case_method_velocity['values'].isin(['multiple matches','nan'])]\n",
    "\n",
    "# # # Reorder (and also add in the methods that are missing)\n",
    "# worst_case_method_depth = pd.merge(worst_case_method_depth,  pd.DataFrame({'values': methods}), how=\"outer\")\n",
    "# worst_case_method_depth = worst_case_method_depth.reindex(worst_case_method_depth['values'].map(dict(zip(methods, range(len(methods))))).sort_values().index)\n",
    "# worst_case_method_depth.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# worst_case_method_velocity = pd.merge(worst_case_method_velocity,  pd.DataFrame({'values': methods}), how=\"outer\")\n",
    "# worst_case_method_velocity = worst_case_method_velocity.reindex(worst_case_method_velocity['values'].map(dict(zip(methods, range(len(methods))))).sort_values().index)\n",
    "# worst_case_method_velocity.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b76f8",
   "metadata": {},
   "source": [
    "## Find number of cells with each hazard rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_counts, hazard_props = create_binned_counts_and_props_hazard(methods, fps, '', catchment_name_str,catchment_gdf, bbox)\n",
    "hazard_counts_urban, hazard_props_urban = create_binned_counts_and_props_hazard(methods, fps, 'Urban', catchment_name_str,catchment_gdf, bbox, landcover_urban)\n",
    "hazard_counts_notwater, hazard_props_notwater = create_binned_counts_and_props_hazard(methods, fps, 'Notwater', catchment_name_str, catchment_gdf,bbox, landcover_notwater)\n",
    "hazard_counts_water, hazard_props_water = create_binned_counts_and_props_hazard(methods, fps, 'Water', catchment_name_str, catchment_gdf,bbox, landcover_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9253a1",
   "metadata": {},
   "source": [
    "## Find number of cells which have moved between hazard categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b165cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_cat_changes = create_binned_counts_and_props_hazard_cat_change(methods, fps, catchment_name_str, bbox)\n",
    "hazard_cat_changes = create_binned_counts_and_props_hazard_cat_change(methods, fps, catchment_name_str, bbox)\n",
    "hazard_cat_changes = create_binned_counts_and_props_hazard_cat_change(methods, fps, catchment_name_str, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46068d8",
   "metadata": {},
   "source": [
    "### Create a dataframe containing all the info on each of the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = pd.DataFrame({'Cluster_num': methods, \"MaxRainfallIntensity\": maxs,  \n",
    "    \"MaxRainfallIntensityMinute\": min_of_maxs,\n",
    "    # All cells\n",
    "   'FloodedArea':totals_df['FloodedArea'],\n",
    "    '%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs'],\n",
    "    '%Diff_FloodedArea_fromSP_formatted':percent_diffs_df['percent_diff_formatted'],\n",
    "    'Abs%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs_abs'],\n",
    "    # Urban cells\n",
    " 'UrbanFloodedArea':totals_df_urban['FloodedArea'],\n",
    " '%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs'] ,\n",
    "  '%Diff_UrbanFloodedArea_fromSP_formatted':percent_diffs_df_urban['percent_diff_formatted'],\n",
    "   'Abs%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs_abs'], \n",
    "    # Not water cells\n",
    " 'NotwaterFloodedArea':totals_df_notwater['FloodedArea'],\n",
    " '%Diff_NotwaterFloodedArea_fromSP':percent_diffs_df_notwater['percent_diffs'] ,\n",
    "  '%Diff_NotwaterFloodedArea_fromSP_formatted':percent_diffs_df_notwater['percent_diff_formatted'],\n",
    "   'Abs%Diff_NotwaterFloodedArea_fromSP':percent_diffs_df_notwater['percent_diffs_abs'],    \n",
    "    # Water cells\n",
    " 'WaterFloodedArea':totals_df_water['FloodedArea'],\n",
    " '%Diff_WaterFloodedArea_fromSP':percent_diffs_df_water['percent_diffs'] ,\n",
    "  '%Diff_WaterFloodedArea_fromSP_formatted':percent_diffs_df_water['percent_diff_formatted'],\n",
    "   'Abs%Diff_WaterFloodedArea_fromSP':percent_diffs_df_water['percent_diffs_abs'],                                        \n",
    "   #'WorstCaseDepth_ncells': worst_case_method_depth['counts'].tolist(),\n",
    "   # 'WorstCaseVelocity_ncells': worst_case_method_velocity['counts'].tolist(), \n",
    "                                'colour':colours_df['colour']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681011fa",
   "metadata": {},
   "source": [
    "### Add the depth/velocity category breakdowns and hazard categories to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [velocity_props, depth_props,  velocity_props_urban, depth_props_urban,  velocity_props_notwater, depth_props_notwater, velocity_props_water, depth_props_water, \n",
    "       velocity_counts, depth_counts, velocity_counts_urban, depth_counts_urban, velocity_counts_notwater, depth_counts_notwater, velocity_counts_water, depth_counts_water,\n",
    "       hazard_counts, hazard_props, hazard_counts_urban, hazard_props_urban, hazard_counts_notwater, hazard_props_notwater, hazard_counts_water, hazard_props_water]\n",
    "suffixes = ['_propcells', '_propcells', '_propcells_urban','_propcells_urban','_propcells_notwater','_propcells_notwater','_propcells_water','_propcells_water',\n",
    "            '_countcells','_countcells','_countcells_urban', '_countcells_urban','_countcells_notwater', '_countcells_notwater', '_countcells_notwater', '_countcells_notwater',\n",
    "            '_countcells', '_propcells',  '_countcells_urban', '_propcells_urban', '_countcells_notwater', '_propcells_notwater', '_countcells_water', '_propcells_water']\n",
    "\n",
    "for num, df in enumerate(dfs):\n",
    "    # Reformat the dataframe\n",
    "    df = df.set_index('index').T\n",
    "    # Add the correct suffix to the column names\n",
    "    df = df.add_suffix(suffixes[num]) \n",
    "    # Add Cluster_num column for joining\n",
    "    df['Cluster_num'] = df.index#\n",
    "    # Join to cluster results dataframe\n",
    "    cluster_results = pd.merge(cluster_results,  df, how=\"outer\", on = 'Cluster_num')\n",
    "    \n",
    "# cluster_results = pd.merge(cluster_results, hazard_cat_changes,  how=\"outer\", on = 'Cluster_num')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90adfff",
   "metadata": {},
   "source": [
    "### Finding proportion of area/urban area flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2975ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_results['%floodedarea_urban'] = round(cluster_results['UrbanFloodedArea']/cluster_results['FloodedArea']*100,2)\n",
    "# cluster_results['%_of_area_flooded'] =(cluster_results['FloodedArea']/29.589)*100\n",
    "# cluster_results['%_of_urban_area_flooded'] =(cluster_results['UrbanFloodedArea']/7.987)*100\n",
    "# # Add NAs for SP\n",
    "# cluster_results['%Diff_FloodedArea_fromSP_formatted']=cluster_results['%Diff_FloodedArea_fromSP_formatted'].fillna('')\n",
    "# cluster_results['%Diff_UrbanFloodedArea_fromSP_formatted']=cluster_results['%Diff_UrbanFloodedArea_fromSP_formatted'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30496b10",
   "metadata": {},
   "source": [
    "## Summarise the number of cells in different depth/velocity categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1d4c8",
   "metadata": {},
   "source": [
    "#### Get one dataframe containing the values for all methods, one row per cell per method \n",
    "Also including the water class variable in that cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each_cells_value = produce_df_of_cell_by_cell_values(model_directory, catchment_name_str, bbox, methods, landcover_water_flat, landcover_urban_flat)\n",
    "# # rename for consistency\n",
    "# each_cells_value['short_id'] = each_cells_value['short_id'].map({'6h_feh_singlepeak': 'FEH'}).fillna(each_cells_value['short_id'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b701b1",
   "metadata": {},
   "source": [
    "### Rename the profile names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_key == 'Idealised':\n",
    "    cluster_results['Cluster_num']=['C', 'FL1', 'FL2', 'FL3', 'FL4','BL6', 'BL7', 'BL8','BL9']\n",
    "    ### Reorder to C in middle\n",
    "    cluster_results = cluster_results.reindex([1,2,3,4,0,5,6,7,8])\n",
    "    cluster_results.reset_index(inplace=True, drop=True)\n",
    "if methods_key == 'Observed':\n",
    "    methods = ['6h_feh_singlepeak','6h_c1','6h_c8','6h_c15','6h_c3','6h_c11','6h_c10','6h_c9','6h_c13','6h_c6',\n",
    "                 '6h_c2','6h_c12','6h_c14','6h_c4','6h_c7','6h_c5']\n",
    "    cluster_results = cluster_results.reindex(cluster_results['Cluster_num'].map(dict(zip(methods, range(len(methods))))).sort_values().index)\n",
    "    cluster_results.reset_index(inplace=True, drop=True)\n",
    "    cluster_results['Cluster_num'] = cluster_results['Cluster_num'].map({'6h_feh_singlepeak': 'FEH'}).fillna(cluster_results['Cluster_num'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d6402",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6facf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to the folder\n",
    "path = \"Outputs/Data/{}Profiles/{}/\".format(methods_key, catchment_name)\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "# Create a new directory because it does not exist\n",
    "if not isExist:\n",
    "    os.makedirs(path)\n",
    "# Save\n",
    "cluster_results.to_csv(path + \"{}allclusters_summary.csv\".format(region), index=False)\n",
    "# cluster_results.to_csv(path + \"{}allclusters_summary_arable_dw_ig.csv\".format(region), index=False)\n",
    "# cluster_results.to_csv(path + \"{}allclusters_summary_ng_cg_hg.csv\".format(region), index=False)\n",
    "\n",
    "# Save\n",
    "# each_cells_value.to_csv(path + \"{}individual_cell_values.csv\".format(region), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cea586",
   "metadata": {},
   "source": [
    "### Delete tiff files (as these aren't used again and take up a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f688f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for method in short_ids:\n",
    "#     print(method)\n",
    "#     if method != '6h_feh_sp':\n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_cat_difference.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_classified.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_classified.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "        \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_classified.tif\".format(method)) \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_classified.tif\".format(method)) \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_classified.tif\".format(method)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24fdf0",
   "metadata": {},
   "source": [
    "### Cross checking results with QGIS (raster layer unique values report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = fps[3]\n",
    "# raster = prepare_rainfall_scenario_raster(fp.format('Depth'), bbox, remove_little_values)[0]\n",
    "# unique, counts = np.unique(raster, return_counts=True)\n",
    "# df = pd.DataFrame({'values': unique, 'counts':counts})\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
