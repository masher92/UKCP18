{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1491398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from my_functions_observedprofiles import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29431f3",
   "metadata": {},
   "source": [
    "### Get version of landcover array with just 'urban' and 'rural' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7c8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "landcover, out_meta = prepare_rainfall_scenario_raster(model_directory + \"LandCover_clipped.tif\", True)\n",
    "# Convert the 1 and 6 values to 10 (for urban) and the rest to 11 (for non-urban).  \n",
    "landcover_mod =  np.where(landcover==1, 10, landcover)\n",
    "landcover_mod =  np.where(landcover_mod==6, 10, landcover_mod)\n",
    "# Convert the rest of the classes to 11\n",
    "for i in [1,2,3,4,5,7,8,9]:\n",
    "    landcover_mod =  np.where(landcover_mod==i, 11, landcover_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ab40f",
   "metadata": {},
   "source": [
    "### Define the names of the methods (shorter and longer versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "276042a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ids = ['6h_feh_sp','6h_c1','6h_c2','6h_c3','6h_c4', '6h_c5', '6h_c6','6h_c7',\n",
    "            '6h_c8','6h_c9', '6h_c10', '6h_c11','6h_c12','6h_c13','6h_c14', '6h_c15']   \n",
    "methods = ['6h_single-peak', 'Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5', 'Cluster6', 'Cluster7', 'Cluster8',\n",
    "           'Cluster9','Cluster10', 'Cluster11',  'Cluster12','Cluster13', 'Cluster14', 'Cluster15']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fabc1",
   "metadata": {},
   "source": [
    "### Find maximum intensity for each method and minute in which it occurs (to use in sorting results analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b24edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "min_of_maxs = []\n",
    "\n",
    "# Add FEH data\n",
    "feh_precip=pd.read_csv(\"../../CreateSyntheticRainfallEvents/ReFH2_singlepeak/6hr_100yrRP/PostLossRemoval/6h_feh_singlepeak.csv\")\n",
    "maxs.append(feh_precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "min_of_maxs.append(feh_precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())\n",
    "\n",
    "#Add observed profile data\n",
    "for cluster_num in range(1,16):\n",
    "    precip=pd.read_csv(\"../../CreateSyntheticRainfallEvents/ObservedProfiles/6hr_100yrRP/PostLossRemoval/cluster{}_urban_summer.csv\".format(cluster_num))\n",
    "    maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "    min_of_maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea22f7",
   "metadata": {},
   "source": [
    "### Create versions of lists of methods, in order based on max intensity and the the timing of the max intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0354d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ids_by_loading=  pd.DataFrame({\"min\": min_of_maxs, 'method_name': short_ids}).sort_values('min')[\"method_name\"].tolist()\n",
    "short_ids_by_loading.remove('6h_feh_sp')\n",
    "short_ids_by_loading = ['6h_feh_sp']+short_ids_by_loading\n",
    "\n",
    "short_ids_by_intensity = pd.DataFrame({\"min\": maxs, 'method_name': short_ids}).sort_values('min', ascending = False)[\"method_name\"].tolist()\n",
    "short_ids_by_intensity.remove('6h_feh_sp')\n",
    "short_ids_by_intensity = ['6h_feh_sp']+short_ids_by_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fba7f1",
   "metadata": {},
   "source": [
    "### Create dataframe of colours for each cluster (based on their loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d90119a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours_df = create_colours_df(short_ids_by_loading, short_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748b25b",
   "metadata": {},
   "source": [
    "### Create list of filepaths, formatted to be used for either depth or velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f847f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = []\n",
    "for method_num, short_id in enumerate(short_ids):\n",
    "    fp = model_directory + \"{}/{} (Max).Resampled.Terrain.tif\".format(short_id, '{}')\n",
    "    fps.append(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa5f30",
   "metadata": {},
   "source": [
    "### Define breaks for categorising velocity and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3abe3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define breaks to split the depths/velocities on\n",
    "breaks_depths = np.array([0, 0.3, 0.6, 1.2, 100])  \n",
    "labels_depth = ['<=0.3m', '0.3-0.6m', '0.6-1.2m', '>1.2m']\n",
    "breaks_velocity = np.array([0,0.25,0.5,2,100])\n",
    "labels_velocity = [\"<=0.25m/s\", \"0.25-0.5m/s\", \"0.5-2m/s\", \">2m/s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ec44",
   "metadata": {},
   "source": [
    "# <u> Flood extent </u>\n",
    "To examine whether the rainfall's temporal distribution influences the total extent of flooding, the number of flooded cells and the total flooded area in km2 (incl. only cells with depth >0.1m) is compared between the profile with a single peak, and the three methods for producing multi-peaked rainfall events. b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130fbb2",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area in each depth/velocity bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af59db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts, velocity_props = create_binned_counts_and_props(fps, 'Velocity', breaks_velocity, labels_velocity, remove_little_values)\n",
    "depth_counts, depth_props = create_binned_counts_and_props(fps, 'Depth', breaks_depths, labels_depth, remove_little_values)\n",
    "\n",
    "velocity_counts_urban, velocity_props_urban = create_binned_counts_and_props_urban(fps, 'Velocity', breaks_velocity, labels_velocity, remove_little_values, landcover_mod)\n",
    "depth_counts_urban, depth_props_urban = create_binned_counts_and_props_urban(fps, 'Depth', breaks_depths, labels_depth, remove_little_values, landcover_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d8deb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_totals_df (velocity_counts):\n",
    "    totals_df =pd.DataFrame(velocity_counts.sum(numeric_only=True)).T\n",
    "    totals_df = totals_df.iloc[[len(totals_df)-1]]\n",
    "    # Convert this to the total flooded area for each method\n",
    "    totals_df_area = (totals_df * 1)/1000000\n",
    "    totals_df_area = totals_df_area.T\n",
    "    totals_df_area.reset_index(inplace=True)\n",
    "    totals_df_area.rename(columns={'index': 'short_id', 0: 'FloodedArea'}, inplace = True)\n",
    "    return totals_df_area\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ca3a",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d51a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = create_totals_df(velocity_counts)\n",
    "totals_df_urban = create_totals_df(velocity_counts_urban)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cf9b7",
   "metadata": {},
   "source": [
    "### Create dataframes containing the % diff in the flooded area between single peak and each other method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd99a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_diffs_df = find_percentage_diff (totals_df, fps) \n",
    "percent_diffs_df_urban = find_percentage_diff (totals_df_urban, fps)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb2a6",
   "metadata": {},
   "source": [
    "\n",
    "## Find number of cells in which each method leads to the worst flooding (depth/velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f48368b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of flooded cells with the worst flooding for each method\n",
    "worst_case_method_depth = find_worst_case_method(fps, short_ids, 'Depth')\n",
    "worst_case_method_velocity = find_worst_case_method(fps, short_ids,  'Velocity') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d31cf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple matches and nan\n",
    "worst_case_method_depth = worst_case_method_depth[~worst_case_method_depth['values'].isin(['multiple matches','nan'])]\n",
    "worst_case_method_velocity = worst_case_method_velocity[~worst_case_method_velocity['values'].isin(['multiple matches','nan'])]\n",
    "\n",
    "# # Reorder (and also add in the methods that are missing)\n",
    "worst_case_method_depth = pd.merge(worst_case_method_depth,  pd.DataFrame({'values': short_ids}), how=\"outer\")\n",
    "worst_case_method_depth = worst_case_method_depth.reindex(worst_case_method_depth['values'].map(dict(zip(short_ids, range(len(short_ids))))).sort_values().index)\n",
    "worst_case_method_depth.reset_index(inplace=True,drop=True)\n",
    "\n",
    "worst_case_method_velocity = pd.merge(worst_case_method_velocity,  pd.DataFrame({'values': short_ids}), how=\"outer\")\n",
    "worst_case_method_velocity = worst_case_method_velocity.reindex(worst_case_method_velocity['values'].map(dict(zip(short_ids, range(len(short_ids))))).sort_values().index)\n",
    "worst_case_method_velocity.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265f595",
   "metadata": {},
   "source": [
    "## Find number of cells with each hazard rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e6af974",
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "../../../../FloodModelling/MeganModel_New/6h_c1/hazard_classified.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:307\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:218\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../../../../FloodModelling/MeganModel_New/6h_c1/hazard_classified.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hazard_counts, hazard_props \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_binned_counts_and_props_hazard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/a319/gy17m2a/PhD/Scripts/CatchmentAnalysis/ProcessModelResults/ObservedProfiles/my_functions_observedprofiles.py:92\u001b[0m, in \u001b[0;36mcreate_binned_counts_and_props_hazard\u001b[0;34m(fps)\u001b[0m\n\u001b[1;32m     90\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../../FloodModelling/MeganModel_New/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/hazard_classified.tif\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fp\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Read in data\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m hazard \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_rainfall_scenario_raster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_little_values\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Count the number of each value\u001b[39;00m\n\u001b[1;32m     94\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(hazard, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/nfs/a319/gy17m2a/PhD/Scripts/CatchmentAnalysis/ProcessModelResults/ObservedProfiles/my_functions_observedprofiles.py:306\u001b[0m, in \u001b[0;36mprepare_rainfall_scenario_raster\u001b[0;34m(input_raster_fp, remove_little_values)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_rainfall_scenario_raster\u001b[39m(input_raster_fp, remove_little_values):\n\u001b[1;32m    303\u001b[0m     \n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# Clip the raster files to the extent of the catchment boundary\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Also return out_meta which contains..\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     raster, out_meta \u001b[38;5;241m=\u001b[39m \u001b[43mopen_and_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_raster_fp\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# If looking at velocity, then also read in depth raster as this is needed to filter out cells where \u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# the depth is below 0.1m   \u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# Set cell values to Null in cells which have a value <0.1 in the depth raster\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_little_values \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/nfs/a319/gy17m2a/PhD/Scripts/CatchmentAnalysis/ProcessModelResults/ObservedProfiles/my_functions_observedprofiles.py:264\u001b[0m, in \u001b[0;36mopen_and_clip\u001b[0;34m(input_raster_fp)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_and_clip\u001b[39m(input_raster_fp):\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Read in data as array\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mrasterio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_raster_fp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Create a bounding box \u001b[39;00m\n\u001b[1;32m    266\u001b[0m     minx, miny \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m437000\u001b[39m,  \u001b[38;5;241m426500\u001b[39m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/rasterio/env.py:444\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    441\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/rasterio/__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    307\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:309\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: ../../../../FloodModelling/MeganModel_New/6h_c1/hazard_classified.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "hazard_counts, hazard_props = create_binned_counts_and_props_hazard(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3adbe8",
   "metadata": {},
   "source": [
    "## Find number of cells which have moved between hazard categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f912be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_cat_changes = create_binned_counts_and_props_hazard_cat_change(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46068d8",
   "metadata": {},
   "source": [
    "# Create a dataframe containing all the info on each of the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = pd.DataFrame({'Cluster_num': short_ids, \"MaxRainfallIntensity\": maxs,  \n",
    "    \"MaxRainfallIntensityMinute\": min_of_maxs,\n",
    "   'TotalFloodedArea':totals_df['FloodedArea'],'%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs'],\n",
    "    '%Diff_FloodedArea_fromSP_formatted':percent_diffs_df['percent_diff_formatted'],\n",
    "    'Abs%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs_abs'],'UrbanFloodedArea':totals_df_urban['FloodedArea'],\n",
    "  '%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs'] ,\n",
    "  '%Diff_UrbanFloodedArea_fromSP_formatted':percent_diffs_df_urban['percent_diff_formatted'],\n",
    "    'Abs%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs_abs'], \n",
    "    'WorstCaseDepth_ncells': worst_case_method_depth['counts'].tolist(),\n",
    "    'WorstCaseVelocity_ncells': worst_case_method_velocity['counts'].tolist(), 'colour':colours_df['colour']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b829c6",
   "metadata": {},
   "source": [
    "### Add the depth/velocity category breakdowns and hazard categories to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d03d47e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hazard_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [velocity_props, depth_props, velocity_props_urban, depth_props_urban, velocity_counts, depth_counts,\n\u001b[0;32m----> 2\u001b[0m           velocity_counts_urban, depth_counts_urban,\u001b[43mhazard_counts\u001b[49m, hazard_props]\n\u001b[1;32m      3\u001b[0m suffixes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_propcells\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_propcells\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_propcells_urban\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_propcells_urban\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_countcells\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_countcells\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_countcells_urban\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_countcells_urban\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_numcells\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_propcells\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dfs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Reformat the dataframe\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hazard_counts' is not defined"
     ]
    }
   ],
   "source": [
    "dfs = [velocity_props, depth_props, velocity_props_urban, depth_props_urban, velocity_counts, depth_counts,\n",
    "          velocity_counts_urban, depth_counts_urban,hazard_counts, hazard_props]\n",
    "suffixes = ['_propcells', '_propcells','_propcells_urban','_propcells_urban','_countcells','_countcells','_countcells_urban','_countcells_urban',\n",
    "'_numcells', '_propcells']\n",
    "\n",
    "for num, df in enumerate(dfs):\n",
    "    # Reformat the dataframe\n",
    "    df = df.set_index('index').T\n",
    "    # Add the correct suffix to the column names\n",
    "    df = df.add_suffix(suffixes[num]) \n",
    "    # Add Cluster_num column for joining\n",
    "    df['Cluster_num'] = df.index#\n",
    "    # Join to cluster results dataframe\n",
    "    cluster_results = pd.merge(cluster_results,  df, how=\"outer\", on = 'Cluster_num')\n",
    "    \n",
    "cluster_results = pd.merge(cluster_results, hazard_cat_changes,  how=\"outer\", on = 'Cluster_num')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d6402",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6facf3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcluster_results\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/allclusters_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_results' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_results.to_csv(\"Data/allclusters_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bc321",
   "metadata": {},
   "source": [
    "### Delete tiff files (as these aren't used again and take up a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af47c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6h_feh_sp\n",
      "6h_c1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../FloodModelling/MeganModel_New/6h_c1/hazard_cat_difference.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Already removed all intermediarys for 6h_feh_sp in the SyntheticProfiles script\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6h_feh_sp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../../../FloodModelling/MeganModel_New/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/hazard_cat_difference.tif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../../FloodModelling/MeganModel_New/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/Depth_difffromsinglepeak_classified.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(method)) \n\u001b[1;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../../FloodModelling/MeganModel_New/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/Depth_difffromsinglepeak_posneg.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(method)) \n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../FloodModelling/MeganModel_New/6h_c1/hazard_cat_difference.tif'"
     ]
    }
   ],
   "source": [
    "for method in short_ids:\n",
    "    print(method)\n",
    "    # Already removed all intermediarys for 6h_feh_sp in the SyntheticProfiles script\n",
    "    if method != '6h_feh_sp':\n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_cat_difference.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_classified.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_classified.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "        \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_classified.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_classified.tif\".format(method)) \n",
    "        os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_classified.tif\".format(method)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
