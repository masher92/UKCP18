{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1491398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from my_functions_sp_scaled import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29431f3",
   "metadata": {},
   "source": [
    "### Get version of landcover array with just 'urban' and 'rural' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7c8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "landcover, out_meta = prepare_rainfall_scenario_raster(\"../../ProcessLandCoverData/LandCover_clipped.tif\", True)\n",
    "# Convert the 1 and 6 values to 10 (for urban) and the rest to 11 (for non-urban).  \n",
    "landcover_mod =  np.where(landcover==1, 10, landcover)\n",
    "landcover_mod =  np.where(landcover_mod==6, 10, landcover_mod)\n",
    "# Convert the rest of the classes to 11\n",
    "for i in [1,2,3,4,5,7,8,9]:\n",
    "    landcover_mod =  np.where(landcover_mod==i, 11, landcover_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ab40f",
   "metadata": {},
   "source": [
    "### Define the names of the method (shorter and longer versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276042a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_ids = ['6h_feh_sp', '6h_sp', '6h_sp_+10']   \n",
    "# methods = ['6h_feh_single-peak','6h_single-peak','6h_single-peak_+10%volume']  \n",
    "\n",
    "methods = ['6h_feh_sp','6h_sp_+0','6h_sp_+05','6h_sp_+10','6h_sp_+20']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fabc1",
   "metadata": {},
   "source": [
    "### Find maximum intensity for each method and minute in which it occurs (to use in sorting results analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b24edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "min_of_maxs = []\n",
    "\n",
    "# Add FEH data\n",
    "feh_precip=pd.read_csv(\"../../CreateSyntheticRainfallEvents/ReFH2_singlepeak/6hr_100yrRP/PostLossRemoval/6h_feh_singlepeak_urban.csv\")\n",
    "maxs.append(feh_precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "min_of_maxs.append(feh_precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())\n",
    "\n",
    "for method in methods[1:]:\n",
    "    precip=pd.read_csv(\"../../CreateSyntheticRainfallEvents/SinglePeak_Scaled/6hr_100yrRP/PostLossRemoval/{}_urban.csv\".format(method))\n",
    "    maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].max())\n",
    "    min_of_maxs.append(precip[\"Total net rain mm (Observed rainfall - 01/08/2022) - urbanised model\"].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea22f7",
   "metadata": {},
   "source": [
    "### Create versions of lists of methods, in order based on max intensity and the the timing of the max intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0354d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_ids_by_loading=  pd.DataFrame({\"min\": min_of_maxs, 'method_name': short_ids}).sort_values('min')[\"method_name\"].tolist()\n",
    "# short_ids_by_loading.remove('6h_feh_sp')\n",
    "# short_ids_by_loading = ['6h_feh_sp']+short_ids_by_loading\n",
    "\n",
    "# short_ids_by_intensity = pd.DataFrame({\"min\": maxs, 'method_name': short_ids}).sort_values('min', ascending = False)[\"method_name\"].tolist()\n",
    "# short_ids_by_intensity.remove('6h_feh_sp')\n",
    "# short_ids_by_intensity = ['6h_feh_sp']+short_ids_by_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cb827",
   "metadata": {},
   "source": [
    "### Create dataframe of colours for each cluster (based on their loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colours_df = create_colours_df(short_ids_by_loading, short_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748b25b",
   "metadata": {},
   "source": [
    "### Create list of filepaths, formatted to be used for either depth or velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f847f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = []\n",
    "for method_num, method in enumerate(methods):\n",
    "    if method == \"6h_feh_sp\":\n",
    "        model_directory = '../../../../FloodModelling/Model_FEH_profiles/'\n",
    "    elif method == \"6h_sp\":\n",
    "        model_directory = '../../../../FloodModelling/Model_SyntheticProfiles/'        \n",
    "    else:\n",
    "        model_directory = '../../../../FloodModelling/Model_SinglePeak_Scaled/'\n",
    "    \n",
    "    fp = model_directory + \"{}/{} (Max).Resampled.Terrain.tif\".format(method, '{}')\n",
    "    fps.append(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa5f30",
   "metadata": {},
   "source": [
    "### Define breaks for categorising velocity and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3abe3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define breaks to split the depths/velocities on\n",
    "breaks_depths = np.array([0, 0.3, 0.6, 1.2, 100])  \n",
    "labels_depth = ['<=0.3m', '0.3-0.6m', '0.6-1.2m', '>1.2m']\n",
    "breaks_velocity = np.array([0,0.25,0.5,2,100])\n",
    "labels_velocity = [\"<=0.25m/s\", \"0.25-0.5m/s\", \"0.5-2m/s\", \">2m/s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ec44",
   "metadata": {},
   "source": [
    "# <u> Flood extent </u>\n",
    "To examine whether the rainfall's temporal distribution influences the total extent of flooding, the number of flooded cells and the total flooded area in km2 (incl. only cells with depth >0.1m) is compared between the profile with a single peak, and the three methods for producing multi-peaked rainfall events. b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130fbb2",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area in each depth/velocity bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af59db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_counts, velocity_props = create_binned_counts_and_props(fps, 'Velocity', breaks_velocity, labels_velocity, remove_little_values)\n",
    "depth_counts, depth_props = create_binned_counts_and_props(fps, 'Depth', breaks_depths, labels_depth, remove_little_values)\n",
    "\n",
    "velocity_counts_urban, velocity_props_urban = create_binned_counts_and_props_urban(fps, 'Velocity', breaks_velocity, labels_velocity, remove_little_values, landcover_mod)\n",
    "depth_counts_urban, depth_props_urban = create_binned_counts_and_props_urban(fps, 'Depth', breaks_depths, labels_depth, remove_little_values, landcover_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ca3a",
   "metadata": {},
   "source": [
    "### Create dataframes containing the (total/urban) flooded area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d51a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_df = create_totals_df(velocity_counts)\n",
    "totals_df_urban = create_totals_df(velocity_counts_urban)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34d9a8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_id</th>\n",
       "      <th>FloodedArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6h_feh_sp</td>\n",
       "      <td>1.701144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6h_sp_+0</td>\n",
       "      <td>1.746234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6h_sp_+05</td>\n",
       "      <td>1.519598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6h_sp_+10</td>\n",
       "      <td>1.587015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6h_sp_+20</td>\n",
       "      <td>1.744572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    short_id  FloodedArea\n",
       "0  6h_feh_sp     1.701144\n",
       "1   6h_sp_+0     1.746234\n",
       "2  6h_sp_+05     1.519598\n",
       "3  6h_sp_+10     1.587015\n",
       "4  6h_sp_+20     1.744572"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cf9b7",
   "metadata": {},
   "source": [
    "### Create dataframes containing the % diff in the flooded area between single peak and each other method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87cb0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_diffs_df = find_percentage_diff (totals_df, fps) \n",
    "percent_diffs_df_urban = find_percentage_diff (totals_df_urban, fps)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb2a6",
   "metadata": {},
   "source": [
    "## Find number of cells in which each method leads to the worst flooding (depth/velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48368b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of flooded cells with the worst flooding for each method\n",
    "worst_case_method_depth = find_worst_case_method(fps, methods, 'Depth')\n",
    "worst_case_method_velocity = find_worst_case_method(fps, methods,  'Velocity') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple matches and nan\n",
    "worst_case_method_depth = worst_case_method_depth[~worst_case_method_depth['values'].isin(['multiple matches','nan'])]\n",
    "worst_case_method_velocity = worst_case_method_velocity[~worst_case_method_velocity['values'].isin(['multiple matches','nan'])]\n",
    "\n",
    "# # Reorder (and also add in the methods that are missing)\n",
    "worst_case_method_depth = pd.merge(worst_case_method_depth,  pd.DataFrame({'values': short_ids}), how=\"outer\")\n",
    "worst_case_method_depth = worst_case_method_depth.reindex(worst_case_method_depth['values'].map(dict(zip(short_ids, range(len(short_ids))))).sort_values().index)\n",
    "worst_case_method_depth.reset_index(inplace=True,drop=True)\n",
    "\n",
    "worst_case_method_velocity = pd.merge(worst_case_method_velocity,  pd.DataFrame({'values': short_ids}), how=\"outer\")\n",
    "worst_case_method_velocity = worst_case_method_velocity.reindex(worst_case_method_velocity['values'].map(dict(zip(short_ids, range(len(short_ids))))).sort_values().index)\n",
    "worst_case_method_velocity.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b76f8",
   "metadata": {},
   "source": [
    "## Find number of cells with each hazard rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8f41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binned_counts_and_props(fps, variable_name, breaks, labels, remove_little_values):\n",
    "    # Create dataframes to populate with values\n",
    "    counts_df = pd.DataFrame()\n",
    "    proportions_df = pd.DataFrame()        \n",
    "\n",
    "    # Loop through each rainfall scenario\n",
    "    # Get the raster containing its values, and count the number of each unique value, and construct into a dataframe\n",
    "    for fp in fps  :\n",
    "        # Classify depth/velocity rasters into depth/velocity bins\n",
    "        raster = prepare_rainfall_scenario_raster(fp.format(variable_name), remove_little_values)[0]\n",
    "        unique, counts = np.unique(raster, return_counts=True)\n",
    "        df = pd.DataFrame({'values': unique, 'counts':counts})\n",
    "\n",
    "        # Add a new column specifying the bin which each value falls within\n",
    "        df['bins']= pd.cut(unique, bins=breaks, right=False)\n",
    "\n",
    "        # Create a new dataframe showing the number of cells in each of the bins\n",
    "        groups = df.groupby(['bins']).sum()\n",
    "        groups  = groups.reset_index()\n",
    "\n",
    "        # Find the total number of cells\n",
    "        total_n_cells = groups ['counts'].sum()\n",
    "        # Find the number of cells in each group as a proportion of the total\n",
    "        groups['Proportion'] = round((groups['counts']/total_n_cells) *100,1)\n",
    "\n",
    "        # Add values to dataframes\n",
    "        method_name = fp.split(\"/\")[6]\n",
    "        counts_df[method_name] = groups['counts']\n",
    "        proportions_df[method_name] = groups['Proportion']\n",
    "\n",
    "    # Reset index to show the groups\n",
    "    counts_df.reset_index(inplace=True)\n",
    "    proportions_df.reset_index(inplace=True)\n",
    "\n",
    "    # Set index values\n",
    "    counts_df['index'] = labels\n",
    "    proportions_df['index'] = labels\n",
    "\n",
    "    return counts_df, proportions_df\n",
    "\n",
    "def create_binned_counts_and_props_hazard(fps):\n",
    "\n",
    "    # Create dataframes to populate with values\n",
    "    counts_df = pd.DataFrame()\n",
    "    proportions_df = pd.DataFrame()      \n",
    "\n",
    "    for fp in fps:\n",
    "        # Define filepath\n",
    "        fp = fp.replace('{} (Max).Resampled.Terrain', 'hazard_classified')\n",
    "        # Read in data\n",
    "        hazard = prepare_rainfall_scenario_raster(fp, remove_little_values)[0]\n",
    "        # Count the number of each value\n",
    "        unique, counts = np.unique(hazard, return_counts=True)\n",
    "        df = pd.DataFrame({'values': unique, 'counts':counts})\n",
    "        # Remove Nan values\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Find the total number of cells\n",
    "        total_n_cells = df ['counts'].sum()\n",
    "        # Find the number of cells in each group as a proportion of the total\n",
    "        df['Proportion'] = round((df['counts']/total_n_cells) *100,1)\n",
    "        \n",
    "        # Add values to dataframes\n",
    "        method_name = fp.split(\"/\")[6]\n",
    "        counts_df[method_name] = df['counts']\n",
    "        proportions_df[method_name] = df['Proportion']\n",
    "\n",
    "    # Reset index to show the groups\n",
    "    counts_df.reset_index(inplace=True)\n",
    "    proportions_df.reset_index(inplace=True)\n",
    "\n",
    "    # Set index values\n",
    "    labels_hazard = ['Low hazard', 'Moderate hazard', 'Significant hazard', 'Extreme hazard']\n",
    "    counts_df['index'] = labels_hazard\n",
    "    proportions_df['index'] = labels_hazard\n",
    "    return counts_df, proportions_df\n",
    "\n",
    "def create_binned_counts_and_props_hazard_cat_change(fps):\n",
    "    \n",
    "    replacement_dict = {-3.0: 'Hazard_3CatsLower', -2.0 : 'Hazard_2CatsLower', -1.0 : 'Hazard_1CatsLower', 0: 'Hazard_SameCat',\n",
    "        1 : 'Hazard_1CatsHigher', 2: 'Hazard_2CatsHigher', 3: 'Hazard_3CatsHigher'}\n",
    "    \n",
    "    # Create dataframes to populate with values\n",
    "    counts_df = pd.DataFrame(columns = [\"values\"])\n",
    "    proportions_df = pd.DataFrame(columns = [\"values\"]) \n",
    "\n",
    "    for fp in fps[1:]:\n",
    "        # Add values to dataframes\n",
    "        method_name = fp.split(\"/\")[6]\n",
    "        # Read in hazard data \n",
    "        fp = fp.replace('{} (Max).Resampled.Terrain', 'hazard_cat_difference')\n",
    "        hazard = prepare_rainfall_scenario_raster(fp, False)[0]\n",
    "        unique, counts = np.unique(hazard, return_counts=True)\n",
    "        df = pd.DataFrame({'values': unique, method_name:counts})\n",
    "        # Remove NA columns\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Add to dataframes\n",
    "        counts_df= counts_df.merge(df[['values', method_name]], on = 'values', how = 'outer')\n",
    "\n",
    "        # Find the total number of cells\n",
    "        total_n_cells = df [method_name].sum()\n",
    "        # Find the number of cells in each group as a proportion of the total\n",
    "        df[method_name] = round((df[method_name]/total_n_cells) *100,1)\n",
    "\n",
    "       # Add to dataframes\n",
    "        proportions_df= proportions_df.merge(df[['values', method_name]], on = 'values', how = 'outer')\n",
    "\n",
    "        # Order intoi ascending order\n",
    "        proportions_df = proportions_df.sort_values(by='values')\n",
    "        counts_df = counts_df.sort_values(by='values')\n",
    "\n",
    "    # Join the two dataframes together and reformat\n",
    "    both_dfs = pd.DataFrame(columns = [\"Cluster_num\"])  \n",
    "    for num, df in enumerate([counts_df,proportions_df]):\n",
    "        df=df.replace({\"values\": replacement_dict})\n",
    "        df.rename(columns={'values': 'Cluster_num'}, inplace=True)\n",
    "        df = df.set_index('Cluster_num').T\n",
    "        if num == 0:\n",
    "            df = df.add_suffix('_countcells')\n",
    "        else:\n",
    "            df = df.add_suffix('_propcells')\n",
    "        df['Cluster_num'] = df.index\n",
    "        both_dfs = pd.merge(both_dfs, df,  how=\"outer\", on = 'Cluster_num')\n",
    "    \n",
    "    return both_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9253a1",
   "metadata": {},
   "source": [
    "## Find number of cells which have moved between hazard categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b165cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hazard_3CatsLower_countcells</th>\n",
       "      <th>Hazard_2CatsLower_countcells</th>\n",
       "      <th>Hazard_1CatsLower_countcells</th>\n",
       "      <th>Hazard_SameCat_countcells</th>\n",
       "      <th>Hazard_1CatsHigher_countcells</th>\n",
       "      <th>Hazard_2CatsHigher_countcells</th>\n",
       "      <th>Hazard_3CatsHigher_countcells</th>\n",
       "      <th>Cluster_num</th>\n",
       "      <th>Hazard_3CatsLower_propcells</th>\n",
       "      <th>Hazard_2CatsLower_propcells</th>\n",
       "      <th>Hazard_1CatsLower_propcells</th>\n",
       "      <th>Hazard_SameCat_propcells</th>\n",
       "      <th>Hazard_1CatsHigher_propcells</th>\n",
       "      <th>Hazard_2CatsHigher_propcells</th>\n",
       "      <th>Hazard_3CatsHigher_propcells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>24810.0</td>\n",
       "      <td>1675916.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6h_sp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>98.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>593.0</td>\n",
       "      <td>7578.0</td>\n",
       "      <td>104398.0</td>\n",
       "      <td>1136855.0</td>\n",
       "      <td>104585.0</td>\n",
       "      <td>28877.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>6h_sp_+5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>82.2</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474.0</td>\n",
       "      <td>13036.0</td>\n",
       "      <td>154260.0</td>\n",
       "      <td>1114926.0</td>\n",
       "      <td>91963.0</td>\n",
       "      <td>27653.0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>6h_sp_+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>79.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>708.0</td>\n",
       "      <td>32443.0</td>\n",
       "      <td>257114.0</td>\n",
       "      <td>1032552.0</td>\n",
       "      <td>80828.0</td>\n",
       "      <td>26031.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>6h_sp_+20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hazard_3CatsLower_countcells  Hazard_2CatsLower_countcells  \\\n",
       "0                           NaN                          88.0   \n",
       "1                         593.0                        7578.0   \n",
       "2                         474.0                       13036.0   \n",
       "3                         708.0                       32443.0   \n",
       "\n",
       "   Hazard_1CatsLower_countcells  Hazard_SameCat_countcells  \\\n",
       "0                       24810.0                  1675916.0   \n",
       "1                      104398.0                  1136855.0   \n",
       "2                      154260.0                  1114926.0   \n",
       "3                      257114.0                  1032552.0   \n",
       "\n",
       "   Hazard_1CatsHigher_countcells  Hazard_2CatsHigher_countcells  \\\n",
       "0                          110.0                            4.0   \n",
       "1                       104585.0                        28877.0   \n",
       "2                        91963.0                        27653.0   \n",
       "3                        80828.0                        26031.0   \n",
       "\n",
       "   Hazard_3CatsHigher_countcells Cluster_num  Hazard_3CatsLower_propcells  \\\n",
       "0                            NaN       6h_sp                          NaN   \n",
       "1                          882.0    6h_sp_+5                          0.0   \n",
       "2                          872.0   6h_sp_+10                          0.0   \n",
       "3                          870.0   6h_sp_+20                          0.0   \n",
       "\n",
       "   Hazard_2CatsLower_propcells  Hazard_1CatsLower_propcells  \\\n",
       "0                          0.0                          1.5   \n",
       "1                          0.5                          7.5   \n",
       "2                          0.9                         11.0   \n",
       "3                          2.3                         18.0   \n",
       "\n",
       "   Hazard_SameCat_propcells  Hazard_1CatsHigher_propcells  \\\n",
       "0                      98.5                           0.0   \n",
       "1                      82.2                           7.6   \n",
       "2                      79.5                           6.6   \n",
       "3                      72.2                           5.7   \n",
       "\n",
       "   Hazard_2CatsHigher_propcells  Hazard_3CatsHigher_propcells  \n",
       "0                           0.0                           NaN  \n",
       "1                           2.1                           0.1  \n",
       "2                           2.0                           0.1  \n",
       "3                           1.8                           0.1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard_cat_changes = create_binned_counts_and_props_hazard_cat_change(fps)\n",
    "hazard_cat_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46068d8",
   "metadata": {},
   "source": [
    "### Create a dataframe containing all the info on each of the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7a197a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6h_feh_sp', '6h_sp', '6h_sp_+5', '6h_sp_+10', '6h_sp_+20']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "884a8b57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cluster_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCluster_num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMaxRainfallIntensity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMaxRainfallIntensityMinute\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_of_maxs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#    'TotalFloodedArea':totals_df['FloodedArea'],'%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs'],\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     '%Diff_FloodedArea_fromSP_formatted':percent_diffs_df['percent_diff_formatted'],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     'Abs%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs_abs'],'UrbanFloodedArea':totals_df_urban['FloodedArea'],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     'WorstCaseDepth_ncells': worst_case_method_depth['counts'].tolist(),\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     'WorstCaseVelocity_ncells': worst_case_method_velocity['counts'].tolist()}) \u001b[39;00m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/pygeospatial/lib/python3.9/site-packages/pandas/core/internals/construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "cluster_results = pd.DataFrame({'Cluster_num': methods, \"MaxRainfallIntensity\": maxs,  \n",
    "    \"MaxRainfallIntensityMinute\": min_of_maxs})\n",
    "#    'TotalFloodedArea':totals_df['FloodedArea'],'%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs'],\n",
    "#     '%Diff_FloodedArea_fromSP_formatted':percent_diffs_df['percent_diff_formatted'],\n",
    "#     'Abs%Diff_FloodedArea_fromSP':percent_diffs_df['percent_diffs_abs'],'UrbanFloodedArea':totals_df_urban['FloodedArea'],\n",
    "#   '%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs'] ,\n",
    "#   '%Diff_UrbanFloodedArea_fromSP_formatted':percent_diffs_df_urban['percent_diff_formatted'],\n",
    "#     'Abs%Diff_UrbanFloodedArea_fromSP':percent_diffs_df_urban['percent_diffs_abs'], \n",
    "#     'WorstCaseDepth_ncells': worst_case_method_depth['counts'].tolist(),\n",
    "#     'WorstCaseVelocity_ncells': worst_case_method_velocity['counts'].tolist()}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681011fa",
   "metadata": {},
   "source": [
    "### Add the depth/velocity category breakdowns and hazard categories to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [velocity_props, depth_props, velocity_props_urban, depth_props_urban, velocity_counts, depth_counts,\n",
    "          velocity_counts_urban, depth_counts_urban,hazard_counts, hazard_props]\n",
    "suffixes = ['_propcells', '_propcells','_propcells_urban','_propcells_urban','_countcells','_countcells','_countcells_urban','_countcells_urban',\n",
    "'_numcells', '_propcells']\n",
    "\n",
    "for num, df in enumerate(dfs):\n",
    "    # Reformat the dataframe\n",
    "    df = df.set_index('index').T\n",
    "    # Add the correct suffix to the column names\n",
    "    df = df.add_suffix(suffixes[num]) \n",
    "    # Add Cluster_num column for joining\n",
    "    df['Cluster_num'] = df.index#\n",
    "    # Join to cluster results dataframe\n",
    "    cluster_results = pd.merge(cluster_results,  df, how=\"outer\", on = 'Cluster_num')\n",
    "    \n",
    "cluster_results = pd.merge(cluster_results, hazard_cat_changes,  how=\"outer\", on = 'Cluster_num')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d6402",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6facf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results.to_csv(\"Data/allclusters_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cea586",
   "metadata": {},
   "source": [
    "### Delete tiff files (as these aren't used again and take up a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f688f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for method in short_ids:\n",
    "#     print(method)\n",
    "#     if method != '6h_feh_sp':\n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_cat_difference.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_classified.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_classified.tif\".format(method)) \n",
    "#         os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_difffromsinglepeak_posneg.tif\".format(method)) \n",
    "        \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Depth_classified.tif\".format(method)) \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/hazard_classified.tif\".format(method)) \n",
    "#     os.remove(\"../../../../FloodModelling/MeganModel_New/{}/Velocity_classified.tif\".format(method)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
