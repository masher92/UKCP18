{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca242a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import os\n",
    "import glob as sir_globington_the_file_gatherer\n",
    "import datetime as datetime\n",
    "import iris.coord_categorisation as cat\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import glob as glob\n",
    "\n",
    "# Set up path to root directory\n",
    "root_fp = \"/nfs/a319/gy17m2a/PhD/\"\n",
    "os.chdir(root_fp)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create path to files containing functions\n",
    "sys.path.insert(0, root_fp + 'Scripts/GlobalFunctions')\n",
    "from Spatial_plotting_functions import *\n",
    "from Spatial_geometry_functions import *\n",
    "\n",
    "##################################################################\n",
    "# Load necessary spatial data\n",
    "##################################################################\n",
    "# This is a square area surrounding Leeds\n",
    "leeds_at_centre_gdf = create_leeds_at_centre_outline({'init' :'epsg:3857'})\n",
    "uk_gdf = create_uk_outline({'init' :'epsg:3857'})\n",
    "gb_gdf = create_gb_outline({'init' :'epsg:3857'})\n",
    "##################################################################\n",
    "\n",
    "# ### Establish the ensemble member\n",
    "trim_to_leeds = False\n",
    "\n",
    "ems= ['bc005', 'bc006', 'bc007', 'bc009', 'bc010', 'bc011', 'bc013',  'bc015',  'bc016', 'bc017', 'bc018', 'bc012']\n",
    "yrs_range = \"2002_2020\"\n",
    "resolution = '2.2km' #2.2km, 12km, 2.2km_regridded_12km\n",
    "in_jja=iris.Constraint(time=lambda cell: 6 <= cell.point.month <= 8)\n",
    "yrs= range(2001,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8178f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc015 2001 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2002 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2003 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2004 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2005 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2006 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2007 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2008 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2009 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2010 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2011 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2012 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2013 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2014 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2015 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2016 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2017 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2018 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n",
      "bc015 2019 2.2km\n",
      "12\n",
      "getting mask\n",
      "compressed has length: 802344960\n"
     ]
    }
   ],
   "source": [
    "em = 'bc005'\n",
    "\n",
    "for yr in yrs:\n",
    "    ddir = f\"ProcessedData/TimeSeries/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}_wholeyear/\"\n",
    "    if not os.path.isdir(ddir):\n",
    "            os.makedirs(ddir)\n",
    "            \n",
    "    if not os.path.isfile(ddir + f'{yr}_compressed.npy'):\n",
    "        print(em, yr, resolution)\n",
    "\n",
    "        ### Get a list of filenames for this ensemble member\n",
    "        general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/bc005/1980_2001/bng_bc005a.pr{yr}*'\n",
    "        filenames = [filename for filename in glob.glob(general_filename) if '2000' not in filename and 'pr2020' not in filename]\n",
    "        print(len(filenames))\n",
    "\n",
    "        ### Load in the data\n",
    "        monthly_cubes_list = iris.load(filenames)\n",
    "\n",
    "        ### Concatenate cubes into one\n",
    "        model_cube = monthly_cubes_list.concatenate_cube()      \n",
    "\n",
    "        # ### Trim to UK\n",
    "        if resolution  == '2.2km':\n",
    "            masked_cube = trim_to_bbox_of_region_regriddedobs(model_cube, gb_gdf)\n",
    "        else:\n",
    "            masked_cube = trim_to_bbox_of_region_obs(model_cube, gb_gdf)\n",
    "\n",
    "\n",
    "        ### Get the mask\n",
    "        print(\"getting mask\")\n",
    "        if resolution =='2.2km':\n",
    "            gb_mask = np.load(\"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_2.2km_GB_Mask.npy\")\n",
    "        else:\n",
    "            gb_mask = np.load(\"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_12km_GB_Mask.npy\")\n",
    "\n",
    "\n",
    "          # masked_cube_data = masked_cube * gb_mask[np.newaxis, :, :]\n",
    "\n",
    "        # # APPLY THE MASK\n",
    "        reshaped_mask = np.tile(gb_mask, (masked_cube.shape[0], 1, 1))\n",
    "        reshaped_mask = reshaped_mask.astype(int)\n",
    "        reversed_array = ~reshaped_mask.astype(bool)\n",
    "\n",
    "        # Mask the cube\n",
    "        masked_cube = iris.util.mask_cube(masked_cube, reversed_array)  \n",
    "\n",
    "        # Check the plotting\n",
    "        #iplt.contourf(masked_cube[10])\n",
    "        #plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "        # Get rid of negative values\n",
    "        compressed = masked_cube.data.compressed()\n",
    "        print(f\"compressed has length: {compressed.shape[0]}\")\n",
    "\n",
    "        ########\n",
    "        # Get the times\n",
    "        ########\n",
    "        # Step 2: Get the indices of the non-masked values in the original data\n",
    "        non_masked_indices = np.where(~masked_cube.data.mask)\n",
    "\n",
    "        # Step 3: Extract corresponding time values\n",
    "        time_values = masked_cube.coord('time').points[non_masked_indices[0]]\n",
    "\n",
    "        # Save to file\n",
    "        if not os.path.isfile(ddir + f'timevalues.npy'):\n",
    "            np.save(ddir + f'timevalues.npy', time_values) \n",
    "        np.save(ddir + f'{yr}_compressed.npy', compressed) \n",
    "        iris.save(masked_cube, ddir + f'{yr}_maskedcube.nc') \n",
    "    else:\n",
    "        print(f\"{yr} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edce15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "yr=2005\n",
    "general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/bc005/1980_2001/bng_bc005a.pr{yr}*'\n",
    "\n",
    "filenames = []\n",
    "for filename in glob.glob(general_filename):\n",
    "    if '2000' not in filename and 'pr2020' not in filename:\n",
    "        filenames.append(filename)\n",
    "print(len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad85aea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8171/4258217616.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Extract data for the specified indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_2d' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import iris\n",
    "\n",
    "general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/bc005/1980_2001/bng_bc005a.pr{yr}*'\n",
    "\n",
    "filenames = [filename for filename in glob.glob(general_filename) if '2000' not in filename and 'pr2020' not in filename]\n",
    "\n",
    "# Load cubes lazily\n",
    "cubes = iris.load(filenames)\n",
    "\n",
    "# Concatenate cubes along the time dimension\n",
    "cube = cubes.concatenate_cube()\n",
    "\n",
    "# Extract data for the specified indices\n",
    "data = cube[:, idx_2d[0], idx_2d[1]].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Get a list of filenames for this ensemble member, for just JJA\n",
    "yr=2005\n",
    "general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/bc005/1980_2001/bc005a.pr{yr}*'\n",
    "\n",
    "filenames = []\n",
    "for filename in glob.glob(general_filename):\n",
    "    if '2000' not in filename and 'pr2020' not in filename:\n",
    "        filenames.append(filename)\n",
    "print(len(filenames))\n",
    "\n",
    "### Load in the data\n",
    "monthly_cubes_list = iris.load(filenames)\n",
    "\n",
    "### Concatenate cubes into one\n",
    "model_cube = monthly_cubes_list.concatenate_cube()      \n",
    "\n",
    "# ### Trim to UK\n",
    "if resolution  == '2.2km':\n",
    "    masked_cube = trim_to_bbox_of_region_regriddedobs(model_cube, gb_gdf)\n",
    "else:\n",
    "    masked_cube = trim_to_bbox_of_region_obs(model_cube, gb_gdf)\n",
    "\n",
    "\n",
    "### Get the mask\n",
    "print(\"getting mask\")\n",
    "if resolution =='2.2km':\n",
    "    gb_mask = np.load(\"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_2.2km_GB_Mask.npy\")\n",
    "else:\n",
    "    gb_mask = np.load(\"/nfs/a319/gy17m2a/PhD/datadir/UKCP18_12km_GB_Mask.npy\")\n",
    "\n",
    "\n",
    "  # masked_cube_data = masked_cube * gb_mask[np.newaxis, :, :]\n",
    "\n",
    "# # APPLY THE MASK\n",
    "reshaped_mask = np.tile(gb_mask, (masked_cube.shape[0], 1, 1))\n",
    "reshaped_mask = reshaped_mask.astype(int)\n",
    "reversed_array = ~reshaped_mask.astype(bool)\n",
    "\n",
    "# Mask the cube\n",
    "masked_cube = iris.util.mask_cube(masked_cube, reversed_array)  \n",
    "\n",
    "# Check the plotting\n",
    "#iplt.contourf(masked_cube[10])\n",
    "#plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "# Get rid of negative values\n",
    "compressed = masked_cube.data.compressed()\n",
    "print(f\"compressed has length: {compressed.shape[0]}\")\n",
    "\n",
    "########\n",
    "# Get the times\n",
    "########\n",
    "# Step 2: Get the indices of the non-masked values in the original data\n",
    "non_masked_indices = np.where(~masked_cube.data.mask)\n",
    "\n",
    "# Step 3: Extract corresponding time values\n",
    "time_values = masked_cube.coord('time').points[non_masked_indices[0]]\n",
    "\n",
    "# Save to file\n",
    "if not os.path.isfile(ddir + f'timevalues.npy'):\n",
    "    np.save(ddir + f'timevalues.npy', time_values) \n",
    "np.save(ddir + f'{yr}_compressed.npy', compressed) \n",
    "iris.save(masked_cube, ddir + f'{yr}_maskedcube.nc') \n",
    "else:\n",
    "print(f\"{yr} already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
