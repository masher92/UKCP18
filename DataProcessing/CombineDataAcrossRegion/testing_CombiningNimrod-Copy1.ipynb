{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdd35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "loading 92 filenames\n",
      "deleting cube 47 as it only had one dimension\n",
      "DimCoord([2019-06-01 00:12:30], bounds=[[2019-06-01 00:00:00, 2019-06-01 00:25:00]], standard_name='time', calendar='gregorian', var_name='time')\n",
      "DimCoord([2019-08-31 23:42:30], bounds=[[2019-08-31 23:30:00, 2019-08-31 23:55:00]], standard_name='time', calendar='gregorian', var_name='time')\n",
      "getting mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/iris/coord_systems.py:531: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18.\n",
      "  globe=globe,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value is 0.0\n",
      "0.0\n",
      "243.0625\n",
      "2020\n",
      "loading 92 filenames\n",
      "DimCoord([2020-06-02 00:12:30], bounds=[[2020-06-02 00:00:00, 2020-06-02 00:25:00]], standard_name='time', calendar='gregorian', var_name='time')\n",
      "DimCoord([2020-08-31 23:42:30], bounds=[[2020-08-31 23:30:00, 2020-08-31 23:55:00]], standard_name='time', calendar='gregorian', var_name='time')\n",
      "getting mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/iris/coord_systems.py:531: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18.\n",
      "  globe=globe,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value is 0.0\n",
      "0.0\n",
      "349.0\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# This Script:\n",
    "#    - Gets all 30 mins radar files for one year\n",
    "#    - Joins them and masks out values over the sea\n",
    "#    - Gets a 1D array of the data and removes masked out (over the sea\n",
    "#      values) and np.nan values\n",
    "##################################################################\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# SET UP ENVIRONMENT\n",
    "##################################################################\n",
    "import iris.coord_categorisation\n",
    "import iris\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import matplotlib \n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "import iris.quickplot as qplt\n",
    "import iris.plot as iplt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import colors\n",
    "import glob as glob\n",
    "import datetime\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "\n",
    "# Set up path to root directory\n",
    "root_fp = \"/nfs/a319/gy17m2a/PhD/\"\n",
    "os.chdir(root_fp)\n",
    "\n",
    "# Create path to files containing functions\n",
    "sys.path.insert(0, '/nfs/a319/gy17m2a/PhD/Scripts/GlobalFunctions')\n",
    "from Spatial_plotting_functions import *\n",
    "from Spatial_geometry_functions import *\n",
    "\n",
    "resolution = '2.2km'\n",
    "trim_to_leeds = False\n",
    "\n",
    "# # Constraint to only load JJA data\n",
    "in_jja=iris.Constraint(time=lambda cell: 6 <= cell.point.month <= 8)\n",
    "\n",
    "##################################################################\n",
    "# Load necessary spatial data\n",
    "##################################################################\n",
    "# This is a square area surrounding Leeds\n",
    "leeds_at_centre_gdf = create_leeds_at_centre_outline({'init' :'epsg:3857'})\n",
    "uk_gdf = create_uk_outline({'init' :'epsg:3857'})\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# FOR ONE YEAR AT A TIME\n",
    "##################################################################\n",
    "for year in range(2019, 2021):\n",
    "    print(year)\n",
    "\n",
    "    # Create directory to store outputs in and get general filename to load files from\n",
    "    if resolution =='1km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/OriginalFormat_1km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/OriginalFormat_1km/{year}/*'      \n",
    "    elif resolution == '2.2km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_2.2km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_2.2km/NearestNeighbour/{year}/*'      \n",
    "    elif resolution == '12km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_12km/\"    \n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_12km/NearestNeighbour/{year}/*'      \n",
    "    if not os.path.isdir(ddir):\n",
    "        os.makedirs(ddir)\n",
    "\n",
    "    # GET LIST OF ALL FILENAMES FOR THIS YEAR\n",
    "    filenames =[]\n",
    "    # Find all files in directory which start with this string\n",
    "    for filename in glob.glob(general_filename):\n",
    "        # print(filename)\n",
    "        filenames.append(filename)\n",
    "    print(f\"loading {len(filenames)} filenames\")\n",
    "    sorted_list = sorted(filenames)\n",
    "\n",
    "    # LOAD THE DATA\n",
    "    monthly_cubes_list = iris.load(sorted_list, in_jja)\n",
    "      \n",
    "    ##################################################################\n",
    "    # CLEAN AND JOIN THE DATA\n",
    "    ##################################################################\n",
    "    # Get rid of any files which don't have the time dimension\n",
    "    is_to_delete = []\n",
    "    for i in range(0,len(monthly_cubes_list) ):\n",
    "        if len(monthly_cubes_list[i].shape) <3:\n",
    "            is_to_delete.append(i)\n",
    "    for i in is_to_delete:\n",
    "        print(f\"deleting cube {i} as it only had one dimension\")\n",
    "        del monthly_cubes_list[i] \n",
    "\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_period')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_period')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_reference_time')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_reference_time')\n",
    "        except:\n",
    "            pass           \n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('hour')\n",
    "            monthly_cubes_list[i].remove_coord('hour')\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "    # Try to make attributes the same\n",
    "    iris.util.equalise_attributes(monthly_cubes_list)   \n",
    "    \n",
    "    # CONVERT TO FLOAT64\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        monthly_cubes_list[i].data = monthly_cubes_list[i].data.astype('float64')\n",
    "\n",
    "    cube_jja = monthly_cubes_list.concatenate_cube()\n",
    "\n",
    "    print(cube_jja.coord('time')[0])\n",
    "    print(cube_jja.coord('time')[-1])\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # TRIM TO UK\n",
    "    ##################################################################\n",
    "    cube_jja_uk = trim_to_bbox_of_region_regriddedobs(cube_jja, uk_gdf)\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(cube_jja[10])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "    ##################################################################\n",
    "    # MASK OUT VALUES OVER THE SEA\n",
    "    ##################################################################\n",
    "    print(\"getting mask\")\n",
    "    lsm_cubes_list = iris.load(\"/nfs/a319/gy17m2a/PhD/datadir/lsm_land-cpm_BI_5km.nc\")\n",
    "    lsm = lsm_cubes_list[0]\n",
    "    lsm_nn =lsm.regrid(cube_jja_uk,iris.analysis.Nearest())   \n",
    "\n",
    "\n",
    "    ### Convert land sea mask to a cube of the same shape as our data \n",
    "    # Convert to shape of cube\n",
    "    broadcasted_lsm_data = np.broadcast_to(lsm_nn.data.data, cube_jja_uk.shape)\n",
    "    # Convert to integer\n",
    "    broadcasted_lsm_data_int = broadcasted_lsm_data.astype(int)\n",
    "    # Reverse the array (it is the opposite way round to the exisitng val/no val mask on the radar data)\n",
    "    reversed_array = ~broadcasted_lsm_data_int.astype(bool)\n",
    "\n",
    "    ### Mask the cube using the lsm cube\n",
    "    masked_cube = iris.util.mask_cube(cube_jja_uk, reversed_array)\n",
    "\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(masked_cube[0])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "\n",
    "    # ### Save to check with ncview\n",
    "    # Run linux ncview f'masked-cube-{year}-{resolution}.nc'\n",
    "    ddir + f'{year}_maskedcube.nc'\n",
    "    iris.save(masked_cube, ddir + f'{year}_maskedcube.nc')\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Min value is {np.nanmin(masked_cube.data)}\")\n",
    "\n",
    "    ##################################################################\n",
    "    # COMPRESS DATA (FLATTEN AND REMOVE MASKED VALUES)\n",
    "    ##################################################################\n",
    "\n",
    "    compressed = masked_cube.data.compressed()\n",
    "    compressed.shape[0]\n",
    "    # REMOVE NAN VALUES\n",
    "    #compressed = compressed[~np.isnan(compressed)]\n",
    "    \n",
    "    ########\n",
    "    # Get the times\n",
    "    ########\n",
    "    # Step 2: Get the indices of the non-masked values in the original data\n",
    "    non_masked_indices = np.where(~masked_cube.data.mask)\n",
    "\n",
    "    # Step 3: Extract corresponding time values\n",
    "    time_values = masked_cube.coord('time').points[non_masked_indices[0]]\n",
    "    np.save(ddir + f'{year}_timevalues.npy', time_values) \n",
    "        \n",
    "\n",
    "    # ### Check length of data from flattening it before compressing (shows we have lost 60% values)\n",
    "    # notcompressed = masked_cube.data.flatten()\n",
    "    # (compressed.shape[0] / (notcompressed.shape[0] + compressed.shape[0])) *100\n",
    "\n",
    "    # ### Sense check min/max values\n",
    "    print(np.nanmin(compressed))\n",
    "    print(np.nanmax(compressed))\n",
    "\n",
    "    less0 = compressed[compressed <0]\n",
    "    more0 = compressed[compressed >0]\n",
    "\n",
    "    ##################################################################\n",
    "    # SAVE TO NUMPY ARRAY\n",
    "    ##################################################################\n",
    "    np.save(ddir + f'{year}_compressed.npy', compressed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fiona._env:PROJ: proj_identify: /nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR:fiona._env:PROJ: proj_identify: /nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "loading 92 filenames\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# This Script:\n",
    "#    - Gets all 30 mins radar files for one year\n",
    "#    - Joins them and masks out values over the sea\n",
    "#    - Gets a 1D array of the data and removes masked out (over the sea\n",
    "#      values) and np.nan values\n",
    "##################################################################\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# SET UP ENVIRONMENT\n",
    "##################################################################\n",
    "import iris.coord_categorisation\n",
    "import iris\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import matplotlib \n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "import iris.quickplot as qplt\n",
    "import iris.plot as iplt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import colors\n",
    "import glob as glob\n",
    "import datetime\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "\n",
    "# Set up path to root directory\n",
    "root_fp = \"/nfs/a319/gy17m2a/PhD/\"\n",
    "os.chdir(root_fp)\n",
    "\n",
    "# Create path to files containing functions\n",
    "sys.path.insert(0, '/nfs/a319/gy17m2a/PhD/Scripts/GlobalFunctions')\n",
    "from Spatial_plotting_functions import *\n",
    "from Spatial_geometry_functions import *\n",
    "\n",
    "resolution = '2.2km'\n",
    "trim_to_leeds = False\n",
    "\n",
    "# # Constraint to only load JJA data\n",
    "in_jja=iris.Constraint(time=lambda cell: 6 <= cell.point.month <= 8)\n",
    "\n",
    "##################################################################\n",
    "# Load necessary spatial data\n",
    "##################################################################\n",
    "# This is a square area surrounding Leeds\n",
    "leeds_at_centre_gdf = create_leeds_at_centre_outline({'init' :'epsg:3857'})\n",
    "uk_gdf = create_uk_outline({'init' :'epsg:3857'})\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# FOR ONE YEAR AT A TIME\n",
    "##################################################################\n",
    "for year in range(2016, 2021):\n",
    "    print(year)\n",
    "\n",
    "    # Create directory to store outputs in and get general filename to load files from\n",
    "    if resolution =='1km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/OriginalFormat_1km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/OriginalFormat_1km/{year}/*'      \n",
    "    elif resolution == '2.2km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_2.2km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_2.2km/NearestNeighbour/{year}/*'      \n",
    "    elif resolution == '12km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_12km/\"    \n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_12km/NearestNeighbour/{year}/*'      \n",
    "    if not os.path.isdir(ddir):\n",
    "        os.makedirs(ddir)\n",
    "\n",
    "    # GET LIST OF ALL FILENAMES FOR THIS YEAR\n",
    "    filenames =[]\n",
    "    # Find all files in directory which start with this string\n",
    "    for filename in glob.glob(general_filename):\n",
    "        # print(filename)\n",
    "        filenames.append(filename)\n",
    "    print(f\"loading {len(filenames)} filenames\")\n",
    "    sorted_list = sorted(filenames)\n",
    "\n",
    "    # LOAD THE DATA\n",
    "    monthly_cubes_list = iris.load(sorted_list, in_jja)\n",
    "      \n",
    "    ##################################################################\n",
    "    # CLEAN AND JOIN THE DATA\n",
    "    ##################################################################\n",
    "    # Get rid of any files which don't have the time dimension\n",
    "    is_to_delete = []\n",
    "    for i in range(0,len(monthly_cubes_list) ):\n",
    "        if len(monthly_cubes_list[i].shape) <3:\n",
    "            is_to_delete.append(i)\n",
    "    for i in is_to_delete:\n",
    "        print(f\"deleting cube {i} as it only had one dimension\")\n",
    "        del monthly_cubes_list[i] \n",
    "\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_period')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_period')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_reference_time')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_reference_time')\n",
    "        except:\n",
    "            pass           \n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('hour')\n",
    "            monthly_cubes_list[i].remove_coord('hour')\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "    # Try to make attributes the same\n",
    "    iris.util.equalise_attributes(monthly_cubes_list)   \n",
    "    \n",
    "    # CONVERT TO FLOAT64\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        monthly_cubes_list[i].data = monthly_cubes_list[i].data.astype('float64')\n",
    "\n",
    "    cube_jja = monthly_cubes_list.concatenate_cube()\n",
    "\n",
    "    print(cube_jja.coord('time')[0])\n",
    "    print(cube_jja.coord('time')[-1])\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # TRIM TO UK\n",
    "    ##################################################################\n",
    "    cube_jja_uk = trim_to_bbox_of_region_regriddedobs(cube_jja, uk_gdf)\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(cube_jja[10])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "    ##################################################################\n",
    "    # MASK OUT VALUES OVER THE SEA\n",
    "    ##################################################################\n",
    "    print(\"getting mask\")\n",
    "    lsm_cubes_list = iris.load(\"/nfs/a319/gy17m2a/PhD/datadir/lsm_land-cpm_BI_5km.nc\")\n",
    "    lsm = lsm_cubes_list[0]\n",
    "    lsm_nn =lsm.regrid(cube_jja_uk,iris.analysis.Nearest())   \n",
    "\n",
    "\n",
    "    ### Convert land sea mask to a cube of the same shape as our data \n",
    "    # Convert to shape of cube\n",
    "    broadcasted_lsm_data = np.broadcast_to(lsm_nn.data.data, cube_jja_uk.shape)\n",
    "    # Convert to integer\n",
    "    broadcasted_lsm_data_int = broadcasted_lsm_data.astype(int)\n",
    "    # Reverse the array (it is the opposite way round to the exisitng val/no val mask on the radar data)\n",
    "    reversed_array = ~broadcasted_lsm_data_int.astype(bool)\n",
    "\n",
    "    ### Mask the cube using the lsm cube\n",
    "    masked_cube = iris.util.mask_cube(cube_jja_uk, reversed_array)\n",
    "\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(masked_cube[0])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "\n",
    "    # ### Save to check with ncview\n",
    "    # Run linux ncview f'masked-cube-{year}-{resolution}.nc'\n",
    "    ddir + f'{year}_maskedcube.nc'\n",
    "    iris.save(masked_cube, ddir + f'{year}_maskedcube.nc')\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Min value is {np.nanmin(masked_cube.data)}\")\n",
    "\n",
    "    ##################################################################\n",
    "    # COMPRESS DATA (FLATTEN AND REMOVE MASKED VALUES)\n",
    "    ##################################################################\n",
    "\n",
    "    compressed = masked_cube.data.compressed()\n",
    "    compressed.shape[0]\n",
    "    # REMOVE NAN VALUES\n",
    "    #compressed = compressed[~np.isnan(compressed)]\n",
    "    \n",
    "    ########\n",
    "    # Get the times\n",
    "    ########\n",
    "    # Step 2: Get the indices of the non-masked values in the original data\n",
    "    non_masked_indices = np.where(~masked_cube.data.mask)\n",
    "\n",
    "    # Step 3: Extract corresponding time values\n",
    "    time_values = masked_cube.coord('time').points[non_masked_indices[0]]\n",
    "    np.save(ddir + f'{year}_timevalues.npy', time_values) \n",
    "        \n",
    "\n",
    "    # ### Check length of data from flattening it before compressing (shows we have lost 60% values)\n",
    "    # notcompressed = masked_cube.data.flatten()\n",
    "    # (compressed.shape[0] / (notcompressed.shape[0] + compressed.shape[0])) *100\n",
    "\n",
    "    # ### Sense check min/max values\n",
    "    print(np.nanmin(compressed))\n",
    "    print(np.nanmax(compressed))\n",
    "\n",
    "    less0 = compressed[compressed <0]\n",
    "    more0 = compressed[compressed >0]\n",
    "\n",
    "    ##################################################################\n",
    "    # SAVE TO NUMPY ARRAY\n",
    "    ##################################################################\n",
    "    np.save(ddir + f'{year}_compressed.npy', compressed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "adf2a2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "202.921875\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Extract corresponding time values\n",
    "time_values = masked_cube.coord('time').points[non_masked_indices[0]]\n",
    "np.save(ddir + f'{year}_timevalues.npy', time_values) \n",
    "\n",
    "\n",
    "# ### Check length of data from flattening it before compressing (shows we have lost 60% values)\n",
    "# notcompressed = masked_cube.data.flatten()\n",
    "# (compressed.shape[0] / (notcompressed.shape[0] + compressed.shape[0])) *100\n",
    "\n",
    "# ### Sense check min/max values\n",
    "print(np.nanmin(compressed))\n",
    "print(np.nanmax(compressed))\n",
    "\n",
    "less0 = compressed[compressed <0]\n",
    "more0 = compressed[compressed >0]\n",
    "\n",
    "##################################################################\n",
    "# SAVE TO NUMPY ARRAY\n",
    "##################################################################\n",
    "np.save(ddir + f'{year}_compressed.npy', compressed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# This Script:\n",
    "#    - Gets all 30 mins radar files for one year\n",
    "#    - Joins them and masks out values over the sea\n",
    "#    - Gets a 1D array of the data and removes masked out (over the sea\n",
    "#      values) and np.nan values\n",
    "##################################################################\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# SET UP ENVIRONMENT\n",
    "##################################################################\n",
    "import iris.coord_categorisation\n",
    "import iris\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import matplotlib \n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "import iris.quickplot as qplt\n",
    "import iris.plot as iplt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import colors\n",
    "import glob as glob\n",
    "import datetime\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "\n",
    "# Set up path to root directory\n",
    "root_fp = \"/nfs/a319/gy17m2a/PhD/\"\n",
    "os.chdir(root_fp)\n",
    "\n",
    "# Create path to files containing functions\n",
    "sys.path.insert(0, '/nfs/a319/gy17m2a/PhD/Scripts/GlobalFunctions')\n",
    "from Spatial_plotting_functions import *\n",
    "from Spatial_geometry_functions import *\n",
    "\n",
    "resolution = '2.2km'\n",
    "trim_to_leeds = False\n",
    "\n",
    "# # Constraint to only load JJA data\n",
    "in_jja=iris.Constraint(time=lambda cell: 6 <= cell.point.month <= 8)\n",
    "\n",
    "##################################################################\n",
    "# Load necessary spatial data\n",
    "##################################################################\n",
    "# This is a square area surrounding Leeds\n",
    "leeds_at_centre_gdf = create_leeds_at_centre_outline({'init' :'epsg:3857'})\n",
    "uk_gdf = create_uk_outline({'init' :'epsg:3857'})\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# FOR ONE YEAR AT A TIME\n",
    "##################################################################\n",
    "for year in range(2011, 2012):\n",
    "    print(year)\n",
    "\n",
    "    # Create directory to store outputs in and get general filename to load files from\n",
    "    if resolution =='1km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/OriginalFormat_1km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/OriginalFormat_1km/{year}/*'      \n",
    "    elif resolution == '2.2km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_2.2km/\"\n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_2.2km/NearestNeighbour/{year}/*'      \n",
    "    elif resolution == '12km':\n",
    "        ddir = f\"ProcessedData/TimeSeries/NIMROD/30mins/NIMROD_regridded_12km/\"    \n",
    "        general_filename = f'datadir/NIMROD/30mins/NIMROD_regridded_12km/NearestNeighbour/{year}/*'      \n",
    "    if not os.path.isdir(ddir):\n",
    "        os.makedirs(ddir)\n",
    "\n",
    "    # GET LIST OF ALL FILENAMES FOR THIS YEAR\n",
    "    filenames =[]\n",
    "    # Find all files in directory which start with this string\n",
    "    for filename in glob.glob(general_filename):\n",
    "        # print(filename)\n",
    "        filenames.append(filename)\n",
    "    print(len(filenames))\n",
    "    sorted_list = sorted(filenames)\n",
    "\n",
    "    # LOAD THE DATA\n",
    "    monthly_cubes_list = iris.load(sorted_list, in_jja)\n",
    "      \n",
    "    ##################################################################\n",
    "    # CLEAN AND JOIN THE DATA\n",
    "    ##################################################################\n",
    "    # Get rid of any files which don't have the time dimension\n",
    "    is_to_delete = []\n",
    "    for i in range(0,len(monthly_cubes_list) ):\n",
    "        if len(monthly_cubes_list[i].shape) <3:\n",
    "            is_to_delete.append(i)\n",
    "        for i in is_to_delete:\n",
    "            print(i)\n",
    "            del monthly_cubes_list[i] \n",
    "\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_period')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_period')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('forecast_reference_time')\n",
    "            monthly_cubes_list[i].remove_coord('forecast_reference_time')\n",
    "        except:\n",
    "            pass           \n",
    "        try:\n",
    "            monthly_cubes_list[i].coord('hour')\n",
    "            monthly_cubes_list[i].remove_coord('hour')\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "    # Try to make attributes the same\n",
    "    iris.util.equalise_attributes(monthly_cubes_list)   \n",
    "    \n",
    "    # CONVERT TO FLOAT64\n",
    "    for i in range(0, len(monthly_cubes_list)):\n",
    "        monthly_cubes_list[i].data = monthly_cubes_list[i].data.astype('float64')\n",
    "\n",
    "    cube_jja = monthly_cubes_list.concatenate_cube()\n",
    "\n",
    "    print(cube_jja.coord('time')[0])\n",
    "    print(cube_jja.coord('time')[-1])\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # TRIM TO UK\n",
    "    ##################################################################\n",
    "    cube_jja_uk = trim_to_bbox_of_region_regriddedobs(cube_jja, uk_gdf)\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(cube_jja[10])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "    ##################################################################\n",
    "    # MASK OUT VALUES OVER THE SEA\n",
    "    ##################################################################\n",
    "    print(\"getting mask\")\n",
    "    lsm_cubes_list = iris.load(\"/nfs/a319/gy17m2a/PhD/datadir/lsm_land-cpm_BI_5km.nc\")\n",
    "    lsm = lsm_cubes_list[0]\n",
    "    lsm_nn =lsm.regrid(cube_jja_uk,iris.analysis.Nearest())   \n",
    "\n",
    "\n",
    "    ### Convert land sea mask to a cube of the same shape as our data \n",
    "    # Convert to shape of cube\n",
    "    broadcasted_lsm_data = np.broadcast_to(lsm_nn.data.data, cube_jja_uk.shape)\n",
    "    # Convert to integer\n",
    "    broadcasted_lsm_data_int = broadcasted_lsm_data.astype(int)\n",
    "    # Reverse the array (it is the opposite way round to the exisitng val/no val mask on the radar data)\n",
    "    reversed_array = ~broadcasted_lsm_data_int.astype(bool)\n",
    "\n",
    "    ### Mask the cube using the lsm cube\n",
    "    masked_cube = iris.util.mask_cube(cube_jja_uk, reversed_array)\n",
    "\n",
    "\n",
    "    ### Check plotting\n",
    "    iplt.contourf(masked_cube[0])\n",
    "    plt.gca().coastlines(resolution='10m', color='black', linewidth=0.5);\n",
    "\n",
    "\n",
    "    # ### Save to check with ncview\n",
    "    # Run linux ncview f'masked-cube-{year}-{resolution}.nc'\n",
    "    ddir + f'{year}_maskedcube.nc'\n",
    "    iris.save(masked_cube, ddir + f'{year}_maskedcube.nc')\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Min value is {np.nanmin(masked_cube.data)}\")\n",
    "\n",
    "    ##################################################################\n",
    "    # COMPRESS DATA (FLATTEN AND REMOVE MASKED VALUES)\n",
    "    ##################################################################\n",
    "\n",
    "    compressed = masked_cube.data.compressed()\n",
    "    compressed.shape[0]\n",
    "    # REMOVE NAN VALUES\n",
    "    compressed = compressed[~np.isnan(compressed)]\n",
    "\n",
    "\n",
    "    # ### Check length of data from flattening it before compressing (shows we have lost 60% values)\n",
    "    # notcompressed = masked_cube.data.flatten()\n",
    "    # (compressed.shape[0] / (notcompressed.shape[0] + compressed.shape[0])) *100\n",
    "\n",
    "    # ### Sense check min/max values\n",
    "    print(np.nanmin(compressed))\n",
    "    print(np.nanmax(compressed))\n",
    "\n",
    "    less0 = compressed[compressed <0]\n",
    "    more0 = compressed[compressed >0]\n",
    "\n",
    "    ##################################################################\n",
    "    # SAVE TO NUMPY ARRAY\n",
    "    ##################################################################\n",
    "    np.save(ddir + f'{year}_compressed.npy', compressed) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
