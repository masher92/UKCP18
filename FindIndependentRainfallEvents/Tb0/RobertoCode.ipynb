{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan  4 14:16:57 2020\n",
    "\n",
    "@author: Roberto Villalobos\n",
    "\n",
    " Sub-hourly climatology script\n",
    " Will calculate statistics at various durations in a similar fashion to TBR_desc_stats_auto.R\n",
    " Output should preserve same format as R script\n",
    " Resulting data will be visualised in R using desc_stats_explorer.R\n",
    " \n",
    " \n",
    " Next speed-up: save data from all durations, for one station, and write once instead of writing at each duration\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Packages\n",
    "import os\n",
    "#os.chdir('D:/PhD/13. Scripts/phd-python-code')\n",
    "os.chdir('C:/PhD/13. Scripts/phd-python-code/Intense_QC/')\n",
    "##import intense.intense as ex\n",
    "import intense_Roberto_03 as ex\n",
    "import glob\n",
    "import os.path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "#import UKQC.intense_CW as ex\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import numpy.ma as ma\n",
    "\n",
    "\n",
    "def P_missing(series):\n",
    "    return(max(100*series.isna().sum() / series.shape[0], 100*(1-series.shape[0]/(24*365))))\n",
    "    \n",
    "def P_missing_NL(series):\n",
    "    return(max(100*series.isna().sum() / series.shape[0], 100*(1-series.shape[0]/(24*30))))\n",
    "    \n",
    "    \n",
    "# ETCCDI utility function 1\n",
    "def prep_etccdi_variable(input_path, index_name, aggregation, data_source):\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Omit final year (2010) of HADEX2 - suspiciously large CDD for Malaysia\n",
    "    if data_source == 'HADEX2':\n",
    "        ds = ds.sel(time=slice(datetime.datetime(1951, 1, 1, 0),\n",
    "                               datetime.datetime(2009, 12, 31, 23)))\n",
    "\n",
    "    # Calculate maximum rainfall value over whole period\n",
    "    vals = ds[index_name].values\n",
    "    if index_name in ['CWD', 'CDD']:\n",
    "        vals = ds[index_name].values.astype('timedelta64[s]')\n",
    "        vals = vals.astype('float32') / 86400.0\n",
    "        vals[vals < 0.0] = np.nan\n",
    "    vals = ma.masked_invalid(vals)\n",
    "    if aggregation == 'max':\n",
    "        data = ma.max(vals, axis=0)\n",
    "    if aggregation == 'mean':\n",
    "        data = ma.mean(vals, axis=0)\n",
    "\n",
    "    # Convert back from to a xarray DataArray for easy plotting\n",
    "    # - masked array seems to be interpreted as np array (i.e. nans are present\n",
    "    # in the xarray DataArray\n",
    "    data2 = xr.DataArray(data, coords={'Latitude': ds['lat'].values,\n",
    "                                       'Longitude': ds['lon'].values}, dims=('Latitude', 'Longitude'),\n",
    "                         name=index_name)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    return data2\n",
    "# ETCCDI utility function 2\n",
    "def get_etccdi_value(etccdi_data, index_name, lon, lat):\n",
    "    lon = float(lon)\n",
    "    lat = float(lat)\n",
    "\n",
    "    # Check gauge longitude and convert to -180 - 180 range if necessary\n",
    "    if lon > 180.0:\n",
    "        lon = lon - 360.0\n",
    "\n",
    "    # Array location indices for closest cell centre to gauge location\n",
    "    location_indices = {'GHCNDEX': {}, 'HADEX2': {}}\n",
    "    for data_source in location_indices.keys():\n",
    "        location_indices[data_source]['lon'] = (np.argmin(\n",
    "            np.abs(etccdi_data[data_source][index_name]['Longitude'].values - lon)))\n",
    "        location_indices[data_source]['lat'] = (np.argmin(\n",
    "            np.abs(etccdi_data[data_source][index_name]['Latitude'].values - lat)))\n",
    "\n",
    "    # Maximum of ETCCDI index values from GHCNDEX and HADEX2 for cell\n",
    "    etccdi_index_values = {}\n",
    "    for data_source in location_indices.keys():\n",
    "        yi = location_indices[data_source]['lat']\n",
    "        xi = location_indices[data_source]['lon']\n",
    "        etccdi_index_values[data_source] = etccdi_data[data_source][index_name].values[yi, xi]\n",
    "    etccdi_vals = np.asarray(list(etccdi_index_values.values()))\n",
    "    if np.any(np.isfinite(etccdi_vals)):\n",
    "        max_index = np.max(etccdi_vals[np.isfinite(etccdi_vals)])\n",
    "    else:\n",
    "        max_index = np.nan\n",
    "\n",
    "    # For cases where no value for the cell, look in 3x3 window and take the maximum\n",
    "    if np.isnan(max_index):\n",
    "        etccdi_index_window = {}\n",
    "        for data_source in location_indices.keys():\n",
    "            yi = location_indices[data_source]['lat']\n",
    "            xi = location_indices[data_source]['lon']\n",
    "            window = etccdi_data[data_source][index_name].values[yi - 1:yi + 2, xi - 1:xi + 2]\n",
    "            if np.any(np.isfinite(window)):\n",
    "                etccdi_index_window[data_source] = np.max(window[np.isfinite(window)])\n",
    "            else:\n",
    "                etccdi_index_window[data_source] = np.nan\n",
    "\n",
    "        window_vals = np.asarray(list(etccdi_index_window.values()))\n",
    "        if np.any(np.isfinite(window_vals)):\n",
    "            max_index_window = np.max(window_vals[np.isfinite(window_vals)])\n",
    "        else:\n",
    "            max_index_window = np.nan\n",
    "\n",
    "    else:\n",
    "        max_index_window = np.nan\n",
    "\n",
    "    return max_index, max_index_window\n",
    "\n",
    "# Prepare ETCCDI variables\n",
    "def read_etccdi_data(etccdi_data_folder):\n",
    "    etccdi_data = {\"GHCNDEX\": {}, \"HADEX2\": {}}\n",
    "    etccdi_indices = ['CWD', 'CDD', 'R99p', 'PRCPTOT', 'SDII', 'Rx1day']\n",
    "    periods = {\"GHCNDEX\": '1951-2018', \"HADEX2\": '1951-2010'}\n",
    "    aggregations = {}\n",
    "    for index in etccdi_indices:\n",
    "        aggregations[index] = 'max'\n",
    "    aggregations['SDII'] = 'mean'\n",
    "    for data_source in etccdi_data.keys():\n",
    "        for index in etccdi_indices:\n",
    "            etccdi_data_path = (etccdi_data_folder + '/RawData_' + data_source +\n",
    "                                '_' + index + '_' + periods[data_source] +\n",
    "                                '_ANN_from-90to90_from-180to180.nc')\n",
    "            etccdi_data[data_source][index] = prep_etccdi_variable(etccdi_data_path,\n",
    "                                                                   index, aggregations[index], data_source)\n",
    "    return etccdi_data\n",
    "  \n",
    "\n",
    "# Calculate critical interarrival time       \n",
    "def crit_dur(file, metadir, etccdi_data):\n",
    "    # Read station data\n",
    "    try:\n",
    "        data = pd.read_csv(file)\n",
    "        \n",
    "        # Get datetime index\n",
    "        data.index = pd.DatetimeIndex(data['ob_time'])\n",
    "        # Get metadata in file\n",
    "        station_id = data['id'][1]\n",
    "        station_name = data['src_id'][1]\n",
    "    except:\n",
    "        print('Could not read data for '+file)\n",
    "    \n",
    "    # Additional metadata\n",
    "    try:\n",
    "        metadata = ex.readIntense(metadir+station_id+\".txt\", only_metadata = False)\n",
    "        lon = metadata.longitude\n",
    "        lat = metadata.latitude\n",
    "    except:\n",
    "        lon = -999\n",
    "        lat = -999\n",
    "        print('Could not read metadata for '+file)\n",
    "        \n",
    "    # Drop un-needed columns\n",
    "    data = pd.DataFrame(data['accum'])\n",
    "    # Calculate time difference vector to identify if data is 15-min or other type\n",
    "    tdifs = data.index.to_series().diff()/np.timedelta64(1,'s')\n",
    "    tdifs = tdifs.resample('M').apply(lambda x: stats.mode(x)[0]) \n",
    "    \n",
    "    # If at least one month has 15-min data, consider the entire record as 15-min\n",
    "    if tdifs[tdifs == 900].shape[0] >= 1:\n",
    "        tdif = 900\n",
    "    else:\n",
    "        tdif = 60\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Calculate percentage missing values \n",
    "    # Converting float to strings forces count to include all values and not ignore nans\n",
    "    # Need to re-work pdifs to account for very few instances where SHQC-removed events \n",
    "    # add lots of nans in terms of observations but not in terms of time (see Andover case)\n",
    "\n",
    "    # Percentage missing, per month:\n",
    "    pdifsm = 100*((data.astype('str').resample(\"M\").count() - data.resample(\"M\").count())/data.astype('str').resample(\"M\").count())\n",
    "    pdifsm.rename(columns={\"accum\":\"p_missing\"}, inplace = True)\n",
    "    \n",
    "    # Percentage missing, according to hourly data, this is a superior method:\n",
    "    # Mixed mode because of manually deleted years in SubH data not found in H data\n",
    "    pdifshm = metadata.data.resample('M').apply(P_missing_NL)\n",
    "    \n",
    "    pdifshm = pd.concat([pdifsm, pdifshm], axis = 1)\n",
    "    pdifshm[0] = np.where((pdifshm[0] == 100*(1-28/30))|(pdifshm[0] == 100*(1-29/30)),0,pdifshm[0])\n",
    "    \n",
    "    # Instead of years over which to iterate, we want a corrected number of years, n\n",
    "    # with useable data for out k largest\n",
    "    \n",
    "    # Remove months with missing data\n",
    "    months = pdifshm[pdifshm < 15].dropna().shape[0]\n",
    "    good_months = pdifshm[pdifshm < 15].dropna().index.strftime(\"%Y%m\")\n",
    "    \n",
    "    # Number of years with less than 15% missing data\n",
    "    n = int(months/12)\n",
    "\n",
    "    \n",
    "    output= pd.DataFrame(columns = [\"Station_id\",\"Station_name\",\"Lon\",\"Lat\",\n",
    "                                    \"Critical_interarrival_time\"])\n",
    "   \n",
    "    # If there is at least one complete year:\n",
    "    if n >= 1:\n",
    "    \n",
    "        data = data.dropna()\n",
    "        dur = 15\n",
    "        # We first resample to 15-minutes as this is the smallest common duration\n",
    "        rolling = data['accum'].resample(str(dur)+'min', label = 'right').sum()\n",
    "        # Next we create a time series where we remove all the zero values\n",
    "        wet_only = np.where(rolling == 0, np.nan, rolling)\n",
    "        rolling = pd.DataFrame(rolling)\n",
    "        rolling['non_zero'] = wet_only\n",
    "        # Now we keep only this column and drop nas\n",
    "        rolling = rolling['non_zero'].copy()\n",
    "        rolling = rolling.dropna()\n",
    "        # We can now calculate the non-rainy period durations\n",
    "        bi = rolling.index.to_series().diff()/np.timedelta64(1,'s')\n",
    "        \n",
    "        # Remove missing data gaps\n",
    "        bi = pd.DataFrame(bi)\n",
    "        bi['ym'] = bi.index.strftime(\"%Y%m\")\n",
    "        #bi = bi.loc[bi['ym'].isin(good_months)] # This line removes missing data, does not change results by much\n",
    "        bi = bi['ob_time']\n",
    "        # Every difference of 900s is due to consecutive wet periods\n",
    "        # If we remove them we are left with the duration of non-rainy periods\n",
    "        bi = bi.loc[bi != 900] \n",
    "        # Remove also intervals over 28 days\n",
    "        #bi = bi.loc[bi < 28*24*60*60]\n",
    "        # Use a longer 3 month interval to test sensistivity\n",
    "        #bi = bi.loc[bi < 3*30*24*60*60]\n",
    "        # Use a 6-month gap\n",
    "        #bi = bi.loc[bi < 6*30*24*60*60]\n",
    "        # After checking individual gauges that show changes: all observed changes due to existance of \n",
    "        # real missing data, considering a monthly duration is used to filter missing data,\n",
    "        # A suggested value of 31 days max gap is allowed. \n",
    "        #bi = bi.loc[bi < 31*24*60*60]\n",
    "        \n",
    "                # Use CDD index as solution to define maximum allowable dry period\n",
    "        # By definition it is the ideal solution\n",
    "\n",
    "        longest_dry_period, longest_dry_period_filled = get_etccdi_value(etccdi_data, 'CDD', lon, lat)\n",
    "        if np.isnan(longest_dry_period):\n",
    "            if np.isnan(longest_dry_period_filled):\n",
    "                # Default to 30 days if needed\n",
    "                bi = bi.loc[bi < 30*24*60*60]\n",
    "            else:\n",
    "                bi = bi.loc[bi< longest_dry_period_filled*24*60*60]\n",
    "        else:\n",
    "            bi = bi.loc[bi< longest_dry_period*24*60*60]\n",
    "\n",
    "        \n",
    "        hist_bi=np.histogram(bi, bins = np.linspace(0,np.nanmax(bi),int(np.nanmax(bi)/900)))\n",
    "        \n",
    "        # N0 is the total number of non-rainy periods\n",
    "        h = hist_bi[0]\n",
    "        s1_0 = (1/len(bi))*np.nansum(bi)\n",
    "        s2_0 = (1/len(bi))*np.nansum(bi**2)\n",
    "        nk = []\n",
    "        nk.append(len(bi))\n",
    "        s1k = []\n",
    "        s1k.append(s1_0)\n",
    "        s2k = []\n",
    "        s2k.append(s2_0)\n",
    "        cv = []\n",
    "        cv.append( (1/s1k[0])*(((s2k[0]-s1k[0]**2)*(nk[0]/(nk[0]-1)))**0.5 ))\n",
    "        # Calculate N_k = N_k-1 - h_k\n",
    "        crit_dur = np.nan\n",
    "        for i in range(1,len(h)):\n",
    "            ni = nk[i-1] - h[i]\n",
    "            nk.append(ni)\n",
    "            s1_i = (1/ni)*(nk[i-1]*s1k[i-1]-h[i]*(i)) \n",
    "            s1k.append(s1_i)\n",
    "            s2_i = (1/ni)*(nk[i-1]*s2k[i-1]-h[i]*(i)**2) \n",
    "            s2k.append(s2_i)\n",
    "            cv.append( (1/s1k[i])*(((s2k[i]-s1k[i]**2)*(nk[i]/(nk[i]-1)))**0.5 ))\n",
    "            if cv[i] <= 1:\n",
    "                crit_dur = (i-1)*900/(60**2)\n",
    "                break\n",
    "            \n",
    "        output = output.append({\"Station_id\": station_id,\n",
    "                                \"Station_name\":station_name,\n",
    "                                \"Lon\": lon,\"Lat\":lat,\n",
    "                                \"Critical_interarrival_time\":crit_dur}, \n",
    "                                                    ignore_index = True)\n",
    "        return output\n",
    "    else:\n",
    "        print('Station '+station_id+' does not have complete years.')\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "        \n",
    "etccdi_data =  read_etccdi_data(r\"C:\\Rainfall data\\ETCCDI\")        \n",
    "        \n",
    "# Just ams after adding overlap code, takes about 30 mins to run... more like 3 mins in the new laptop!\n",
    "in_dir = \"C:/Ultimate_QC/FINAL_HQC_FM_SHQC_M_SHData\"\n",
    "metadir = \"C:/Ultimate_QC/QCd_Data/FL13UK/\"\n",
    "\n",
    "files = glob.glob(os.path.join(in_dir,\"*.txt\"))\n",
    "#files.sort(reverse = True)\n",
    "#file = files[2]\n",
    "logdir = \"C:/Ultimate_QC/Climatology_RX_cor/Joined2\"\n",
    "\n",
    "logdir2 = logdir+\"/interarrival_thresholds_CDD_noMissing.txt\"\n",
    "\n",
    "out = Parallel(n_jobs = 14, verbose = 9)(delayed(crit_dur)(file, metadir, etccdi_data) for file in files)\n",
    "\n",
    "#out = [x for x in out if x != None]\n",
    "\n",
    "ams_out  = pd.concat([item for item in out])\n",
    "\n",
    "ams_out.to_csv(logdir2, index = False)\n",
    "#crit_dur(files[5], metadir)\n",
    "#file = in_dir+\"/EA362710501.txt\" # large gap of real missing data\n",
    "#file = in_dir+\"/SEPA525510.txt\" # real, 35 day gap\n",
    "# Test effect of 28 to 31 day change\n",
    "#file = in_dir+\"/EA999998.txt\"\n",
    "\n",
    "\n",
    "#np.nanmean(ams_out[\"Critical_interarrival_time\"])\n",
    "#%%\n",
    "# Find 5-day max, Honister tel\n",
    "\n",
    "file = in_dir+\"/EA592463.txt\"\n",
    "\n",
    "# Read station data\n",
    "try:\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    # Get datetime index\n",
    "    data.index = pd.DatetimeIndex(data['ob_time'])\n",
    "    # Get metadata in file\n",
    "    station_id = data['id'][1]\n",
    "    station_name = data['src_id'][1]\n",
    "except:\n",
    "    print('Could not read data for '+file)\n",
    "\n",
    "# Additional metadata\n",
    "try:\n",
    "    metadata = ex.readIntense(metadir+station_id+\".txt\", only_metadata = False)\n",
    "    lon = metadata.longitude\n",
    "    lat = metadata.latitude\n",
    "except:\n",
    "    lon = -999\n",
    "    lat = -999\n",
    "    print('Could not read metadata for '+file)\n",
    "    \n",
    "# Drop un-needed columns\n",
    "data = pd.DataFrame(data['accum'])\n",
    "rolling5d = data['accum'].rolling('7200 min').sum()\n",
    "rolling5d.loc[rolling5d == max(rolling5d)].index\n",
    "\n",
    "#%% Updates\n",
    "\n",
    "etccdi_data =  read_etccdi_data(r\"C:\\Rainfall data\\ETCCDI\")        \n",
    "        \n",
    "# Just ams after adding overlap code, takes about 30 mins to run... more like 3 mins in the new laptop!\n",
    "metadir = r\"D:\\Rainfall_storage\\2020updates\\QC\\QCd_Data\\updates/\"\n",
    "in_dir = r\"D:\\Rainfall_storage\\2020updates\\QC\\QCd_Data\\updates_SH_FR_SHQCd\"\n",
    " \n",
    "files = glob.glob(os.path.join(in_dir,\"*.txt\"))\n",
    "#files.sort(reverse = True)\n",
    "#file = files[2]\n",
    "logdir = \"D:/Rainfall_storage/2020updates/QC/Summary\"\n",
    "\n",
    "logdir2 = logdir+\"/interarrival_thresholds_CDD_noMissing.txt\"\n",
    "\n",
    "out = Parallel(n_jobs = 14, verbose = 9)(delayed(crit_dur)(file, metadir, etccdi_data) for file in files)\n",
    "\n",
    "#out = [x for x in out if x != None]\n",
    "\n",
    "ams_out  = pd.concat([item for item in out])\n",
    "\n",
    "ams_out.to_csv(logdir2, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
