{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a8f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ran for eveything now with excluding the gauges that we don't want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3871500",
   "metadata": {},
   "source": [
    "# Create a set of dimensionless profiles\n",
    "Read in all of the events, for all durations, for all gauges, for all ensemble members.  \n",
    "Convert them to dimensionless profiles, with 12 values between 0 and 1.  \n",
    "Each value is a dimensionless, cumulative rainfall value (cumulative rainfall at this timestep, normalised by the total event rainfall):\n",
    "- 0 means no rainfall has occurred, and \n",
    "- 1 means the total event rainfall has been reached.  \n",
    "\n",
    "If there are less than 12 values, then these are filled in with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3c5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quintile_with_max_cumulative_rainfall(cumulative_rainfall):\n",
    "    total_rainfall = cumulative_rainfall[-1]  # Total cumulative rainfall at the end\n",
    "    total_time = len(cumulative_rainfall)  # Total time steps\n",
    "\n",
    "    # Calculate the time index for each quintile\n",
    "    quintile_times = np.linspace(0, total_time, 6, dtype=int)  # Divide into 5 equal parts\n",
    "\n",
    "    # Calculate cumulative rainfall in each quintile\n",
    "    quintile_rainfall = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        start_idx = quintile_times[i]\n",
    "        end_idx = quintile_times[i + 1] if i < 4 else total_time\n",
    "        quintile_rainfall[i] = cumulative_rainfall[end_idx - 1] - cumulative_rainfall[start_idx]\n",
    "\n",
    "    # Find the quintile with the maximum cumulative rainfall\n",
    "    max_quintile = np.argmax(quintile_rainfall)\n",
    "\n",
    "    # Return the quintile index (1-indexed)\n",
    "    return max_quintile + 1\n",
    "\n",
    "def process_file(gauge_num, fp):\n",
    "    # Read the CSV file\n",
    "    test = pd.read_csv(fp)\n",
    "    \n",
    "    # Find duration from file name\n",
    "    pattern = re.compile(r'(\\d+\\.?\\d*)hrs')\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        duration = match.group(1)\n",
    "    else:\n",
    "        duration = None\n",
    "    \n",
    "    precipitation_sum = test['precipitation (mm)'].sum()\n",
    "\n",
    "    if len(test) == 1:\n",
    "        print(f\"Only 1 value at gauge_num {gauge_num} for {file}\")\n",
    "        return None, None, None\n",
    "    else:\n",
    "        times = np.array(range(0, len(test)))\n",
    "        normalized_time, normalized_rainfall = create_dimensionless_profile(times, np.array(test['precipitation (mm/hr)']))\n",
    "        max_quintile = find_quintile_with_max_cumulative_rainfall(normalized_rainfall)\n",
    "        # print(max_quintile)\n",
    "        interpolated_rainfall = interpolate_and_bin(normalized_time, normalized_rainfall)\n",
    "        # heaviest_segment = categorize_normalized_rainstorm(interpolated_rainfall)\n",
    "        real_duration = len(test)/2\n",
    "    return interpolated_rainfall, normalized_rainfall, duration, real_duration, precipitation_sum, max_quintile    \n",
    "\n",
    "def check_for_nan(profiles_list, dimensionless_profiles_list, durations_for_nimrod_profiles,real_durations_for_nimrod_profiles,volumes_for_nimrod_profiles, max_quintiles_ls):\n",
    "    new_profiles_ls = []\n",
    "    new_dimensionless_profiles_ls = []\n",
    "    new_real_durations_ls = []\n",
    "    new_durations_ls = []\n",
    "    new_volumes_ls = []\n",
    "    new_max_quintile_ls = []\n",
    "    \n",
    "    for i, profile in enumerate(profiles_list):\n",
    "        if np.isnan(profile).any():\n",
    "            print(f\"NaN values found in profile {i}\")\n",
    "        else:\n",
    "            new_profiles_ls.append(profile)\n",
    "            new_durations_ls.append(durations_for_nimrod_profiles[i])\n",
    "            new_real_durations_ls.append(volumes_for_nimrod_profiles[i])\n",
    "            new_volumes_ls.append(volumes_for_nimrod_profiles[i])\n",
    "            new_dimensionless_profiles_ls.append(dimensionless_profiles_list[i])\n",
    "            new_max_quintile_ls.append(max_quintiles_ls[i])\n",
    "            \n",
    "    return new_profiles_ls, new_dimensionless_profiles_ls, new_durations_ls,new_real_durations_ls, new_volumes_ls,new_max_quintile_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f9a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_nums = range(0,1263)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65f57f",
   "metadata": {},
   "source": [
    "# NIMROD\n",
    "### Make profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc3ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIMROD_2.2km_filtered_100\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "NaN values found in profile 19653\n",
      "NaN values found in profile 20092\n",
      "NaN values found in profile 26755\n",
      "NaN values found in profile 26770\n",
      "NaN values found in profile 26785\n",
      "NaN values found in profile 26800\n",
      "NaN values found in profile 26817\n",
      "NaN values found in profile 26832\n",
      "NaN values found in profile 26847\n",
      "NaN values found in profile 27832\n",
      "NaN values found in profile 27847\n",
      "NaN values found in profile 27863\n",
      "NaN values found in profile 27879\n",
      "NaN values found in profile 27897\n",
      "NaN values found in profile 27912\n",
      "NaN values found in profile 27927\n",
      "NaN values found in profile 29016\n",
      "NaN values found in profile 29031\n",
      "NaN values found in profile 29046\n",
      "NaN values found in profile 29062\n",
      "NaN values found in profile 29078\n",
      "NaN values found in profile 29093\n",
      "NaN values found in profile 29108\n",
      "NaN values found in profile 31073\n",
      "NaN values found in profile 31088\n",
      "NaN values found in profile 31103\n",
      "NaN values found in profile 31139\n",
      "NaN values found in profile 31154\n",
      "NaN values found in profile 31169\n",
      "NaN values found in profile 31518\n",
      "NaN values found in profile 34079\n",
      "NaN values found in profile 34094\n",
      "NaN values found in profile 34329\n",
      "NaN values found in profile 34359\n",
      "NaN values found in profile 35545\n",
      "NaN values found in profile 44630\n",
      "NaN values found in profile 44645\n",
      "NaN values found in profile 44660\n",
      "NaN values found in profile 44675\n",
      "NaN values found in profile 44692\n",
      "NaN values found in profile 44707\n",
      "NaN values found in profile 44722\n",
      "NaN values found in profile 46047\n",
      "NaN values found in profile 46062\n",
      "NaN values found in profile 46077\n",
      "NaN values found in profile 46092\n",
      "NaN values found in profile 46093\n",
      "NaN values found in profile 46111\n",
      "NaN values found in profile 46126\n",
      "NaN values found in profile 46141\n",
      "NaN values found in profile 53526\n",
      "NaN values found in profile 61686\n",
      "NaN values found in profile 61701\n",
      "NaN values found in profile 61707\n",
      "NaN values found in profile 61716\n",
      "NaN values found in profile 61731\n",
      "NaN values found in profile 61738\n",
      "NaN values found in profile 61749\n",
      "NaN values found in profile 61764\n",
      "NaN values found in profile 61779\n",
      "NaN values found in profile 71610\n",
      "NaN values found in profile 73693\n",
      "NaN values found in profile 86835\n",
      "NaN values found in profile 86865\n",
      "NaN values found in profile 86895\n",
      "NaN values found in profile 86910\n",
      "NaN values found in profile 105005\n",
      "NaN values found in profile 105035\n",
      "NaN values found in profile 105068\n",
      "NaN values found in profile 105083\n",
      "NaN values found in profile 108695\n",
      "NaN values found in profile 108710\n",
      "NaN values found in profile 108741\n",
      "NaN values found in profile 108756\n",
      "NIMROD_2.2km_filtered_300\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "NaN values found in profile 19653\n",
      "NaN values found in profile 20092\n",
      "NaN values found in profile 26755\n",
      "NaN values found in profile 26770\n",
      "NaN values found in profile 26785\n",
      "NaN values found in profile 26800\n",
      "NaN values found in profile 26817\n",
      "NaN values found in profile 26832\n",
      "NaN values found in profile 26847\n",
      "NaN values found in profile 27832\n",
      "NaN values found in profile 27847\n",
      "NaN values found in profile 27863\n",
      "NaN values found in profile 27879\n",
      "NaN values found in profile 27897\n",
      "NaN values found in profile 27912\n",
      "NaN values found in profile 27927\n",
      "NaN values found in profile 29016\n",
      "NaN values found in profile 29031\n",
      "NaN values found in profile 29046\n",
      "NaN values found in profile 29062\n",
      "NaN values found in profile 29078\n",
      "NaN values found in profile 29093\n",
      "NaN values found in profile 29108\n",
      "NaN values found in profile 31073\n",
      "NaN values found in profile 31088\n",
      "NaN values found in profile 31103\n",
      "NaN values found in profile 31139\n",
      "NaN values found in profile 31154\n",
      "NaN values found in profile 31169\n",
      "NaN values found in profile 31518\n",
      "NaN values found in profile 34079\n",
      "NaN values found in profile 34094\n",
      "NaN values found in profile 34329\n",
      "NaN values found in profile 34359\n",
      "NaN values found in profile 35545\n",
      "NaN values found in profile 44630\n",
      "NaN values found in profile 44645\n",
      "NaN values found in profile 44660\n",
      "NaN values found in profile 44675\n",
      "NaN values found in profile 44692\n",
      "NaN values found in profile 44707\n",
      "NaN values found in profile 44722\n",
      "NaN values found in profile 46047\n",
      "NaN values found in profile 46062\n",
      "NaN values found in profile 46077\n",
      "NaN values found in profile 46092\n",
      "NaN values found in profile 46093\n",
      "NaN values found in profile 46111\n",
      "NaN values found in profile 46126\n",
      "NaN values found in profile 46141\n",
      "NaN values found in profile 53526\n",
      "NaN values found in profile 61686\n",
      "NaN values found in profile 61701\n",
      "NaN values found in profile 61707\n",
      "NaN values found in profile 61716\n",
      "NaN values found in profile 61731\n",
      "NaN values found in profile 61738\n",
      "NaN values found in profile 61749\n",
      "NaN values found in profile 61764\n",
      "NaN values found in profile 61779\n",
      "NaN values found in profile 71610\n",
      "NaN values found in profile 73693\n",
      "NaN values found in profile 86835\n",
      "NaN values found in profile 86865\n",
      "NaN values found in profile 86895\n",
      "NaN values found in profile 86910\n",
      "NaN values found in profile 105005\n",
      "NaN values found in profile 105035\n",
      "NaN values found in profile 105068\n",
      "NaN values found in profile 105083\n",
      "NaN values found in profile 108695\n",
      "NaN values found in profile 108710\n",
      "NaN values found in profile 108741\n",
      "NaN values found in profile 108756\n"
     ]
    }
   ],
   "source": [
    "for nimrod_option in [\"NIMROD_2.2km_filtered_100\",\"NIMROD_2.2km_filtered_300\"]:\n",
    "    print(nimrod_option)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    nimrod_profiles = []\n",
    "    durations_for_nimrod_profiles = []\n",
    "    real_durations_for_nimrod_profiles = []\n",
    "    volumes_for_nimrod_profiles = []\n",
    "    dimensionless_profiles = [] \n",
    "    max_quintiles = []\n",
    "\n",
    "    # Process each gauge\n",
    "    for gauge_num in range(0,1269):\n",
    "        # the gauges that didn't work for finding events\n",
    "        if gauge_num not in [423, 444, 827, 888]:\n",
    "            # Excluding ones near windfarms\n",
    "            if gauge_num not in [27,36,57,61,75,97,101,106,120,132,190,204, 239, 285,348,376]:\n",
    "                if gauge_num % 100 == 0:\n",
    "                    print(gauge_num)\n",
    "\n",
    "                # Create a list of all the event CSVs\n",
    "                files = [f for f in os.listdir(f\"../../../ProcessedData/IndependentEvents/NIMROD/{nimrod_option}/{gauge_num}/\") if f.endswith('.csv')]\n",
    "                files = np.sort(files)\n",
    "\n",
    "                # Process each file\n",
    "                for file in files:\n",
    "                    fp = f\"../../../ProcessedData/IndependentEvents/NIMROD/{nimrod_option}/{gauge_num}/{file}\"\n",
    "                    interpolated_rainfall, normalized_rainfall, duration, real_duration, precipitation_sum, max_quintile  =  process_file(gauge_num, fp)\n",
    "                    \n",
    "                    if interpolated_rainfall is not None and duration is not None and precipitation_sum is not None:\n",
    "                        nimrod_profiles.append(interpolated_rainfall)\n",
    "                        durations_for_nimrod_profiles.append(duration)\n",
    "                        real_durations_for_nimrod_profiles.append(real_duration)\n",
    "                        volumes_for_nimrod_profiles.append(precipitation_sum)\n",
    "                        dimensionless_profiles.append(normalized_rainfall)\n",
    "                        max_quintiles.append(max_quintile)\n",
    "\n",
    "    ## Remove profiles containing NANs\n",
    "    nimrod_profiles, dimensionless_profiles, durations_for_nimrod_profiles, real_durations_for_nimrod_profiles, volumes_for_nimrod_profiles, max_quintiles =  check_for_nan(nimrod_profiles, dimensionless_profiles, durations_for_nimrod_profiles, real_durations_for_nimrod_profiles, volumes_for_nimrod_profiles, max_quintiles)\n",
    "    names =['profiles', \"dimensionless_profiles\",'durations_for_profiles',  'real_durations_for_profiles','volumes_for_profiles','max_quintiles']\n",
    "    \n",
    "    for number, file in enumerate([nimrod_profiles, dimensionless_profiles, durations_for_nimrod_profiles, real_durations_for_nimrod_profiles,volumes_for_nimrod_profiles, max_quintiles]):\n",
    "        file_name = names[number]\n",
    "        cache_filepath = f\"/nfs/a319/gy17m2a/PhD/ProcessedData/Profiles/NIMROD/{nimrod_option}_{file_name}.pkl\"\n",
    "        with open(cache_filepath, 'wb') as f:\n",
    "            pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde96374",
   "metadata": {},
   "source": [
    "# UKCP18\n",
    "### Make and pickle profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c3f6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results\n",
    "model_profiles = []\n",
    "durations_for_model_profiles = []\n",
    "real_durations_for_model_profiles = []\n",
    "volumes_for_model_profiles = []\n",
    "normalised_rainfalls = []\n",
    "heaviest_segments = []\n",
    "\n",
    "# Process each gauge\n",
    "em='bc005'\n",
    "for gauge_num in gauge_nums:\n",
    "    # the gauges that didn't work for finding events\n",
    "    if gauge_num not in [444, 827, 888]:\n",
    "        # Excluding ones near windfarms\n",
    "        if gauge_num not in [27,36,57,61,75,97,101,106,120,132,190,204, 239, 285,348,376]:\n",
    "            if gauge_num % 100 == 0:\n",
    "                print(gauge_num)\n",
    "            # Create a list of all the event CSVs\n",
    "            files = [f for f in os.listdir(f\"../../ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/\") if f.endswith('.csv')]\n",
    "            files = np.sort(files)\n",
    "\n",
    "            # Process each file\n",
    "            for file in files:\n",
    "                fp = f\"../../ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/{file}\"\n",
    "                interpolated_rainfall, duration, real_duration, precipitation_sum, normalised_rainfall = process_file(gauge_num, fp)\n",
    "                if interpolated_rainfall is not None and duration is not None and precipitation_sum is not None:\n",
    "                    model_profiles.append(interpolated_rainfall)\n",
    "                    durations_for_model_profiles.append(duration)\n",
    "                    real_durations_for_model_profiles.append(real_duration)\n",
    "                    volumes_for_model_profiles.append(precipitation_sum)\n",
    "                    normalised_rainfalls.append(normalised_rainfall)\n",
    "                    heaviest_segments.append(categorize_normalized_rainstorm(interpolated_rainfall))\n",
    "                \n",
    "## Remove profiles containing NANs\n",
    "names =['model_profiles', 'model_durations_for_profiles','model_real_durations_for_profiles','model_volumes_for_profiles', 'heaviest_segments']\n",
    "for number, file in enumerate([model_profiles, durations_for_model_profiles, real_durations_for_model_profiles, volumes_for_model_profiles, heaviest_segments]):\n",
    "    file_name = names[number]\n",
    "    cache_filepath = f\"/nfs/a319/gy17m2a/PhD/ProcessedData/Profiles/UKCP18_30mins/{em}/{file_name}.pkl\"\n",
    "    with open(cache_filepath, 'wb') as f:\n",
    "        pickle.dump(file, f)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e57a8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121063\n",
      "121063\n",
      "121063\n",
      "121063\n"
     ]
    }
   ],
   "source": [
    "print(len(model_profiles))\n",
    "print(len(durations_for_model_profiles))\n",
    "print(len(volumes_for_model_profiles))\n",
    "print(len(real_durations_for_model_profiles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
