{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154ad2ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (493361138.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_29898/493361138.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    553, 858 has problems\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "553, 858 has problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1244bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb216\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2069.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2070.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2071.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2072.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2073.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2074.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2075.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2076.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2077.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2078.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2079.pkl\n",
      "Pickle file exists, so loading that\n",
      "(85, 241)\n",
      "gauge num is 958\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2069.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Time to load data is 226.08 seconds\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/0.5hrs_2069_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/1hrs_2069_v2_part0.csv\n",
      "Finding the AMAX for 2hr events for gauge 959 in year 2069\n",
      "Event doesnt contain NAN, total event precip is 23.906681060791016\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/2hrs_2069_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/3hrs_2069_v2_part0.csv\n",
      "Finding the AMAX for 6hr events for gauge 959 in year 2069\n",
      "Event doesnt contain NAN, total event precip is 87.09635162353516\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/6hrs_2069_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/12hrs_2069_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/24hrs_2069_v2_part0.csv\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2070.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Time to load data is 175.6 seconds\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/0.5hrs_2070_v2_part0.csv\n",
      "Finding the AMAX for 1hr events for gauge 959 in year 2070\n",
      "Event doesnt contain NAN, total event precip is 41.70130920410156\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/1hrs_2070_v2_part0.csv\n",
      "Finding the AMAX for 2hr events for gauge 959 in year 2070\n",
      "Event doesnt contain NAN, total event precip is 41.70130920410156\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/2hrs_2070_v2_part0.csv\n",
      "Finding the AMAX for 3hr events for gauge 959 in year 2070\n",
      "Event doesnt contain NAN, total event precip is 41.70130920410156\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/3hrs_2070_v2_part0.csv\n",
      "Finding the AMAX for 6hr events for gauge 959 in year 2070\n",
      "Event doesnt contain NAN, total event precip is 41.70130920410156\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/6hrs_2070_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/12hrs_2070_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/24hrs_2070_v2_part0.csv\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2071.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Time to load data is 259.39 seconds\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/0.5hrs_2071_v2_part0.csv\n",
      "Finding the AMAX for 1hr events for gauge 959 in year 2071\n",
      "Event doesnt contain NAN, total event precip is 71.4769058227539\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/1hrs_2071_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/2hrs_2071_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/3hrs_2071_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/6hrs_2071_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/12hrs_2071_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/24hrs_2071_v2_part0.csv\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2072.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Time to load data is 163.44 seconds\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/0.5hrs_2072_v2_part0.csv\n",
      "Finding the AMAX for 1hr events for gauge 959 in year 2072\n",
      "Event doesnt contain NAN, total event precip is 29.32965087890625\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/1hrs_2072_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/2hrs_2072_v2_part0.csv\n",
      "Finding the AMAX for 3hr events for gauge 959 in year 2072\n",
      "Event doesnt contain NAN, total event precip is 28.418682098388672\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/3hrs_2072_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/6hrs_2072_v2_part0.csv\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/12hrs_2072_v2_part0.csv\n",
      "Finding the AMAX for 24hr events for gauge 959 in year 2072\n",
      "Event doesnt contain NAN, total event precip is 64.92768859863281\n",
      "/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/Future/bb216/959/WholeYear/24hrs_2072_v2_part0.csv\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2073.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2074.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2075.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2076.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2077.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2078.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n",
      "/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_bb216/WholeYear/cube_2079.pkl\n",
      "Pickle file exists, so loading that\n",
      "(84, 237)\n",
      "gauge num is 959\n",
      "(17280, 519, 423)\n",
      "Files all already exist\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category =UserWarning,)\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj, transform\n",
    "import time\n",
    "\n",
    "from Identify_Events_Functions import *\n",
    "from Prepare_Data_Functions import *\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "yrs_range= '2060_2081'\n",
    "em = 'bb216'\n",
    "gauge_num = 950\n",
    "yr = 206\n",
    "timeperiod = 'Future'\n",
    "print(em)\n",
    "\n",
    "if timeperiod == 'Future':\n",
    "    sample_yr=2066\n",
    "elif timeperiod == 'Present':\n",
    "    sample_yr=2006\n",
    "\n",
    "for gauge_num in range(958, 960):\n",
    "    for yr in range(2069, 2080)   : \n",
    "        # Get Tb0 values at each gauge\n",
    "        tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "        # Read in a sample cube for finding the location of gauge in grid\n",
    "        sample_cube = iris.load(f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}/bng_{em}a.pr{sample_yr}01.nc')[0][1,:,:]\n",
    "\n",
    "        ######################################################\n",
    "        ### Get all the data for one year, into one cube\n",
    "        # (if it already exists in a pickle file, then load it from there)\n",
    "        ######################################################\n",
    "        general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}/bng_{em}a.pr{yr}*'\n",
    "        pickle_file_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_{em}/WholeYear/cube_{yr}.pkl\"\n",
    "        print(pickle_file_filepath)\n",
    "\n",
    "        if os.path.exists(pickle_file_filepath):\n",
    "            print(\"Pickle file exists, so loading that\")\n",
    "            full_year_cube = load_cube_from_picklefile(pickle_file_filepath)\n",
    "        else:\n",
    "            print(\"Pickle file doesnt exist, so creating and then saving that\")\n",
    "\n",
    "            ### Get the data filepaths\n",
    "            print(f\"Loading data for year {yr}\")\n",
    "\n",
    "            # Create cube list\n",
    "            cubes = load_files_to_cubelist(yr, general_filename)\n",
    "\n",
    "            # Join them into one (with error handling to deal with times which are wrong)\n",
    "            try:\n",
    "                full_year_cube = cubes.concatenate_cube()\n",
    "                print(\"Concatenation successful!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Initial concatenation failed: {str(e)}\")\n",
    "\n",
    "                # If initial concatenation fails, remove problematic cubes and try again\n",
    "                try:\n",
    "                    full_year_cube = remove_problematic_cubes(cubes)\n",
    "                    print(\"Concatenation successful after removing problematic cubes!\")\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Concatenation failed after removing problematic cubes: {str(e)}\")               \n",
    "            save_cube_as_pickle_file(full_year_cube, pickle_file_filepath)\n",
    "\n",
    "        # Find location\n",
    "        Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(tbo_vals, gauge_num, sample_cube)\n",
    "\n",
    "        # Function to process each gauge\n",
    "        print(f\"gauge num is {gauge_num}\")             \n",
    "\n",
    "        base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{timeperiod}/{em}/{gauge_num}/WholeYear\"\n",
    "        # Create the directory if it doesnt exist\n",
    "        if not os.path.isdir(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "        print(full_year_cube.shape)      \n",
    "        ######################################################\n",
    "        ## Check if any files are missing, across the 3 filtering options\n",
    "        # If there are: code will continue to run\n",
    "        # If not: code will move to next gauge\n",
    "        ######################################################                \n",
    "        # Create a flag to record whether we are missing any of the files we need\n",
    "        missing_files = False\n",
    "\n",
    "        # Check if we are missing any of the files, and if so, change the flag to True\n",
    "        if not all(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "            missing_files = True\n",
    "\n",
    "        # If we are missing some files then get the data for the grid cell, \n",
    "        if missing_files:\n",
    "\n",
    "            # Extract data for the specified indices\n",
    "            start= time.time()\n",
    "            one_location_cube = full_year_cube[:, idx_2d[0], idx_2d[1]]\n",
    "            data = one_location_cube.data\n",
    "            end=time.time()\n",
    "            print(f\"Time to load data is {round(end-start,2)} seconds\")\n",
    "\n",
    "            ##### Filter cube according to different options\n",
    "            # Convert to dataframe\n",
    "            df = pd.DataFrame(data, columns=['precipitation (mm/hr)'])\n",
    "            df['times'] = one_location_cube.coord('time').units.num2date(one_location_cube.coord('time').points)\n",
    "            df['precipitation (mm)'] = df['precipitation (mm/hr)'] / 2   \n",
    "\n",
    "            # Search dataframe for events corresponding to durations\n",
    "            for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "\n",
    "                filename =  f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\n",
    "                if not os.path.exists(filename):\n",
    "                    print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr}\")\n",
    "                    # Find events\n",
    "                    events_v2 = search_for_valid_events(df, duration=duration, Tb0=Tb0)\n",
    "\n",
    "                    # Save events to CSV\n",
    "                    for num, event in enumerate(events_v2):\n",
    "                        if len(event) > 1:\n",
    "                                event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                                print(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                                if event['precipitation (mm/hr)'].isna().any():\n",
    "                                    print(\"NANs in this event\")\n",
    "                else:\n",
    "                    print(f\"already exists{filename}\")\n",
    "                    pass   \n",
    "        else:\n",
    "            print(\"Files all already exist\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
