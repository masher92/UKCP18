{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd328e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category =UserWarning,)\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj, transform\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '../')\n",
    "from Identify_Events_Functions import *\n",
    "from Prepare_Data_Functions import *\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "######################################################\n",
    "### Define data for finding the indepedent events at each gauge\n",
    "######################################################\n",
    "yr = 2006\n",
    "\n",
    "filtering_name = 'filtered_100'\n",
    "\n",
    "# Get Tb0 values at each gauge\n",
    "tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "# Read in a sample cube for finding the location of gauge in grid\n",
    "sample_cube = iris.load(f'/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/NIMROD_regridded_2.2km/{filtering_name}/AreaWeighted/2012/rg_metoffice-c-band-rain-radar_uk_20120602_30mins.nc')[0][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91047fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Get all the 5 minute data for one year, into one cube\n",
    "# (if it already exists in a pickle file, then load it from there)\n",
    "######################################################\n",
    "general_filename = f'/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/NIMROD_regridded_2.2km/{filtering_name}/AreaWeighted/{yr}/*'.format(year=yr)\n",
    "pickle_file_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/nimrod_30mins_2.2km/{filtering_name}/WholeYear/cube_{yr}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5490f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file exists, so loading that\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(pickle_file_filepath):\n",
    "    print(\"Pickle file exists, so loading that\")\n",
    "    full_year_cube = load_cube_from_picklefile(pickle_file_filepath)\n",
    "else:\n",
    "    print(\"Pickle file doesnt exist, so creating and then saving that\")\n",
    "    \n",
    "    ### Get the data filepaths\n",
    "    print(f\"Loading data for year {yr}\")\n",
    "    \n",
    "    # Create cube list\n",
    "    cubes = load_files_to_cubelist(yr, general_filename)\n",
    "    \n",
    "    # Join them into one (with error handling to deal with times which are wrong)\n",
    "    try:\n",
    "        full_year_cube = cubes.concatenate_cube()\n",
    "        print(\"Concatenation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Initial concatenation failed: {str(e)}\")\n",
    "\n",
    "        # If initial concatenation fails, remove problematic cubes and try again\n",
    "        try:\n",
    "            full_year_cube = remove_problematic_cubes(cubes)\n",
    "            print(\"Concatenation successful after removing problematic cubes!\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Concatenation failed after removing problematic cubes: {str(e)}\")               \n",
    "    save_cube_as_pickle_file(full_year_cube, pickle_file_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauge num is 0\n",
      "Time to load data is 20.78 seconds\n",
      "Finding the AMAX for 0.5hr events for gauge 0 in year 2006\n",
      "len of full 0.5h window before trimming: 1\n",
      "len of full 0.5h window after trimming: 1\n",
      "1 event 1\n",
      "No overlapping times 0, 0.5 2006\n",
      "Finding the AMAX for 1hr events for gauge 0 in year 2006\n",
      "len of full 1h window before trimming: 2\n",
      "len of full 1h window after trimming: 2\n",
      "1 event 2\n",
      "No overlapping times 0, 1 2006\n",
      "Finding the AMAX for 2hr events for gauge 0 in year 2006\n",
      "len of full 2h window before trimming: 4\n",
      "len of full 2h window after trimming: 4\n",
      "1 event 4\n",
      "No overlapping times 0, 2 2006\n",
      "Finding the AMAX for 3hr events for gauge 0 in year 2006\n",
      "len of full 3h window before trimming: 6\n",
      "len of full 3h window after trimming: 6\n",
      "1 event 6\n",
      "No overlapping times 0, 3 2006\n",
      "Finding the AMAX for 6hr events for gauge 0 in year 2006\n",
      "len of full 6h window before trimming: 12\n",
      "len of full 6h window after trimming: 12\n",
      "1 event 12\n",
      "No overlapping times 0, 6 2006\n",
      "Finding the AMAX for 12hr events for gauge 0 in year 2006\n",
      "len of full 12h window before trimming: 24\n",
      "len of full 12h window after trimming: 22\n",
      "1 event 22\n",
      "No overlapping times 0, 12 2006\n",
      "Finding the AMAX for 24hr events for gauge 0 in year 2006\n",
      "len of full 24h window before trimming: 48\n",
      "len of full 24h window after trimming: 48\n",
      "2 events\n",
      "Before trimming: event1 is  18 , event2 is  31\n",
      "After trimming: event1 is  4 , event2 is  24\n",
      "No overlapping times 0, 24 2006\n",
      "gauge num is 1\n",
      "Time to load data is 174.26 seconds\n",
      "Finding the AMAX for 0.5hr events for gauge 1 in year 2006\n",
      "len of full 0.5h window before trimming: 1\n",
      "len of full 0.5h window after trimming: 1\n",
      "1 event 1\n",
      "No overlapping times 1, 0.5 2006\n",
      "Finding the AMAX for 1hr events for gauge 1 in year 2006\n",
      "len of full 1h window before trimming: 2\n",
      "len of full 1h window after trimming: 2\n",
      "1 event 2\n",
      "No overlapping times 1, 1 2006\n",
      "Finding the AMAX for 2hr events for gauge 1 in year 2006\n",
      "len of full 2h window before trimming: 4\n",
      "len of full 2h window after trimming: 4\n",
      "1 event 4\n",
      "No overlapping times 1, 2 2006\n",
      "Finding the AMAX for 3hr events for gauge 1 in year 2006\n",
      "len of full 3h window before trimming: 6\n",
      "len of full 3h window after trimming: 6\n",
      "1 event 6\n",
      "No overlapping times 1, 3 2006\n",
      "Finding the AMAX for 6hr events for gauge 1 in year 2006\n",
      "len of full 6h window before trimming: 12\n",
      "len of full 6h window after trimming: 12\n",
      "1 event 12\n",
      "No overlapping times 1, 6 2006\n",
      "Finding the AMAX for 12hr events for gauge 1 in year 2006\n",
      "len of full 12h window before trimming: 24\n",
      "len of full 12h window after trimming: 23\n",
      "1 event 23\n",
      "No overlapping times 1, 12 2006\n",
      "Finding the AMAX for 24hr events for gauge 1 in year 2006\n",
      "len of full 24h window before trimming: 48\n",
      "len of full 24h window after trimming: 23\n",
      "1 event 23\n",
      "No overlapping times 1, 24 2006\n",
      "gauge num is 2\n",
      "Time to load data is 23.26 seconds\n",
      "Finding the AMAX for 0.5hr events for gauge 2 in year 2006\n",
      "len of full 0.5h window before trimming: 1\n",
      "len of full 0.5h window after trimming: 1\n",
      "1 event 1\n",
      "No overlapping times 2, 0.5 2006\n",
      "Finding the AMAX for 1hr events for gauge 2 in year 2006\n",
      "len of full 1h window before trimming: 2\n",
      "len of full 1h window after trimming: 2\n",
      "1 event 2\n",
      "No overlapping times 2, 1 2006\n",
      "Finding the AMAX for 2hr events for gauge 2 in year 2006\n",
      "len of full 2h window before trimming: 4\n",
      "len of full 2h window after trimming: 4\n",
      "1 event 4\n",
      "No overlapping times 2, 2 2006\n",
      "Finding the AMAX for 3hr events for gauge 2 in year 2006\n",
      "len of full 3h window before trimming: 6\n",
      "len of full 3h window after trimming: 6\n",
      "1 event 6\n",
      "No overlapping times 2, 3 2006\n",
      "Finding the AMAX for 6hr events for gauge 2 in year 2006\n",
      "len of full 6h window before trimming: 12\n",
      "len of full 6h window after trimming: 11\n",
      "1 event 11\n",
      "No overlapping times 2, 6 2006\n",
      "Finding the AMAX for 12hr events for gauge 2 in year 2006\n",
      "len of full 12h window before trimming: 24\n",
      "len of full 12h window after trimming: 24\n",
      "1 event 24\n",
      "No overlapping times 2, 12 2006\n",
      "Finding the AMAX for 24hr events for gauge 2 in year 2006\n",
      "len of full 24h window before trimming: 48\n",
      "len of full 24h window after trimming: 46\n",
      "1 event 46\n",
      "No overlapping times 2, 24 2006\n",
      "gauge num is 3\n"
     ]
    }
   ],
   "source": [
    "gauge_nums = range(0,1293)\n",
    "# Function to process each gauge\n",
    "for gauge_num in gauge_nums:\n",
    "    if not gauge_num in [423, 444, 827, 888]:\n",
    "        print(f\"gauge num is {gauge_num}\")\n",
    "\n",
    "        # Find location\n",
    "        Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(tbo_vals, gauge_num, sample_cube)\n",
    "\n",
    "        ######################################################\n",
    "        ## Check if any files are missing, across the 3 filtering options\n",
    "        # If there are: code will continue to run\n",
    "        # If not: code will move to next gauge\n",
    "        ######################################################\n",
    "        # Create a flag to record whether we are missing any of the files we need\n",
    "        missing_files = False\n",
    "        # Define directory filepath which will store results\n",
    "        base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/NIMROD_30mins/NIMROD_2.2km_{filtering_name}_New/{gauge_num}/WholeYear\"\n",
    "\n",
    "        # Create the directory if it doesnt exist\n",
    "        if not os.path.isdir(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "        # Check if we are missing any of the files, and if so, change the flag to True\n",
    "        if not any(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "            missing_files = True\n",
    "\n",
    "        # If we are missing some files then get the data for the grid cell, \n",
    "        if missing_files:\n",
    "\n",
    "            # Extract data for the specified indices\n",
    "            start= time.time()\n",
    "            one_location_cube = full_year_cube[:, idx_2d[0], idx_2d[1]]\n",
    "            data = one_location_cube.data\n",
    "            end=time.time()\n",
    "            print(f\"Time to load data is {round(end-start,2)} seconds\")\n",
    "\n",
    "            ##### Filter cube according to different options\n",
    "            # Convert to dataframe\n",
    "            df = pd.DataFrame(data, columns=['precipitation (mm/hr)'])\n",
    "            df['times'] = one_location_cube.coord('time').units.num2date(one_location_cube.coord('time').points)\n",
    "            df['precipitation (mm)'] = df['precipitation (mm/hr)'] / 2   \n",
    "\n",
    "            # Search dataframe for events corresponding to durations\n",
    "            for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "                base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/NIMROD_30mins/NIMROD_2.2km_{filtering_name}_New/{gauge_num}/WholeYear\"\n",
    "                filename =  f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\n",
    "                if not os.path.exists(filename):\n",
    "                    print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr}\")\n",
    "                    # Find events\n",
    "                    # events_v2 = search_for_valid_events(df, duration=duration, Tb0=Tb0)\n",
    "                    events_v2 = find_amax_indy_events_v2(df, duration=duration, Tb0=Tb0, gauge_num=gauge_num, yr=yr)\n",
    "\n",
    "                    # Save events to CSV\n",
    "                    for num, event in enumerate(events_v2):\n",
    "                        event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                        if event['precipitation (mm/hr)'].isna().any():\n",
    "                            print(\"NANs in this event\")\n",
    "                else:\n",
    "                    print(f\"already exists{filename}\")\n",
    "                    pass   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
