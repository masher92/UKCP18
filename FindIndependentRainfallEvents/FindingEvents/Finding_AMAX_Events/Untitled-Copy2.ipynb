{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category =UserWarning,)\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj, transform\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '../')\n",
    "from Identify_Events_Functions import *\n",
    "from Prepare_Data_Functions import *\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "yrs_range= '2060_2081' #'2002_2020'\n",
    "em = 'bb198'\n",
    "yr=2066\n",
    "\n",
    "# Get Tb0 values at each gauge\n",
    "tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "# Read in a sample cube for finding the location of gauge in grid\n",
    "sample_cube = iris.load(f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}/bng_{em}a.pr{yr}01.nc')[0][1,:,:]\n",
    "\n",
    "######################################################\n",
    "### Get all the data for one year, into one cube\n",
    "# (if it already exists in a pickle file, then load it from there)\n",
    "######################################################\n",
    "general_filename = f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}/bng_{em}a.pr{yr}*'\n",
    "pickle_file_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_{em}//WholeYear/cube_{yr}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c953b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file exists, so loading that\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(pickle_file_filepath):\n",
    "    print(\"Pickle file exists, so loading that\")\n",
    "    full_year_cube = load_cube_from_picklefile(pickle_file_filepath)\n",
    "else:\n",
    "    print(\"Pickle file doesnt exist, so creating and then saving that\")\n",
    "    \n",
    "    ### Get the data filepaths\n",
    "    print(f\"Loading data for year {yr}\")\n",
    "    \n",
    "    # Create cube list\n",
    "    cubes = load_files_to_cubelist(yr, general_filename)\n",
    "    \n",
    "    # Join them into one (with error handling to deal with times which are wrong)\n",
    "    try:\n",
    "        full_year_cube = cubes.concatenate_cube()\n",
    "        print(\"Concatenation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Initial concatenation failed: {str(e)}\")\n",
    "\n",
    "        # If initial concatenation fails, remove problematic cubes and try again\n",
    "        try:\n",
    "            full_year_cube = remove_problematic_cubes(cubes)\n",
    "            print(\"Concatenation successful after removing problematic cubes!\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Concatenation failed after removing problematic cubes: {str(e)}\")               \n",
    "    save_cube_as_pickle_file(full_year_cube, pickle_file_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c0252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_amax_indy_events_v2(df, duration, Tb0, gauge_num, yr):\n",
    "    rainfall_cores = find_rainfall_core(df, duration=duration, Tb0=Tb0)\n",
    "    rainfall_events_expanded = []\n",
    "\n",
    "    for rainfall_core in rainfall_cores:\n",
    "        print(rainfall_core)\n",
    "        rainfall_core_after_search1 = search1(df, rainfall_core)\n",
    "        rainfall_core_after_search2 = search2(df, rainfall_core_after_search1)\n",
    "        rainfall_core_after_search3 = search3(df, rainfall_core_after_search2, Tb0=Tb0)\n",
    "        if len(rainfall_core_after_search3[rainfall_core_after_search3['precipitation (mm/hr)'] > 0.1]) > 0:\n",
    "            rainfall_events_expanded.append(rainfall_core_after_search3)\n",
    "    \n",
    "    # Collect all 'times' values from each DataFrame\n",
    "    all_times = pd.Series(dtype='datetime64[ns]')  # Create an empty series with datetime type\n",
    "\n",
    "    for a_df in rainfall_events_expanded:\n",
    "        all_times = all_times.append(a_df['times'], ignore_index=True)\n",
    "\n",
    "    # Check for duplicates and raise an exception if any are found\n",
    "    if all_times.duplicated().any():\n",
    "        raise ValueError(f\"Overlapping times detected in the DataFrames {em}, {duration} {yr}\")\n",
    "    else:\n",
    "        print(f\"No overlapping times {em}, {duration} {yr}\")\n",
    "    \n",
    "        return rainfall_events_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb197a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauge num is 1169\n",
      "Time to load data is 321.37 seconds\n",
      "0.5\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/0.5hrs_2066_v2_part0.csv\n",
      "1\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/1hrs_2066_v2_part0.csv\n",
      "2\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/2hrs_2066_v2_part0.csv\n",
      "3\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/3hrs_2066_v2_part0.csv\n",
      "6\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/6hrs_2066_v2_part0.csv\n",
      "12\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/12hrs_2066_v2_part0.csv\n",
      "24\n",
      "already exists/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/bb198/1169/WholeYear/24hrs_2066_v2_part0.csv\n"
     ]
    }
   ],
   "source": [
    "failed_gauges = []\n",
    "gauge_nums = range(1169,1170)\n",
    "# Function to process each gauge\n",
    "for gauge_num in gauge_nums:\n",
    "    if not gauge_num in [423, 444, 827, 888]:\n",
    "            print(f\"gauge num is {gauge_num}\")\n",
    "            \n",
    "            # Find location\n",
    "            Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(tbo_vals, gauge_num, sample_cube)\n",
    "            \n",
    "            ######################################################\n",
    "            ## Check if any files are missing, across the 3 filtering options\n",
    "            # If there are: code will continue to run\n",
    "            # If not: code will move to next gauge\n",
    "            ######################################################\n",
    "            # Create a flag to record whether we are missing any of the files we need\n",
    "            missing_files = False\n",
    "            # Define directory filepath which will store results\n",
    "            base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/WholeYear\"\n",
    "            \n",
    "            # Create the directory if it doesnt exist\n",
    "            if not os.path.isdir(base_dir):\n",
    "                os.makedirs(base_dir)\n",
    "            # Check if we are missing any of the files, and if so, change the flag to True\n",
    "            if not all(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "                missing_files = True\n",
    "                \n",
    "            # If we are missing some files then get the data for the grid cell, \n",
    "            if missing_files:\n",
    "        \n",
    "                # Extract data for the specified indices\n",
    "                start= time.time()\n",
    "                one_location_cube = full_year_cube[:, idx_2d[0], idx_2d[1]]\n",
    "                data = one_location_cube.data\n",
    "                end=time.time()\n",
    "                print(f\"Time to load data is {round(end-start,2)} seconds\")\n",
    "                \n",
    "                ##### Filter cube according to different options\n",
    "                # Convert to dataframe\n",
    "                df = pd.DataFrame(data, columns=['precipitation (mm/hr)'])\n",
    "                df['times'] = one_location_cube.coord('time').units.num2date(one_location_cube.coord('time').points)\n",
    "                df['precipitation (mm)'] = df['precipitation (mm/hr)'] / 2   \n",
    "                \n",
    "                # Search dataframe for events corresponding to durations\n",
    "                for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "                    print(duration)\n",
    "                    base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/WholeYear\"\n",
    "\n",
    "                    filename =  f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\n",
    "                    if not os.path.exists(filename):\n",
    "                        print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr}\")\n",
    "                        # Find events\n",
    "                        # events_v2 = search_for_valid_events(df, duration=duration, Tb0=Tb0)\n",
    "                        events_v2 = find_amax_indy_events_v2(df, duration=duration, Tb0=Tb0, gauge_num=gauge_num, yr=yr)\n",
    "\n",
    "                        # Save events to CSV\n",
    "                        for num, event in enumerate(events_v2):\n",
    "                            event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                            if event['precipitation (mm/hr)'].isna().any():\n",
    "                                print(\"NANs in this event\")\n",
    "                    else:\n",
    "                        print(f\"already exists{filename}\")\n",
    "                        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87c6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'em' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_190624/1126571900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/WholeYear\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34mf\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'em' is not defined"
     ]
    }
   ],
   "source": [
    "for duration in [12]:\n",
    "        print(duration)\n",
    "        base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{em}/{gauge_num}/WholeYear\"\n",
    "\n",
    "        filename =  f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr}\")\n",
    "            # Find events\n",
    "            # events_v2 = search_for_valid_events(df, duration=duration, Tb0=Tb0)\n",
    "            events_v2 = find_amax_indy_events_v2(df, duration=duration, Tb0=Tb0, gauge_num=gauge_num, yr=yr)\n",
    "\n",
    "            # Save events to CSV\n",
    "            for num, event in enumerate(events_v2):\n",
    "                # event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                if event['precipitation (mm/hr)'].isna().any():\n",
    "                    print(\"NANs in this event\")\n",
    "        else:\n",
    "            print(f\"already exists{filename}\")\n",
    "            pass   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
