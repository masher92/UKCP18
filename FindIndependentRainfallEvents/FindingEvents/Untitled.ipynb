{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc3864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file exists, so loading that\n",
      "gauge num is 500\n",
      "(690, 670)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "from Identify_Events_Functions import *\n",
    "from Prepare_Data_Functions import *\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def filtered_cube (cube, filter_above):\n",
    "    # cube = cube.copy()\n",
    "    cube.data = np.where(cube.data < 0, np.nan, cube.data)\n",
    "    cube.data = np.where(cube.data > filter_above, np.nan, cube.data)\n",
    "    return cube \n",
    "\n",
    "######################################################\n",
    "### Define which rainfall data we are looking for events in\n",
    "######################################################\n",
    "dataset_name = 'filtered_100'\n",
    "dataset_path_pattern = '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/5mins/OriginalFormat_1km/{year}/*'\n",
    "\n",
    "######################################################\n",
    "### Define data for finding the indepedent events at each gauge\n",
    "######################################################\n",
    "yr = 2009\n",
    "\n",
    "# Get Tb0 values at each gauge\n",
    "tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "# Read in a sample cube for finding the location of gauge in grid\n",
    "sample_cube = iris.load(f'/nfs/a161/gy17m2a/PhD/datadir/NIMROD/5mins/OriginalFormat_1km/{yr}/metoffice-c-band-rain-radar_uk_{yr}0602.nc')[0][1,:,:]\n",
    "\n",
    "######################################################\n",
    "### Get all the 5 minute data for one year, into one cube\n",
    "# (if it already exists in a pickle file, then load it from there)\n",
    "######################################################\n",
    "general_filename = dataset_path_pattern.format(year=yr)\n",
    "pickle_file_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/nimrod_5mins/unfiltered/WholeYear/cube_{yr}.pkl\"\n",
    "\n",
    "if os.path.exists(pickle_file_filepath):\n",
    "    print(\"Pickle file exists, so loading that\")\n",
    "    full_year_cube = load_cube_from_picklefile(pickle_file_filepath)\n",
    "else:\n",
    "    print(\"Pickle file doesnt exist, so creating and then saving that\")\n",
    "    \n",
    "    ### Get the data filepaths\n",
    "    print(f\"Loading data for year {yr}\")\n",
    "    \n",
    "    # Create cube list\n",
    "    cubes = load_files_to_cubelist(yr, general_filename)\n",
    "    \n",
    "    # Clean cubes of things which are problematic for concatenation\n",
    "    cubes = clean_cubes(cubes)\n",
    "    \n",
    "    # Join them into one (with error handling to deal with times which are wrong)\n",
    "    try:\n",
    "        full_year_cube = cubes.concatenate_cube()\n",
    "        print(\"Concatenation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Initial concatenation failed: {str(e)}\")\n",
    "\n",
    "        # If initial concatenation fails, remove problematic cubes and try again\n",
    "        try:\n",
    "            full_year_cube = remove_problematic_cubes(cubes)\n",
    "            print(\"Concatenation successful after removing problematic cubes!\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Concatenation failed after removing problematic cubes: {str(e)}\")               \n",
    "    save_cube_as_pickle_file(full_year_cube, pickle_file_filepath)\n",
    "\n",
    "######################################################\n",
    "# Find events at each gauge\n",
    "######################################################\n",
    "failed_gauges = []\n",
    "gauge_nums = range(500,1263)\n",
    "# Function to process each gauge\n",
    "for gauge_num in gauge_nums:\n",
    "    if not gauge_num in [423, 444, 827, 888]:\n",
    "            print(f\"gauge num is {gauge_num}\")\n",
    "            \n",
    "            ######################################################\n",
    "            ## Check if any files are missing, across the 3 filtering options\n",
    "            # If there are: code will continue to run\n",
    "            # If not: code will move to next gauge\n",
    "            ######################################################\n",
    "            for dataset_name in ['unfiltered', 'filtered_100', 'filtered_300']:\n",
    "                # Create a flag to record whether we are missing any of the files we need\n",
    "                missing_files = False\n",
    "                # Define directory filepath which will store results\n",
    "                base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/NIMROD_5mins/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear\"\n",
    "                # Create the directory if it doesnt exist\n",
    "                if not os.path.isdir(base_dir):\n",
    "                    os.makedirs(base_dir)\n",
    "                # Check if we are missing any of the files, and if so, change the flag to True\n",
    "                if not any(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "                    missing_files = True\n",
    "                \n",
    "            # If we are missing some files then get the data for the grid cell, \n",
    "            if missing_files:\n",
    "                # Find the Tb0 and index of this gauge\n",
    "                Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(tbo_vals, gauge_num, sample_cube)\n",
    "\n",
    "                # Extract data for the specified indices\n",
    "                start= time.time()\n",
    "                one_location_cube = full_year_cube[:, idx_2d[0], idx_2d[1]]\n",
    "                data = one_location_cube.data\n",
    "                end=time.time()\n",
    "                print(f\"Time to load data is {round(end-start,2)} seconds\")\n",
    "\n",
    "                ##### Filter cube according to different options\n",
    "                # Find events with filtered cubes\n",
    "                filtering_dict = {1000000:'unfiltered', 300:'filtered_300',100:'filtered_100'}\n",
    "                for filtering_key, dataset_name in filtering_dict.items():\n",
    "                    print(f\"running for {dataset_name}\")\n",
    "                    # Create cube with filterings applied\n",
    "                    cube = filtered_cube(one_location_cube,  filter_above=filtering_key)\n",
    "                    print(\"reloading data\")\n",
    "                    data = cube.data\n",
    "                    print(f\"max value is {np.nanmax(cube.data)}\")\n",
    "                    # Convert to dataframe\n",
    "                    df = create_df_with_gaps_filled_in(cube, data, time_resolution = 5)\n",
    "                    # Search dataframe for events corresponding to durations\n",
    "                    for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "                        base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/NIMROD_5mins/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear\"\n",
    "\n",
    "                        filename =  f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\n",
    "                        if not os.path.exists(filename):\n",
    "                            print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr} for {dataset_name}\")\n",
    "                            # Find events\n",
    "                            events_v2 = search_for_valid_events(df, duration=duration, Tb0=Tb0)\n",
    "\n",
    "                            # Save events to CSV\n",
    "                            for num, event in enumerate(events_v2):\n",
    "                                if len(event) > 1:\n",
    "                                        event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "                                        if event['precipitation (mm/hr)'].isna().any():\n",
    "                                            print(\"NANs in this event\")\n",
    "                        else:\n",
    "                            print(f\"already exists{filename}\")\n",
    "                            pass   \n",
    "\n",
    "print(f\"failed gauges are: {failed_gauges}\")\n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
