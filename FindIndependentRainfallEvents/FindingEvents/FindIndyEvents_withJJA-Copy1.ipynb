{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956ce088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category =UserWarning,)\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "from Identify_Events_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1576b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "def find_amax_indy_events_v2 (df, duration, Tb0):\n",
    "    \n",
    "    rainfall_cores = find_rainfall_core(df, duration=duration, Tb0= Tb0)\n",
    "    rainfall_events_expanded= []\n",
    "\n",
    "    for rainfall_core in rainfall_cores:\n",
    "        rainfall_core_after_search1 = search1(df, rainfall_core)\n",
    "        rainfall_core_after_search2 = search2(df, rainfall_core_after_search1)\n",
    "        rainfall_core_after_search3 = search3(df, rainfall_core_after_search2, Tb0= Tb0)\n",
    "        # If the event is not entirely dry \n",
    "        if len(rainfall_core_after_search3[rainfall_core_after_search3['precipitation (mm/hr)']>0.1]) >0:\n",
    "            rainfall_events_expanded.append(rainfall_core_after_search3)\n",
    "    \n",
    "    return rainfall_events_expanded\n",
    "\n",
    "def find_gauge_Tb0_and_location_in_grid (gauge_num, sample_cube):\n",
    "    # Get data just for this gauge\n",
    "    gauge1 = tbo_vals.iloc[gauge_num]\n",
    "    # Find the interevent arrival time (Tb0)\n",
    "    Tb0 = int(gauge1['Critical_interarrival_time'])\n",
    "    # Find the coordinates of the cell containing this gauge\n",
    "    closest_point, idx_2d = find_position_obs(sample_cube,gauge1['Lat'], gauge1['Lon'], plot_radius = 30, plot=False)\n",
    "    \n",
    "    return Tb0, idx_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ce427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_cube(year, cache, filenames_pattern):\n",
    "    if year in cache:\n",
    "        print(f\"Using cached data for year {year}\")\n",
    "        return cache[year]\n",
    "\n",
    "    print(f\"Loading data for year {year}\")\n",
    "    filenames = [filename for filename in glob.glob(filenames_pattern) if f'pr{year}' in filename]\n",
    "\n",
    "    if not filenames:\n",
    "        raise FileNotFoundError(f\"No files found for the year {year} with pattern {filenames_pattern}\")\n",
    "\n",
    "    cubes = iris.load(filenames)\n",
    "    try:\n",
    "        cube = cubes.concatenate_cube()\n",
    "    except iris.exceptions.ConcatenateError as e:\n",
    "        # Handle concatenation error by unifying metadata\n",
    "        for c in cubes:\n",
    "            c.rename(cubes[0].name())\n",
    "        cube = cubes.merge_cube()\n",
    "    \n",
    "    cache[year] = cube\n",
    "\n",
    "    return cube\n",
    "\n",
    "def save_cube_to_disk(cube, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(cube, f)\n",
    "\n",
    "def load_cube_from_disk(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# Custom limited-size cache\n",
    "class LimitedSizeDict(OrderedDict):\n",
    "    def __init__(self, *args, max_size=100, **kwargs):\n",
    "        self.max_size = max_size\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if len(self) >= self.max_size:\n",
    "            self.popitem(last=False)\n",
    "        OrderedDict.__setitem__(self, key, value)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db473fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2009\n",
      "Processing gauge 0\n",
      "All files already exist for gauge 0 and year 2009\n",
      "Processing gauge 1\n",
      "All files already exist for gauge 1 and year 2009\n",
      "Processing gauge 2\n",
      "All files already exist for gauge 2 and year 2009\n",
      "Processing gauge 3\n",
      "All files already exist for gauge 3 and year 2009\n",
      "Processing gauge 4\n",
      "All files already exist for gauge 4 and year 2009\n",
      "Processing gauge 5\n",
      "All files already exist for gauge 5 and year 2009\n",
      "Processing gauge 6\n",
      "All files already exist for gauge 6 and year 2009\n",
      "Processing gauge 7\n",
      "All files already exist for gauge 7 and year 2009\n",
      "Processing gauge 8\n",
      "All files already exist for gauge 8 and year 2009\n",
      "Processing gauge 9\n",
      "All files already exist for gauge 9 and year 2009\n",
      "Processing gauge 10\n",
      "All files already exist for gauge 10 and year 2009\n",
      "Processing gauge 11\n",
      "All files already exist for gauge 11 and year 2009\n",
      "Processing gauge 12\n",
      "All files already exist for gauge 12 and year 2009\n",
      "Processing gauge 13\n",
      "All files already exist for gauge 13 and year 2009\n",
      "Processing gauge 14\n",
      "All files already exist for gauge 14 and year 2009\n",
      "Processing gauge 15\n",
      "All files already exist for gauge 15 and year 2009\n",
      "Processing gauge 16\n",
      "All files already exist for gauge 16 and year 2009\n",
      "Processing gauge 17\n",
      "All files already exist for gauge 17 and year 2009\n",
      "Processing gauge 18\n",
      "All files already exist for gauge 18 and year 2009\n",
      "Processing gauge 19\n",
      "All files already exist for gauge 19 and year 2009\n",
      "Processing gauge 20\n",
      "All files already exist for gauge 20 and year 2009\n",
      "Processing gauge 21\n",
      "All files already exist for gauge 21 and year 2009\n",
      "Processing gauge 22\n",
      "All files already exist for gauge 22 and year 2009\n",
      "Processing gauge 23\n",
      "All files already exist for gauge 23 and year 2009\n",
      "Processing gauge 24\n",
      "All files already exist for gauge 24 and year 2009\n",
      "Processing gauge 25\n",
      "All files already exist for gauge 25 and year 2009\n",
      "Processing gauge 26\n",
      "All files already exist for gauge 26 and year 2009\n",
      "Processing gauge 27\n",
      "All files already exist for gauge 27 and year 2009\n",
      "Processing gauge 28\n",
      "All files already exist for gauge 28 and year 2009\n",
      "Processing gauge 29\n",
      "All files already exist for gauge 29 and year 2009\n",
      "Processing gauge 30\n",
      "All files already exist for gauge 30 and year 2009\n",
      "Processing gauge 31\n",
      "All files already exist for gauge 31 and year 2009\n",
      "Processing gauge 32\n",
      "All files already exist for gauge 32 and year 2009\n",
      "Processing gauge 33\n",
      "All files already exist for gauge 33 and year 2009\n",
      "Processing gauge 34\n",
      "All files already exist for gauge 34 and year 2009\n",
      "Processing gauge 35\n",
      "All files already exist for gauge 35 and year 2009\n",
      "Processing gauge 36\n",
      "All files already exist for gauge 36 and year 2009\n",
      "Processing gauge 37\n",
      "All files already exist for gauge 37 and year 2009\n",
      "Processing gauge 38\n",
      "All files already exist for gauge 38 and year 2009\n",
      "Processing gauge 39\n",
      "All files already exist for gauge 39 and year 2009\n",
      "Processing gauge 40\n",
      "All files already exist for gauge 40 and year 2009\n",
      "Processing gauge 41\n",
      "All files already exist for gauge 41 and year 2009\n",
      "Processing gauge 42\n",
      "All files already exist for gauge 42 and year 2009\n",
      "Processing gauge 43\n",
      "All files already exist for gauge 43 and year 2009\n",
      "Processing gauge 44\n",
      "All files already exist for gauge 44 and year 2009\n",
      "Processing gauge 45\n",
      "All files already exist for gauge 45 and year 2009\n",
      "Processing gauge 46\n",
      "All files already exist for gauge 46 and year 2009\n",
      "Processing gauge 47\n",
      "All files already exist for gauge 47 and year 2009\n",
      "Processing gauge 48\n",
      "All files already exist for gauge 48 and year 2009\n",
      "Processing gauge 49\n",
      "All files already exist for gauge 49 and year 2009\n",
      "Processing gauge 50\n",
      "All files already exist for gauge 50 and year 2009\n",
      "Processing gauge 51\n",
      "All files already exist for gauge 51 and year 2009\n",
      "Processing gauge 52\n",
      "All files already exist for gauge 52 and year 2009\n",
      "Processing gauge 53\n",
      "All files already exist for gauge 53 and year 2009\n",
      "Processing gauge 54\n",
      "All files already exist for gauge 54 and year 2009\n",
      "Processing gauge 55\n",
      "All files already exist for gauge 55 and year 2009\n",
      "Processing gauge 56\n",
      "All files already exist for gauge 56 and year 2009\n",
      "Processing gauge 57\n",
      "All files already exist for gauge 57 and year 2009\n",
      "Processing gauge 58\n",
      "All files already exist for gauge 58 and year 2009\n",
      "Processing gauge 59\n",
      "All files already exist for gauge 59 and year 2009\n",
      "Processing gauge 60\n",
      "All files already exist for gauge 60 and year 2009\n",
      "Processing gauge 61\n",
      "All files already exist for gauge 61 and year 2009\n",
      "Processing gauge 62\n",
      "All files already exist for gauge 62 and year 2009\n",
      "Processing gauge 63\n",
      "All files already exist for gauge 63 and year 2009\n",
      "Processing gauge 64\n",
      "All files already exist for gauge 64 and year 2009\n",
      "Processing gauge 65\n",
      "All files already exist for gauge 65 and year 2009\n",
      "Processing gauge 66\n",
      "All files already exist for gauge 66 and year 2009\n",
      "Processing gauge 67\n",
      "All files already exist for gauge 67 and year 2009\n",
      "Processing gauge 68\n",
      "All files already exist for gauge 68 and year 2009\n",
      "Processing gauge 69\n",
      "All files already exist for gauge 69 and year 2009\n",
      "Processing gauge 70\n",
      "All files already exist for gauge 70 and year 2009\n",
      "Processing gauge 71\n",
      "All files already exist for gauge 71 and year 2009\n",
      "Processing gauge 72\n",
      "All files already exist for gauge 72 and year 2009\n",
      "Processing gauge 73\n",
      "All files already exist for gauge 73 and year 2009\n",
      "Processing gauge 74\n",
      "All files already exist for gauge 74 and year 2009\n",
      "Processing gauge 75\n",
      "All files already exist for gauge 75 and year 2009\n",
      "Processing gauge 76\n",
      "All files already exist for gauge 76 and year 2009\n",
      "Processing gauge 77\n",
      "All files already exist for gauge 77 and year 2009\n",
      "Processing gauge 78\n",
      "All files already exist for gauge 78 and year 2009\n",
      "Processing gauge 79\n",
      "All files already exist for gauge 79 and year 2009\n",
      "Processing gauge 80\n",
      "All files already exist for gauge 80 and year 2009\n",
      "Processing gauge 81\n",
      "All files already exist for gauge 81 and year 2009\n",
      "Processing gauge 82\n",
      "All files already exist for gauge 82 and year 2009\n",
      "Processing gauge 83\n",
      "All files already exist for gauge 83 and year 2009\n",
      "Processing gauge 84\n",
      "All files already exist for gauge 84 and year 2009\n",
      "Processing gauge 85\n",
      "All files already exist for gauge 85 and year 2009\n",
      "Processing gauge 86\n",
      "All files already exist for gauge 86 and year 2009\n",
      "Processing gauge 87\n",
      "All files already exist for gauge 87 and year 2009\n",
      "Processing gauge 88\n",
      "All files already exist for gauge 88 and year 2009\n",
      "Processing gauge 89\n",
      "All files already exist for gauge 89 and year 2009\n",
      "Processing gauge 90\n",
      "All files already exist for gauge 90 and year 2009\n",
      "Processing gauge 91\n",
      "All files already exist for gauge 91 and year 2009\n",
      "Processing gauge 92\n",
      "All files already exist for gauge 92 and year 2009\n",
      "Processing gauge 93\n",
      "All files already exist for gauge 93 and year 2009\n",
      "Processing gauge 94\n",
      "All files already exist for gauge 94 and year 2009\n",
      "Processing gauge 95\n",
      "All files already exist for gauge 95 and year 2009\n",
      "Processing gauge 96\n",
      "All files already exist for gauge 96 and year 2009\n",
      "Processing gauge 97\n",
      "All files already exist for gauge 97 and year 2009\n",
      "Processing gauge 98\n",
      "All files already exist for gauge 98 and year 2009\n",
      "Processing gauge 99\n",
      "All files already exist for gauge 99 and year 2009\n",
      "Processing gauge 100\n",
      "All files already exist for gauge 100 and year 2009\n",
      "Processing gauge 101\n",
      "All files already exist for gauge 101 and year 2009\n",
      "Processing gauge 102\n",
      "All files already exist for gauge 102 and year 2009\n",
      "Processing gauge 103\n",
      "All files already exist for gauge 103 and year 2009\n",
      "Processing gauge 104\n",
      "All files already exist for gauge 104 and year 2009\n",
      "Processing gauge 105\n",
      "All files already exist for gauge 105 and year 2009\n",
      "Processing gauge 106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_filtered_100/106/WholeYear//0.5hrs_2009_v2_part0.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_241022/3290882053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                     \u001b[0mmissing_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_241022/3290882053.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                     \u001b[0mmissing_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "import iris.plot as iplt\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyproj import Proj, transform\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from netCDF4 import Dataset\n",
    "from Identify_Events_Functions import *\n",
    "import concurrent.futures  # For parallel processing\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Custom limited-size cache\n",
    "class LimitedSizeDict(OrderedDict):\n",
    "    def __init__(self, *args, max_size=100, **kwargs):\n",
    "        self.max_size = max_size\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if len(self) >= self.max_size:\n",
    "            self.popitem(last=False)\n",
    "        OrderedDict.__setitem__(self, key, value)\n",
    "\n",
    "# Initialize a limited-size cache\n",
    "cubes_cache = {\n",
    "    'unfiltered': LimitedSizeDict(max_size=10),\n",
    "    'filtered_100': LimitedSizeDict(max_size=10),\n",
    "    'filtered_300': LimitedSizeDict(max_size=10)\n",
    "}\n",
    "\n",
    "def concatenate_with_error_handling(cube_list):\n",
    "    problematic_cube_index = None\n",
    "    start = 0\n",
    "    for i, cube in enumerate(cube_list):\n",
    "        try:\n",
    "            concatenated_cube = cube_list[start:i+1].concatenate_cube()\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating cube {i}: {str(e)}\")\n",
    "            problematic_cube_index = i\n",
    "            start = i\n",
    "    \n",
    "    if 0 <= problematic_cube_index < len(cube_list):\n",
    "        del cube_list[problematic_cube_index]\n",
    "        print(f\"Cube at index {problematic_cube_index} successfully removed from the CubeList.\")\n",
    "    else:\n",
    "        print(f\"Index {problematic_cube_index} is out of range for CubeList.\")\n",
    "    \n",
    "    concatenated_cube = cube_list.concatenate_cube() \n",
    "    return concatenated_cube\n",
    "\n",
    "def load_and_cache_cube(year, cache, filenames_pattern):\n",
    "    if year in cache:\n",
    "        print(f\"Using cached data for year {year}\")\n",
    "        return cache[year]\n",
    "\n",
    "    print(f\"Loading data for year {year}\")\n",
    "    filenames = [filename for filename in glob.glob(filenames_pattern) if '.nc' in filename]\n",
    "\n",
    "    if not filenames:\n",
    "        raise FileNotFoundError(f\"No files found for the year {year} with pattern {filenames_pattern}\")\n",
    "\n",
    "    cubes = iris.load(filenames)\n",
    "    for cube in cubes:\n",
    "        cube.rename(cubes[0].name())\n",
    "    iris.util.equalise_attributes(cubes) \n",
    "    \n",
    "    try:\n",
    "        full_day_cube = cubes.concatenate_cube()\n",
    "    except:\n",
    "        print(\"Error handling concatenation\")\n",
    "        full_day_cube = concatenate_with_error_handling(cubes)\n",
    "    \n",
    "    cache[year] = full_day_cube\n",
    "    return full_day_cube\n",
    "\n",
    "def save_cube_to_disk(cube, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(cube, f)\n",
    "\n",
    "def load_cube_from_disk(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def find_gauge_Tb0_and_location_in_grid(gauge_num, sample_cube):\n",
    "    gauge1 = tbo_vals.iloc[gauge_num]\n",
    "    Tb0 = int(gauge1['Critical_interarrival_time'])\n",
    "    closest_point, idx_2d = find_position_obs(sample_cube, gauge1['Lat'], gauge1['Lon'], plot_radius=10, plot=False)\n",
    "    return Tb0, idx_2d\n",
    "\n",
    "def find_amax_indy_events_v2(df, duration, Tb0):\n",
    "    rainfall_cores = find_rainfall_core(df, duration=duration, Tb0=Tb0)\n",
    "    rainfall_events_expanded = []\n",
    "\n",
    "    for rainfall_core in rainfall_cores:\n",
    "        rainfall_core_after_search1 = search1(df, rainfall_core)\n",
    "        rainfall_core_after_search2 = search2(df, rainfall_core_after_search1)\n",
    "        rainfall_core_after_search3 = search3(df, rainfall_core_after_search2, Tb0=Tb0)\n",
    "        if len(rainfall_core_after_search3[rainfall_core_after_search3['precipitation (mm/hr)'] > 0.1]) > 0:\n",
    "            rainfall_events_expanded.append(rainfall_core_after_search3)\n",
    "    \n",
    "    return rainfall_events_expanded\n",
    "\n",
    "# Get Tb0 values at each gauge\n",
    "tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "\n",
    "# Dataset paths and patterns\n",
    "datasets = {\n",
    "    'unfiltered': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Unfiltered/{year}/*',\n",
    "    'filtered_100': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Filtered_100/{year}/*',\n",
    "    'filtered_300': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Filtered_300/{year}/*'\n",
    "}\n",
    "\n",
    "yr = 2009\n",
    "print(f\"Processing year {yr}\")\n",
    "gauge_nums = range(0,1263)\n",
    "\n",
    "for gauge_num in gauge_nums:\n",
    "    if not gauge_num in [423, 444, 827, 888]:\n",
    "\n",
    "        print(f\"Processing gauge {gauge_num}\")\n",
    "\n",
    "        try:\n",
    "            # Check if files are missing for this year across all datasets\n",
    "            missing_files = False\n",
    "            for dataset_name in datasets.keys():\n",
    "                base_dir = f\"../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear/\"\n",
    "                if not any(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "                    missing_files = True\n",
    "                    break\n",
    "\n",
    "            if missing_files:\n",
    "                # Ensure directories for unfiltered, filtered_100, and filtered_300 exist\n",
    "                for dataset_name in datasets.keys():\n",
    "                    base_dir = f\"../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear/\"\n",
    "                    if not os.path.isdir(base_dir):\n",
    "                        os.makedirs(base_dir)\n",
    "\n",
    "                # Read in a sample cube for finding the location of gauge in grid\n",
    "                sample_cube = iris.load(f'/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Unfiltered/{yr}/metoffice-c-band-rain-radar_uk_{yr}0602_30mins.nc')[0][1,:,:]\n",
    "\n",
    "                # Find the Tb0 and index of this gauge\n",
    "                Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(gauge_num, sample_cube)\n",
    "\n",
    "                for dataset_name, dataset_path_pattern in datasets.items():\n",
    "                    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "                    # Load data for this year\n",
    "                    general_filename = dataset_path_pattern.format(year=yr)\n",
    "                    cache_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/nimrod/WholeYear/cube_{yr}.pkl\"\n",
    "\n",
    "                    try:\n",
    "                        if yr not in cubes_cache[dataset_name]:\n",
    "                            if os.path.exists(cache_filepath):\n",
    "                                cube = load_cube_from_disk(cache_filepath)\n",
    "                                print(\"Loading cube from cache\")\n",
    "                            else:\n",
    "                                cube = load_and_cache_cube(yr, cubes_cache[dataset_name], general_filename)\n",
    "                                save_cube_to_disk(cube, cache_filepath)\n",
    "                                print(\"Loading cube from folder\")\n",
    "                        else:\n",
    "                            cube = cubes_cache[dataset_name][yr]\n",
    "                    except (EOFError, FileNotFoundError) as e:\n",
    "                        print(f\"Error loading cube for year {yr}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract data for the specified indices\n",
    "                    data = cube[:, idx_2d[0], idx_2d[1]].data\n",
    "\n",
    "                    # Create a DataFrame from the data\n",
    "                    df = pd.DataFrame({\n",
    "                        'times': cube[:, idx_2d[0], idx_2d[1]].coord('time').units.num2date(cube.coord('time').points),\n",
    "                        'precipitation (mm/hr)': data,\n",
    "                        'precipitation (mm)': data / 2})\n",
    "\n",
    "                    #############################################\n",
    "                    # Fill in missing values\n",
    "                    #############################################\n",
    "                    df['times'] = pd.to_datetime([t.isoformat() for t in df['times']])\n",
    "\n",
    "                    # Determine the frequency\n",
    "                    freq = '30T'  # 30 minutes\n",
    "\n",
    "                    # Create a full date range\n",
    "                    full_time_range = pd.date_range(start=df['times'].min(), end=df['times'].max(), freq=freq)\n",
    "\n",
    "                    # Set 'time' as index and reindex to the full range\n",
    "                    df.set_index('times', inplace=True)\n",
    "                    df = df.reindex(full_time_range)\n",
    "\n",
    "                    # Reset index and rename it back to 'time'\n",
    "                    df.reset_index(inplace=True)\n",
    "                    df.rename(columns={'index': 'times'}, inplace=True)\n",
    "\n",
    "                    # Calculate time difference in minutes\n",
    "                    df['minutes_since_last'] = df['times'].diff().dt.total_seconds() / 60\n",
    "                    df['minutes_since_last'] = df['minutes_since_last'].fillna(0)\n",
    "\n",
    "                    # Loop through durations\n",
    "                    for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "                        base_dir = f\"../../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}/WholeYear/\"\n",
    "                        if not os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"):\n",
    "                            print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr} for {dataset_name}\")\n",
    "\n",
    "                            # Find events\n",
    "                            events_v2 = find_amax_indy_events_v2(df, duration=duration, Tb0=Tb0)\n",
    "\n",
    "                            # Save events to CSV\n",
    "                            for num, event in enumerate(events_v2):\n",
    "                                if len(event) > 1:\n",
    "                                    event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "\n",
    "            else:\n",
    "                print(f\"All files already exist for gauge {gauge_num} and year {yr}\")\n",
    "\n",
    "            # Clear the cache at the end of processing each year\n",
    "            for cache in cubes_cache.values():\n",
    "                cache.clear()\n",
    "\n",
    "            # Collect garbage to free up memory\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing gauge {gauge_num}: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
