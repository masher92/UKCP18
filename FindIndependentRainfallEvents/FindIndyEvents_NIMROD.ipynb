{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import numpy.ma as ma\n",
    "import tilemapbase\n",
    "import iris.plot as iplt\n",
    "from math import cos, radians\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyproj import Proj, transform\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import iris\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from Identify_Events_Functions import *\n",
    "\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Custom limited-size cache\n",
    "class LimitedSizeDict(OrderedDict):\n",
    "    def __init__(self, *args, max_size=100, **kwargs):\n",
    "        self.max_size = max_size\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if len(self) >= self.max_size:\n",
    "            self.popitem(last=False)\n",
    "        OrderedDict.__setitem__(self, key, value)\n",
    "\n",
    "# Initialize a limited-size cache\n",
    "cubes_cache = {\n",
    "    'unfiltered': LimitedSizeDict(max_size=10),\n",
    "    'filtered_100': LimitedSizeDict(max_size=10),\n",
    "    'filtered_300': LimitedSizeDict(max_size=10)\n",
    "}\n",
    "\n",
    "def load_and_cache_cube(year, cache, filenames_pattern):\n",
    "    if year in cache:\n",
    "        print(f\"Using cached data for year {year}\")\n",
    "        return cache[year]\n",
    "\n",
    "    print(f\"Loading data for year {year}\")\n",
    "    filenames = [filename for filename in glob.glob(filenames_pattern) if '.nc' in filename]\n",
    "\n",
    "    if not filenames:\n",
    "        raise FileNotFoundError(f\"No files found for the year {year} with pattern {filenames_pattern}\")\n",
    "\n",
    "    cubes = iris.load(filenames)\n",
    "    for cube in cubes:\n",
    "        cube.rename(cubes[0].name())\n",
    "    iris.util.equalise_attributes(cubes) \n",
    "    cube = cubes.concatenate_cube()\n",
    "    cache[year] = cube\n",
    "\n",
    "    return cube\n",
    "\n",
    "def save_cube_to_disk(cube, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(cube, f)\n",
    "\n",
    "def load_cube_from_disk(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def find_gauge_Tb0_and_location_in_grid(gauge_num, sample_cube):\n",
    "    # Get data just for this gauge\n",
    "    gauge1 = tbo_vals.iloc[gauge_num]\n",
    "    # Find the interevent arrival time (Tb0)\n",
    "    Tb0 = int(gauge1['Critical_interarrival_time'])\n",
    "    # Find the coordinates of the cell containing this gauge\n",
    "    closest_point, idx_2d = find_position_obs(sample_cube, gauge1['Lat'], gauge1['Lon'], plot_radius=10, plot=False)\n",
    "    \n",
    "    return Tb0, idx_2d\n",
    "\n",
    "def find_amax_indy_events_v2(df, duration, Tb0):\n",
    "    rainfall_cores = find_rainfall_core(df, duration=duration, Tb0=Tb0)\n",
    "    rainfall_events_expanded = []\n",
    "\n",
    "    for rainfall_core in rainfall_cores:\n",
    "        rainfall_core_after_search1 = search1(df, rainfall_core)\n",
    "        rainfall_core_after_search2 = search2(df, rainfall_core_after_search1)\n",
    "        rainfall_core_after_search3 = search3(df, rainfall_core_after_search2, Tb0=Tb0)\n",
    "        # If the event is not entirely dry \n",
    "        if len(rainfall_core_after_search3[rainfall_core_after_search3['precipitation (mm/hr)'] > 0.1]) > 0:\n",
    "            rainfall_events_expanded.append(rainfall_core_after_search3)\n",
    "    \n",
    "    return rainfall_events_expanded\n",
    "\n",
    "# Get tb0 values at each gauge\n",
    "tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "\n",
    "# Dataset paths and patterns\n",
    "datasets = {\n",
    "    'unfiltered': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Unfiltered/{year}/*',\n",
    "    'filtered_100': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Filtered_100/{year}/*',\n",
    "    'filtered_300': '/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Filtered_300/{year}/*'\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through years\n",
    "for gauge_num in range(1206, 1500):\n",
    "    if gauge_num not in [423, 444, 827, 888]:\n",
    "        print(f\"Processing gauge {gauge_num}\")\n",
    "\n",
    "        # Read in a sample cube for finding the location of gauge in grid\n",
    "        sample_cube = iris.load(f'/nfs/a161/gy17m2a/PhD/datadir/NIMROD/30mins/OriginalFormat_1km/Unfiltered/2012/metoffice-c-band-rain-radar_uk_20120602_30mins.nc')[0][1,:,:]\n",
    "\n",
    "        # Find the Tb0 and index of this gauge\n",
    "        Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(gauge_num, sample_cube)\n",
    "\n",
    "        # Loop through gauges\n",
    "        for yr in range(2006, 2021):\n",
    "            print(f\"Processing year {yr}\")\n",
    "\n",
    "            # Ensure directories for unfiltered, filtered_100, and filtered_300 exist\n",
    "            for dataset_name in datasets.keys():\n",
    "                base_dir = f\"../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}\"\n",
    "                if not os.path.isdir(base_dir):\n",
    "                    os.makedirs(base_dir)\n",
    "\n",
    "            # Check if any files are missing for this year across all datasets\n",
    "            missing_files = False\n",
    "            for dataset_name in datasets.keys():\n",
    "                base_dir = f\"../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}\"\n",
    "                if not any(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "                    missing_files = True\n",
    "                    break\n",
    "\n",
    "            if missing_files:\n",
    "                for dataset_name, dataset_path_pattern in datasets.items():\n",
    "                    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "                    # Load data for this year\n",
    "                    general_filename = dataset_path_pattern.format(year=yr)\n",
    "                    cache_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/nimrod/cube_{yr}.pkl\"\n",
    "\n",
    "                    try:\n",
    "                        if yr not in cubes_cache[dataset_name]:\n",
    "                            if os.path.exists(cache_filepath):\n",
    "                                cube = load_cube_from_disk(cache_filepath)\n",
    "                            else:\n",
    "                                cube = load_and_cache_cube(yr, cubes_cache[dataset_name], general_filename)\n",
    "                                save_cube_to_disk(cube, cache_filepath)\n",
    "                        else:\n",
    "                            cube = cubes_cache[dataset_name][yr]\n",
    "                    except (EOFError, FileNotFoundError) as e:\n",
    "                        print(f\"Error loading cube for year {yr}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract data for the specified indices\n",
    "                    data = cube[:, idx_2d[0], idx_2d[1]].data\n",
    "\n",
    "                    # Create a DataFrame from the data\n",
    "                    df = pd.DataFrame({\n",
    "                        'times': cube[:, idx_2d[0], idx_2d[1]].coord('time').units.num2date(cube.coord('time').points),\n",
    "                        'precipitation (mm/hr)': data,\n",
    "                        'precipitation (mm)': data / 2})\n",
    "\n",
    "                    # Loop through durations\n",
    "                    for duration in [0.5, 1, 2, 3, 6, 12, 24]:\n",
    "                        base_dir = f\"../../ProcessedData/IndependentEvents/NIMROD/NIMROD_1km_{dataset_name}/{gauge_num}\"\n",
    "                        if not os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\"):\n",
    "                            print(f\"Finding the AMAX for {duration}hr events for gauge {gauge_num} in year {yr} for {dataset_name}\")\n",
    "\n",
    "                            # Find events\n",
    "                            events_v2 = find_amax_indy_events_v2(df, duration=duration, Tb0=Tb0)\n",
    "\n",
    "                            # Save events to CSV\n",
    "                            for num, event in enumerate(events_v2):\n",
    "                                if len(event) > 1:\n",
    "                                    event.to_csv(f\"{base_dir}/{duration}hrs_{yr}_v2_part{num}.csv\")\n",
    "            else:\n",
    "                print(f\"All files already exist for gauge {gauge_num} and year {yr}\")\n",
    "\n",
    "    # Clear the cache at the end of processing each year\n",
    "    for cache in cubes_cache.values():\n",
    "        cache.clear()\n",
    "\n",
    "    # Collect garbage to free up memory\n",
    "    gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
