{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfeaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from ProcessEventsFunctions import *\n",
    "sys.path.insert(1, 'Old')\n",
    "from Steef_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161cbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/nfs/a319/gy17m2a/PhD/'\n",
    "home_dir2 = '/nfs/a161/gy17m2a/PhD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412d55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_mapping = {1: 'F2', 2: 'F1', 3: 'C', 4: 'B1', 5: 'B2'}\n",
    "quintile_mapping_thirds = {1: 'F', 2: 'C', 3: 'B'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285aba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbo_vals = pd.read_csv(home_dir + 'datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "# Check if the points are within the areas\n",
    "tbo_vals = check_for_gauge_in_areas(tbo_vals, home_dir, ['NW', 'NE', 'ME', 'SE', 'SW'])\n",
    "tbo_vals.loc[tbo_vals['within_area'] == 'NW, C', 'within_area'] = 'NW'\n",
    "tbo_vals.loc[tbo_vals['within_area'] == 'ME, SE', 'within_area'] = 'ME'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323ae47",
   "metadata": {},
   "source": [
    "### Define ensemble members for present and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74ccd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_present = ['bc005', 'bc006', 'bc007', 'bc009', 'bc010', 'bc011', 'bc012', 'bc013', 'bc015', 'bc016', 'bc017', 'bc018']\n",
    "ems_future = ['bb222']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004fdf9",
   "metadata": {},
   "source": [
    "### Get events (considering one set of AMAX producing events (with duplicates deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63159a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb222\n",
      "Processing gauge 800\n"
     ]
    }
   ],
   "source": [
    "home_dir = home_dir2\n",
    "time_period = 'Future'\n",
    "ems = ems_future\n",
    "tb0_vals = tbo_vals\n",
    "save_dir = '/nfs/a319/gy17m2a/PhD/'\n",
    "\n",
    "# events_dict = {}\n",
    "# event_props_ls = []\n",
    "# event_profiles_dict = {}\n",
    "\n",
    "for em in ems:\n",
    "    print(em)\n",
    "    for gauge_num in range(729, 1294):\n",
    "        if gauge_num not in [444, 827, 888]:\n",
    "            if gauge_num % 100 == 0:\n",
    "                print(f\"Processing gauge {gauge_num}\")\n",
    "            indy_events_fp = home_dir + f\"ProcessedData/IndependentEvents/UKCP18_30mins/{time_period}/{em}/{gauge_num}/WholeYear/EventSet/\"\n",
    "\n",
    "            files = [f for f in os.listdir(indy_events_fp) if f.endswith('.csv')]\n",
    "            files = np.sort(files)\n",
    "\n",
    "            for event_num, file in enumerate(files):\n",
    "                fp = indy_events_fp + f\"{file}\"\n",
    "                if '2080' in fp:\n",
    "                    continue\n",
    "\n",
    "                # Get event\n",
    "                this_event = read_event(gauge_num, fp)\n",
    "\n",
    "                # Get times and precipitation values\n",
    "                event_times = this_event['times']\n",
    "                event_precip = this_event['precipitation (mm)']\n",
    "\n",
    "                # Apply the function to adjust the dates in the 'times' column\n",
    "                event_times_fixed = event_times.apply(adjust_feb_dates)\n",
    "\n",
    "                # Create the DataFrame with corrected times\n",
    "                event_df = pd.DataFrame({'precipitation (mm)': event_precip, 'times': event_times_fixed})\n",
    "                # Remove leading and trailing zeroes\n",
    "                event_df = remove_leading_and_trailing_zeroes(event_df)\n",
    "                # Create characteristics dictionary\n",
    "                event_props = create_event_characteristics_dict(event_df)\n",
    "\n",
    "                # Add the duration\n",
    "                event_props['dur_for_which_this_is_amax'] = get_dur_for_which_this_is_amax(fp)\n",
    "                # Add gauge number and ensemble member\n",
    "                event_props['gauge_num'] = gauge_num\n",
    "                event_props['area'] = tb0_vals.iloc[gauge_num]['within_area']\n",
    "                event_props['em'] = em\n",
    "                event_props['filename'] = file\n",
    "\n",
    "                ##########################################\n",
    "                # Specify the keys you want to check\n",
    "                keys_to_check = ['duration', 'year', 'gauge_num', 'month', 'Volume', 'max_intensity']\n",
    "\n",
    "                # Extract the values for the specified keys from dict_to_check\n",
    "                values_to_check = tuple(event_props[key] for key in keys_to_check)\n",
    "\n",
    "                # Initialize a variable to store the found dictionary\n",
    "                matched_dict = None\n",
    "\n",
    "                # Check if a matching dictionary exists in the list based on the specified keys\n",
    "                for index, d in enumerate(event_props_ls):\n",
    "                    if tuple(d[key] for key in keys_to_check) == values_to_check:\n",
    "                        matched_dict = d  # Store the matching dictionary\n",
    "                        break  # Exit the loop since we found a match\n",
    "\n",
    "                if matched_dict:\n",
    "                    # print(\"A matching dictionary found:\", matched_dict, event_props)\n",
    "\n",
    "                    new_value = event_props['dur_for_which_this_is_amax']\n",
    "                    existing_value = matched_dict.get('dur_for_which_this_is_amax', '')\n",
    "                    # Create or update the value as a list\n",
    "                    if isinstance(existing_value, list):\n",
    "                        existing_value.append(new_value)\n",
    "                    else:\n",
    "                        existing_value = [existing_value, new_value]  # Convert existing string to list and add 'yes'\n",
    "                    matched_dict['dur_for_which_this_is_amax'] = existing_value\n",
    "\n",
    "                    event_props_ls[index]= matched_dict\n",
    "\n",
    "                else:\n",
    "                    # print(\"No matching dictionary found in the list.\")\n",
    "\n",
    "                    ##########################################\n",
    "                    events_dict[f\"{em}, {gauge_num}, {event_num}\"] = event_df\n",
    "                    event_props_ls.append(event_props)\n",
    "                    event_profiles_dict[f\"{em}, {gauge_num}, {event_num}\"] = create_profiles_dict(event_df)\n",
    "\n",
    "    print(f\"Finished {em}\")                        \n",
    "\n",
    "    with open(save_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/{time_period}/events_dict_{em}.pickle\", 'wb') as handle:\n",
    "        pickle.dump(events_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(save_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/{time_period}/event_profiles_dict_{em}.pickle\", 'wb') as handle:\n",
    "        pickle.dump(event_profiles_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(save_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/{time_period}/event_props_dict_{em}.pickle\", 'wb') as handle:\n",
    "        pickle.dump(event_props_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c4db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb222\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2177/231556802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # Now you can call the function for both time periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# events_dict_present, event_props_dict_present, event_profiles_dict_present = process_events_alltogether(home_dir2, 'Present',ems_present, tbo_vals, home_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevents_dict_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_props_dict_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_profiles_dict_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_events_alltogether\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_dir2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Future'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mems_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbo_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/a319/gy17m2a/PhD/Scripts/FindIndependentRainfallEvents/ProcessEvents/ProcessEventsFunctions.py\u001b[0m in \u001b[0;36mprocess_events_alltogether\u001b[0;34m(home_dir, time_period, ems, tb0_vals, save_dir)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0;31m# Get event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mthis_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauge_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# Get times and precipitation values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a319/gy17m2a/PhD/Scripts/FindIndependentRainfallEvents/ProcessEvents/ProcessEventsFunctions.py\u001b[0m in \u001b[0;36mread_event\u001b[0;34m(gauge_num, fp)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauge_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_since_last_minutes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# # Now you can call the function for both time periods\n",
    "# events_dict_present, event_props_dict_present, event_profiles_dict_present = process_events_alltogether(home_dir2, 'Present',ems_present, tbo_vals, home_dir)\n",
    "events_dict_future, event_props_dict_future, event_profiles_dict_future = process_events_alltogether(home_dir2, 'Future', ems_future, tbo_vals, home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c0c5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/events_dict_present.pickle\", 'wb') as handle:\n",
    "    pickle.dump(events_dict_present, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/events_dict_future.pickle\", 'wb') as handle:\n",
    "    pickle.dump(events_dict_future, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/event_profiles_dict_present\", 'wb') as handle:\n",
    "    pickle.dump(event_profiles_dict_present, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/event_profiles_dict_future.pickle\", 'wb') as handle:\n",
    "    pickle.dump(event_profiles_dict_future, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/event_props_dict_present.pickle\", 'wb') as handle:\n",
    "    pickle.dump(event_props_dict_present, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/event_props_dict_future.pickle\", 'wb') as handle:\n",
    "    pickle.dump(event_props_dict_future, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c89f0",
   "metadata": {},
   "source": [
    "## Get events for each duration (there'll be cross over in events present for each duration)\n",
    "By duration here we mean the duration for which the AMAX are associated, rather than the actual duration of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of desired durations\n",
    "valid_durations = [\"0.5\", \"1\", \"2\", \"3\", \"6\", \"12\", \"24\"]\n",
    "\n",
    "# Process events for both time periods\n",
    "results_present = process_events_by_duration(home_dir2, 'Present', valid_durations, ems_present, tbo_vals)\n",
    "# results_future = process_events_by_duration('Future', valid_durations, ems_future, tbo_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b135d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_present_events_dict, dur_present_event_props_dict, dur_present_event_profiles_dict = results_present\n",
    "# dur_future_events_dict, dur_future_event_props_dict, dur_future_event_profiles_dict = results_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0238ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/results_each_dur_present.pickle\", 'wb') as handle:\n",
    "    pickle.dump(results_present, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "# with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/results_each_dur_future.pickle\", 'wb') as handle:\n",
    "#     pickle.dump(results_future, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80729dcf",
   "metadata": {},
   "source": [
    "## Do we have the same number of results for each duration?\n",
    "I believe that longer durations have more events because of compound annual maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c060f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 2964\n",
      "1 2964\n",
      "2 2964\n",
      "3 2964\n",
      "6 2966\n",
      "12 3022\n",
      "24 3449\n"
     ]
    }
   ],
   "source": [
    "for duration in valid_durations:\n",
    "    print(duration, len(dur_present_event_props_dict[duration].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46e892c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc005\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc006\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc007\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc009\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc010\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc011\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc012\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc013\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc015\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc016\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc017\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bc018\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb189\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb192\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb195\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb198\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb201\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb204\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb208\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb211\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb216\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb219\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb222\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n",
      "bb225\n",
      "Processing gauge 0\n",
      "Processing gauge 100\n",
      "Processing gauge 200\n",
      "Processing gauge 300\n",
      "Processing gauge 400\n",
      "Processing gauge 500\n",
      "Processing gauge 600\n",
      "Processing gauge 700\n",
      "Processing gauge 800\n",
      "Processing gauge 900\n",
      "Processing gauge 1000\n",
      "Processing gauge 1100\n",
      "Processing gauge 1200\n"
     ]
    }
   ],
   "source": [
    "# List of desired durations\n",
    "duration_bins = ['<4hr', '4-12hr', '12hr+']\n",
    "\n",
    "# Process events for both time periods\n",
    "results_present_dur_categories_simple = process_events_by_duration(home_dir2, 'Present', duration_bins, ems_present, tbo_vals)\n",
    "results_future_dur_categories_simple = process_events_by_duration(home_dir2, 'Future', duration_bins, ems_future, tbo_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a38ee9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/results_present_dur_categories_simple.pickle\", 'wb') as handle:\n",
    "    pickle.dump(results_present_dur_categories_simple, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "with open(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/results_future_dur_categories_simple.pickle\", 'wb') as handle:\n",
    "    pickle.dump(results_future_dur_categories_simple, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
