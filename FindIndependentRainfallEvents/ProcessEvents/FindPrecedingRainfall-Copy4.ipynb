{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05925223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys \n",
    "import iris\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "sys.path.insert(1, '../FindingEvents/')\n",
    "from Identify_Events_Functions import *\n",
    "from Prepare_Data_Functions import *\n",
    "from Get_Events_Functions import *\n",
    "\n",
    "gauge_nums = range(445,1294)\n",
    "em ='bc009'\n",
    "time_period='Present'\n",
    "yrs_range ='2002_2020'\n",
    "sample_cube_yr = 2006\n",
    "yr_1, yr_2 = 2001, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files missing\n",
      "Processing gauge 488\n",
      "Files missing\n",
      "Processing gauge 489\n",
      "Files missing\n",
      "Processing gauge 490\n",
      "Files missing\n",
      "Processing gauge 491\n",
      "Files missing\n",
      "Processing gauge 492\n",
      "Files missing\n",
      "Processing gauge 493\n",
      "Files missing\n",
      "Processing gauge 494\n",
      "Files missing\n",
      "Processing gauge 495\n",
      "Files missing\n",
      "Processing gauge 496\n",
      "Files missing\n",
      "Processing gauge 497\n",
      "Files missing\n",
      "Processing gauge 498\n",
      "Files missing\n",
      "Processing gauge 499\n",
      "Files missing\n",
      "Processing gauge 500\n",
      "Files missing\n",
      "Processing gauge 501\n",
      "Files missing\n",
      "Processing gauge 502\n",
      "Files missing\n",
      "Processing gauge 503\n"
     ]
    }
   ],
   "source": [
    "full_yr_cubes_dict = {}\n",
    "for yr in range(yr_1, yr_2):\n",
    "    # Load data for that year\n",
    "    pickle_file_filepath = f\"/nfs/a319/gy17m2a/PhD/datadir/cache/UKCP18_30mins_{em}/WholeYear/cube_{yr}.pkl\"\n",
    "    full_yr_cubes_dict[yr]= load_cube_from_picklefile(pickle_file_filepath)\n",
    "\n",
    "for em in [em]:\n",
    "    for gauge_num in gauge_nums:\n",
    "        if gauge_num not in [444, 827, 888]:\n",
    "            \n",
    "            missing_files = False\n",
    "            if em == 'nimrod':\n",
    "                indy_events_fp = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/NIMROD_30mins/2km_filtered_100/{gauge_num}/WholeYear/\"\n",
    "            else:\n",
    "                indy_events_fp = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents/UKCP18_30mins/{time_period}/{em}/{gauge_num}/WholeYear/\"\n",
    "\n",
    "            base_dir = f\"/nfs/a161/gy17m2a/PhD/ProcessedData/IndependentEvents_precedingtime/UKCP18_30mins/{em}/{gauge_num}/WholeYear\"\n",
    "            # Create the directory if it doesnt exist\n",
    "            if not os.path.isdir(base_dir):\n",
    "                os.makedirs(base_dir)                \n",
    "\n",
    "            # Check if we are missing any of the files, and if so, change the flag to True\n",
    "            if not any(os.path.exists(f\"{base_dir}/{duration}hrs_{yr}_v2_part0.csv\") for duration in [0.5, 1, 2, 3, 6, 12, 24]):\n",
    "                missing_files = True           \n",
    "                \n",
    "            if missing_files == True:    \n",
    "                print(\"Files missing\")\n",
    "                # Get Tb0 values at each gauge\n",
    "                tbo_vals = pd.read_csv('/nfs/a319/gy17m2a/PhD/datadir/RainGauge/interarrival_thresholds_CDD_noMissing.txt')\n",
    "                sample_cube = iris.load(f'/nfs/a319/gy17m2a/PhD/datadir/UKCP18_every30mins/2.2km_bng/{yrs_range}/{em}/bng_{em}a.pr{sample_cube_yr}01.nc')[0][1,:,:]\n",
    "                Tb0, idx_2d = find_gauge_Tb0_and_location_in_grid(tbo_vals, gauge_num, sample_cube)\n",
    "\n",
    "\n",
    "                print(f\"Processing gauge {gauge_num}\")\n",
    "    #             if gauge_num % 100 == 0:\n",
    "    #                 print(f\"Processing gauge {gauge_num}\")\n",
    "\n",
    "\n",
    "                files = [f for f in os.listdir(indy_events_fp) if f.endswith('.csv')]\n",
    "                files = np.sort(files)\n",
    "\n",
    "                for file in files:\n",
    "                    fp = indy_events_fp +  f\"{file}\"\n",
    "                    # Get the event\n",
    "                    this_event = read_event(gauge_num, fp)\n",
    "                    # Find the year\n",
    "                    match = re.search(r'20\\d{2}', fp)\n",
    "                    yr = match.group(0)\n",
    "                    # Get data\n",
    "                    full_year_cube =  full_yr_cubes_dict[int(yr)]\n",
    "                    # Extract data for the gauge location\n",
    "                    one_location_cube = full_year_cube[:, idx_2d[0], idx_2d[1]]\n",
    "\n",
    "                    # Get the 100 timeslices before this event\n",
    "                    end_index = this_event['Unnamed: 0'][0]\n",
    "                    if end_index<100:\n",
    "                        start_index = end_index-end_index\n",
    "                    else:\n",
    "                        start_index=end_index-100\n",
    "\n",
    "                    if end_index == 0:\n",
    "                        preceding_times_df = pd.DataFrame()\n",
    "                        print(f'Start index was 0 for {file}')\n",
    "                    else:\n",
    "\n",
    "                        # Assuming one_location_cube is your Iris cube and start_index is defined\n",
    "                        time_coord = one_location_cube.coord('time')\n",
    "                        time_unit = time_coord.units\n",
    "\n",
    "                        time_datetimes = time_unit.num2date(one_location_cube[start_index:end_index].coord('time').points)\n",
    "\n",
    "                        # Create a DataFrame with the datetime times\n",
    "                        preceding_times_df = pd.DataFrame({'times': time_datetimes,\n",
    "                            'precip': one_location_cube[start_index:end_index].data})\n",
    "\n",
    "                    # Display the DataFrame\n",
    "                    preceding_times_df.to_csv(f\"{base_dir}/{file}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
