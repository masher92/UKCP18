{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_d50_with_interpolation(sample):\n",
    "    n=5\n",
    "    cumulative_rainfall, cumulative_rainfall_times = create_cumulative_event(sample)\n",
    "    dimensionless_cumulative_rainfall, dimensionless_times =  create_dimensionless_event(cumulative_rainfall, cumulative_rainfall_times)\n",
    "    interpolated_n_cumulative_rainfall, interpolated_n_times = interpolate_rainfall(dimensionless_cumulative_rainfall,n)\n",
    "    interpolated_n_incremental_rainfall = create_incremental_event(interpolated_n_cumulative_rainfall)\n",
    "    max_quintile_profile = find_part_with_most_rain(interpolated_n_incremental_rainfall, n)\n",
    "    \n",
    "    percentile = 0.5\n",
    "    \n",
    "    time_percentage = (np.arange(0, len(sample) + 1) / len(sample)) * 100\n",
    "    \n",
    "    # Find the indices where the cumulative rainfall crosses the percentile_value\n",
    "    indices_below = np.where(dimensionless_cumulative_rainfall < percentile)[0]\n",
    "    indices_above = np.where(dimensionless_cumulative_rainfall >= percentile)[0]\n",
    "\n",
    "    # Ensure there are indices both below and above the percentile value\n",
    "    if len(indices_below) > 0 and len(indices_above) > 0:\n",
    "        index_below = indices_below[-1]  # Last index below the percentile value\n",
    "        index_above = indices_above[0]    # First index above the percentile value\n",
    "\n",
    "        # Perform linear interpolation to find the exact intersection point\n",
    "        x_below = time_percentage[index_below]\n",
    "        y_below = dimensionless_cumulative_rainfall[index_below]\n",
    "\n",
    "        x_above = time_percentage[index_above]\n",
    "        y_above = dimensionless_cumulative_rainfall[index_above]\n",
    "\n",
    "        # Calculate the slope\n",
    "        slope = (y_above - y_below) / (x_above - x_below)\n",
    "        # Use the formula to find the exact x value where the y value equals percentile_value\n",
    "        time_for_percentile = x_below + (percentile - y_below) / slope\n",
    "\n",
    "        return time_for_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69a6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2c89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FormatEvents_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594c9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/nfs/a319/gy17m2a/PhD/'\n",
    "home_dir2 = '/nfs/a161/gy17m2a/PhD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254b686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = ['0.5', '1', '2', '3', '6', '12', '24']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6eb2c",
   "metadata": {},
   "source": [
    "### UKCP18 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c4c4f",
   "metadata": {},
   "source": [
    "### Join together lists for different ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4bb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_props_dict_present = []\n",
    "ems_present = ['bc005', 'bc006', 'bc007', 'bc009', 'bc010', 'bc011', 'bc012', 'bc013', 'bc015', 'bc016', 'bc017', 'bc018']\n",
    "for em in ems_present:\n",
    "    with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/event_props_dict_{em}.pickle\", 'rb') as handle:\n",
    "        one_events_props_dict_present = pickle.load(handle)    \n",
    "    events_props_dict_present = events_props_dict_present + one_events_props_dict_present\n",
    "    \n",
    "## Join into one dataframe    \n",
    "present = pd.DataFrame(events_props_dict_present)\n",
    "present['Climate'] = 'Present'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec28fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_props_dict_future = []\n",
    "# ems_future = ['bb195', 'bb192', 'bb198', 'bb208', 'bb225','bb222', 'bb201', 'bb204', 'bb216', 'bb219', 'bb211']\n",
    "ems_future = ['bb195', 'bb198', 'bb192', 'bb208', 'bb225','bb222', 'bb201', 'bb204', 'bb216', 'bb219', 'bb211', 'bb189'] #bb195, #bb198\n",
    "for em in ems_future:\n",
    "    with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/event_props_dict_{em}.pickle\", 'rb') as handle:\n",
    "        one_events_props_dict_future = pickle.load(handle)    \n",
    "    events_props_dict_future = events_props_dict_future + one_events_props_dict_future\n",
    "    \n",
    "## Join into one dataframe\n",
    "future = pd.DataFrame(events_props_dict_future)\n",
    "future['Climate'] = 'Future'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8203f5",
   "metadata": {},
   "source": [
    "## Make a check on number of files (could shift this to the checking script)\n",
    "NB - the method of searching on part1 doesnt work, because the filename only represents on of the files that is represented by that event\n",
    "\n",
    "\n",
    "24529 is 19 * 1291 and is the number we expert with no part1s for one ensemble member.  \n",
    "For 12 ems it becomes 24529 * 12 = 294348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ec2bb",
   "metadata": {},
   "source": [
    "### Create one dataframe containing both present and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.concat([present, future])\n",
    "\n",
    "# Add D variable (day of year) and date\n",
    "df_long['D'] = (df_long['theta'] * 365.25) / (2 * np.pi)\n",
    "df_long['date'] = df_long.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "df_long['season'] = df_long['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1aa28",
   "metadata": {},
   "source": [
    "### Check the number of files for each duration\n",
    "NB: Number of files for 24h duration is longer due to compound events  \n",
    "Checked this by filtering only rows with part0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13853b0",
   "metadata": {},
   "source": [
    "### Remove entries which are less than 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3d204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def has_duplicates(lst):\n",
    "#     return len(lst) != len(set(lst))\n",
    "\n",
    "# # Apply the function to each row in the specified column\n",
    "# df_long['has_duplicates'] = df_long['dur_for_which_this_is_amax'].apply(has_duplicates)\n",
    "\n",
    "# # Display rows with duplicates\n",
    "# rows_with_duplicates = df_long[df_long['has_duplicates']]\n",
    "# rows_with_duplicates['dur_for_which_this_is_amax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4fd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_with_short_durations_kept = df_long.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd33065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long[df_long['duration'] >=1.5]\n",
    "present = present[present['duration'] >=1.5]\n",
    "future = future[future['duration'] >=1.5]\n",
    "# nan_rows = df_long[df_long['D50'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4ce11",
   "metadata": {},
   "source": [
    "### NIMROD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2bc859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(home_dir +  f\"ProcessedData/AMAX_Events/NIMROD_30mins/event_props_dict.pickle\", 'rb') as handle:\n",
    "#     events_props_dict_nimrod = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f603319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nimrod = pd.DataFrame(events_props_dict_nimrod)\n",
    "# # Add D variable (day of year) and date\n",
    "# nimrod['D'] = (nimrod['theta'] * 365.25) / (2 * np.pi)\n",
    "# nimrod['date'] = nimrod.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "# nimrod['season'] = nimrod['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4102701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "# nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# # Flatten the lists and count occurrences of each number\n",
    "# all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "# number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# # Show the result\n",
    "# print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27805f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "# nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# # Flatten the lists and count occurrences of each number\n",
    "# all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "# number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# # Show the result\n",
    "# print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c62805e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nimrod.to_csv(home_dir + f\"ProcessedData/AMAX_Events/NIMROD_30mins/all_events_characteristics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c3f2d",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (no duplicates for durations)\n",
    "### Group by gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7c334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raleigh_test(group):\n",
    "\n",
    "    angles = group['theta']\n",
    "    n = len(angles)\n",
    "\n",
    "    # Compute vector components\n",
    "    R = np.sqrt((np.sum(np.cos(angles)))**2 + (np.sum(np.sin(angles)))**2)\n",
    "    R_bar = R / n\n",
    "\n",
    "    # Compute Rayleigh's Z statistic\n",
    "    Z = (R_bar**2) * n\n",
    "\n",
    "    # Approximate p-value using normal distribution\n",
    "    p_value = np.exp(-Z)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c4341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num']\n",
    "grouped_by_gauge_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_allevents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521afbb8",
   "metadata": {},
   "source": [
    "### Group by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04471ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num', 'season']\n",
    "grouped_by_gauge_season_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_season_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_season_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_season_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_allevents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dff98b",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (for each duration separately)\n",
    "### Group by gauge, climate and by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0a1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n",
      "Index(['Climate_present', 'gauge_num', 'season', 'D_mean_present', 'R_present',\n",
      "       'D50_mean_present', 'D50_P90_present', 'D50_P10_present',\n",
      "       'D50_median_present', 'F2_percentage_present', 'B2_percentage_present',\n",
      "       'C_percentage_present', 'F1_percentage_present',\n",
      "       'B1_percentage_present', 'Climate_future', 'D_mean_future', 'R_future',\n",
      "       'D50_mean_future', 'D50_P90_future', 'D50_P10_future',\n",
      "       'D50_median_future', 'F2_percentage_future', 'B2_percentage_future',\n",
      "       'C_percentage_future', 'F1_percentage_future', 'B1_percentage_future'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in durations:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long['dur_for_which_this_is_amax'].apply(\n",
    "        lambda x: isinstance(x, list) and str(duration) in x or x == str(duration))]\n",
    "    \n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], float(duration))\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], float(duration))\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "442a7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat(each_dur_per_climate_changes)\n",
    "total_season = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b20f6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total[total['Circular_Uniformity_future']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1076036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydur.csv\", index=False)\n",
    "total_season.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydur.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8904609",
   "metadata": {},
   "source": [
    "### Save original data\n",
    "Don't do this higher up, because the json.dumps thing messes up the formatting for later stages of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "560d3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy and convert lists to JSON strings before saving - not doing this, messed up formatting a bit\n",
    "df_long['dur_for_which_this_is_amax'] = df_long['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics.csv\", index=False)\n",
    "df_long_with_short_durations_kept['dur_for_which_this_is_amax'] = df_long_with_short_durations_kept['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long_with_short_durations_kept.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics_shortdurationskept.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
