{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dur_category (bin_edges, duration_labels, duration):\n",
    "    # Create a pandas Series with the duration\n",
    "    # Use pd.cut to assign the duration to a bin\n",
    "    binned_duration = pd.cut(pd.Series([duration]), bins=bin_edges, labels=duration_labels, right=True,  include_lowest=True)\n",
    "    return binned_duration.iloc[0]\n",
    "\n",
    "def apply_dur_category_to_column(df, duration_column, bin_edges, duration_labels):\n",
    "    \"\"\"\n",
    "    This function applies the `find_dur_category` to a whole column in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the duration column.\n",
    "        duration_column (str): The name of the column to categorize.\n",
    "        bin_edges (list): The bin edges to categorize the values.\n",
    "        duration_labels (list): The labels to assign to the bins.\n",
    "    \n",
    "    Returns:\n",
    "        Series: A pandas Series with the categorized duration values.\n",
    "    \"\"\"\n",
    "    return df[duration_column].apply(lambda duration: find_dur_category(bin_edges, duration_labels, duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69a6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2c89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FormatEvents_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594c9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/nfs/a319/gy17m2a/PhD/'\n",
    "home_dir2 = '/nfs/a161/gy17m2a/PhD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254b686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = ['0.5', '1', '2', '3', '6', '12', '24']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6eb2c",
   "metadata": {},
   "source": [
    "### UKCP18 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c4c4f",
   "metadata": {},
   "source": [
    "### Join together lists for different ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4bb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_props_dict_present = []\n",
    "# ems_present = ['bc005', 'bc006', 'bc007', 'bc009', 'bc010', 'bc011', 'bc012', 'bc013', 'bc015', 'bc016', 'bc017', 'bc018']\n",
    "# for em in ems_present:\n",
    "#     with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/event_props_dict_{em}_NEW.pickle\", 'rb') as handle:\n",
    "#         one_events_props_dict_present = pickle.load(handle)    \n",
    "#         events_props_dict_present = events_props_dict_present + one_events_props_dict_present\n",
    "    \n",
    "# ## Join into one dataframe    \n",
    "# present = pd.DataFrame(events_props_dict_present)\n",
    "# present['Climate'] = 'Present'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec28fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_props_dict_future = []\n",
    "# ems_future = ['bb189', 'bb195', 'bb192', 'bb198', 'bb208', 'bb225','bb222', 'bb201', 'bb204', 'bb219', 'bb211', 'bb216'] #  ,\n",
    "# for em in ems_future:\n",
    "#     with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/event_props_dict_{em}_NEW.pickle\", 'rb') as handle:\n",
    "#         one_events_props_dict_future = pickle.load(handle)    \n",
    "#     events_props_dict_future = events_props_dict_future + one_events_props_dict_future\n",
    "\n",
    "#     ## Join into one dataframe\n",
    "# future = pd.DataFrame(events_props_dict_future)\n",
    "# future['Climate'] = 'Future'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8203f5",
   "metadata": {},
   "source": [
    "## Make a check on number of files (could shift this to the checking script)\n",
    "NB - the method of searching on part1 doesnt work, because the filename only represents on of the files that is represented by that event\n",
    "\n",
    "\n",
    "24529 is 19 * 1291 and is the number we expert with no part1s for one ensemble member.  \n",
    "For 12 ems it becomes 24529 * 12 = 294348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ec2bb",
   "metadata": {},
   "source": [
    "### Create one dataframe containing both present and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long = pd.concat([present, future])\n",
    "# df_long = df_long[df_long['duration'] >=1.5]\n",
    "# # df_long_over8 = df_long[df_long['duration']>8].copy()\n",
    "# # df_long_over6 = df_long[df_long['duration']>6].copy()\n",
    "# # df_long_over4 = df_long[df_long['duration']>4].copy()\n",
    "\n",
    "# present_all_events=df_long[df_long['Climate']=='Present'].copy()\n",
    "# future_all_events=df_long[df_long['Climate']=='Future'].copy()\n",
    "\n",
    "# # # Add D variable (day of year) and date\n",
    "# df_long['D'] = (df_long['theta'] * 365.25) / (2 * np.pi)\n",
    "# df_long['date'] = df_long.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "# df_long['season'] = df_long['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13ed0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_edges = [0, 7.0, 16, 166.5]  # Example bin edges\n",
    "# duration_labels = ['<=7hr', '7-16hr', '16hr+']  # Corresponding labels for the bins\n",
    "\n",
    "# # Apply the function to the entire column\n",
    "# df_long['duration_category_onemore'] = apply_dur_category_to_column(df_long, 'duration', bin_edges, duration_labels)\n",
    "df_long.to_csv(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/df_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0624ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long =pd.read_csv(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/df_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e733cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gini_coefficient(x):\n",
    "#     x = np.sort(x)  # Sort intensities\n",
    "#     n = len(x)\n",
    "#     cumulative = np.cumsum(x) / np.sum(x)\n",
    "#     gini = 1 - (2 / n) * np.sum((np.arange(1, n+1) / n) * cumulative)\n",
    "#     return gini\n",
    "\n",
    "# gini = gini_coefficient(event['precipitation (mm)'])\n",
    "# plt.plot(event['precipitation (mm)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1aa28",
   "metadata": {},
   "source": [
    "### Check the number of files for each duration\n",
    "NB: Number of files for 24h duration is longer due to compound events  \n",
    "Checked this by filtering only rows with part0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13853b0",
   "metadata": {},
   "source": [
    "### Remove entries which are less than 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3d204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def has_duplicates(lst):\n",
    "#     return len(lst) != len(set(lst))\n",
    "\n",
    "# # Apply the function to each row in the specified column\n",
    "# df_long['has_duplicates'] = df_long['dur_for_which_this_is_amax'].apply(has_duplicates)\n",
    "\n",
    "# # Display rows with duplicates\n",
    "# rows_with_duplicates = df_long[df_long['has_duplicates']]\n",
    "# rows_with_duplicates['dur_for_which_this_is_amax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4fd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_with_short_durations_kept = df_long.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd33065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long = df_long[df_long['duration'] >=1.5]\n",
    "# present = present[present['duration'] >=1.5]\n",
    "# future = future[future['duration'] >=1.5]\n",
    "# # nan_rows = df_long[df_long['D50'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4ce11",
   "metadata": {},
   "source": [
    "### NIMROD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2bc859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(home_dir +  f\"ProcessedData/AMAX_Events/NIMROD_30mins/event_props_dict.pickle\", 'rb') as handle:\n",
    "#     events_props_dict_nimrod = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f603319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nimrod = pd.DataFrame(events_props_dict_nimrod)\n",
    "# # Add D variable (day of year) and date\n",
    "# nimrod['D'] = (nimrod['theta'] * 365.25) / (2 * np.pi)\n",
    "# nimrod['date'] = nimrod.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "# nimrod['season'] = nimrod['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4102701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "# nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# # Flatten the lists and count occurrences of each number\n",
    "# all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "# number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# # Show the result\n",
    "# print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27805f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "# nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# # Flatten the lists and count occurrences of each number\n",
    "# all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "# number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# # Show the result\n",
    "# print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c62805e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nimrod.to_csv(home_dir + f\"ProcessedData/AMAX_Events/NIMROD_30mins/all_events_characteristics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c3f2d",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (no duplicates for durations)\n",
    "### Group by gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c4341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num']\n",
    "grouped_by_gauge_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_allevents_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521afbb8",
   "metadata": {},
   "source": [
    "### Group by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04471ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num', 'season']\n",
    "grouped_by_gauge_season_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_season_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_season_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_season_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_allevents_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dff98b",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (for each duration separately)\n",
    "### Group by gauge, climate and by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0a1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in durations:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long['dur_for_which_this_is_amax'].apply(\n",
    "        lambda x: isinstance(x, list) and str(duration) in x or x == str(duration))]\n",
    "    # this_dur = this_dur[this_dur['duration']>8]\n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], float(duration))\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], float(duration))\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c74c3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat(each_dur_per_climate_changes)\n",
    "total_season = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81730749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0468687563705052\n",
      "8.323007820238114\n",
      "16.520870336009104\n",
      "30.566471903461576\n"
     ]
    }
   ],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in ['0.25-2.10 hr','2.10-6.45 hr', '6.45-19.25 hr', '19.25+ hr']:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long[\"DurationRange_personalised_allems\"] == duration]\n",
    "    print(this_dur['duration'].mean())\n",
    "    # this_dur = this_dur[this_dur['duration']>8]\n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], duration)\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], duration)\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b69ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats = pd.concat(each_dur_per_climate_changes)\n",
    "total_season_cats = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaaf3b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.03495474695613\n",
      "8.16475526948649\n",
      "2.6136350843900518\n"
     ]
    }
   ],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in ['12hr+', '4-12hr', '<4hr']:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long[\"DurationRange_simple\"] == duration]\n",
    "    print(this_dur['duration'].mean())\n",
    "    # this_dur = this_dur[this_dur['duration']>8]\n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], duration)\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], duration)\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459b02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats2 = pd.concat(each_dur_per_climate_changes)\n",
    "total_season_cats2 = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da2b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.022927006249958\n",
      "11.439348472846232\n",
      "3.980216158047675\n"
     ]
    }
   ],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in ['16hr+', '7-16hr', '<=7hr']:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long[\"duration_category_onemore\"] == duration]\n",
    "    print(this_dur['duration'].mean())\n",
    "    # this_dur = this_dur[this_dur['duration']>8]\n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], duration)\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], duration)\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee61a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats3 = pd.concat(each_dur_per_climate_changes)\n",
    "total_season_cats3 = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b20f6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total[total['Circular_Uniformity_future']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1076036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydur_new.csv\", index=False)\n",
    "total_season.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydur_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6803cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydurcategory_new.csv\", index=False)\n",
    "total_season_cats.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydurcategory_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eef5097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats2.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydurcategorysimple_new.csv\", index=False)\n",
    "total_season_cats2.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydurcategorysimple_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da0cf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cats3.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydurcategorysimple_new2.csv\", index=False)\n",
    "total_season_cats3.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydurcategorysimple_new2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8904609",
   "metadata": {},
   "source": [
    "### Save original data\n",
    "Don't do this higher up, because the json.dumps thing messes up the formatting for later stages of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "560d3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy and convert lists to JSON strings before saving - not doing this, messed up formatting a bit\n",
    "df_long['dur_for_which_this_is_amax'] = df_long['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics_new.csv\", index=False)\n",
    "df_long_with_short_durations_kept['dur_for_which_this_is_amax'] = df_long_with_short_durations_kept['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long_with_short_durations_kept.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics_shortdurationskept_new.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
