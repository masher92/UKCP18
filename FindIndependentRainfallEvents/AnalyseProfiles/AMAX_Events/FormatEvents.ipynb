{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ae4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def interpret_circular_shift(present_D_mean, future_D_mean):\n",
    "#     # Calculate the circular difference\n",
    "#     circular_diff = ((future_D_mean - present_D_mean + 182.625) % 365.25) - 182.625\n",
    "    \n",
    "#     # Define end-of-year and start-of-year thresholds\n",
    "#     end_of_year_threshold = 330\n",
    "#     start_of_year_threshold = 30\n",
    "    \n",
    "#     # Detect if we are straddling the December-January boundary\n",
    "#     if present_D_mean >= end_of_year_threshold and future_D_mean <= start_of_year_threshold:\n",
    "#         # Weâ€™re crossing from December to January\n",
    "#         interpretation = f\"The negative circular difference ({circular_diff:.2f} days) crosses December-January, meaning events are shifting later into the winter season.\"\n",
    "#     elif circular_diff > 0:\n",
    "#         interpretation = f\"Events are happening {circular_diff:.2f} days later in the future.\"\n",
    "#     else:\n",
    "#         interpretation = f\"Events are happening {-circular_diff:.2f} days earlier in the future.\"\n",
    "\n",
    "#     return circular_diff, interpretation\n",
    "\n",
    "# # Example usage\n",
    "# present_D_mean = 350  # Late December\n",
    "# future_D_mean = 15    # Early January\n",
    "\n",
    "# circular_diff, interpretation = interpret_circular_shift(present_D_mean, future_D_mean)\n",
    "# print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69a6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2c89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FormatEvents_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594c9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/nfs/a319/gy17m2a/PhD/'\n",
    "home_dir2 = '/nfs/a161/gy17m2a/PhD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254b686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = ['0.5', '1', '2', '3', '6', '12', '24']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6eb2c",
   "metadata": {},
   "source": [
    "### UKCP18 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c4c4f",
   "metadata": {},
   "source": [
    "### Join together lists for different ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa4bb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_props_dict_present = []\n",
    "ems_present = ['bc005', 'bc006', 'bc007', 'bc009', 'bc010', 'bc011', 'bc012', 'bc013', 'bc015', 'bc016', 'bc017', 'bc018']\n",
    "for em in ems_present:\n",
    "    with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Present/event_props_dict_{em}.pickle\", 'rb') as handle:\n",
    "        one_events_props_dict_present = pickle.load(handle)    \n",
    "    events_props_dict_present = events_props_dict_present + one_events_props_dict_present\n",
    "    \n",
    "## Join into one dataframe    \n",
    "present = pd.DataFrame(events_props_dict_present)\n",
    "present['Climate'] = 'Present'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec28fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_props_dict_future = []\n",
    "# ems_future = ['bb195', 'bb192', 'bb198', 'bb208', 'bb225','bb222', 'bb201', 'bb204', 'bb216', 'bb219', 'bb211']\n",
    "ems_future = ['bb195', 'bb198', 'bb192', 'bb208', 'bb225','bb222', 'bb201', 'bb204', 'bb216', 'bb219', 'bb211', 'bb189'] #bb195, #bb198\n",
    "for em in ems_future:\n",
    "    with open(home_dir +  f\"ProcessedData/AMAX_Events/UKCP18_30mins/Future/event_props_dict_{em}.pickle\", 'rb') as handle:\n",
    "        one_events_props_dict_future = pickle.load(handle)    \n",
    "    events_props_dict_future = events_props_dict_future + one_events_props_dict_future\n",
    "    \n",
    "## Join into one dataframe\n",
    "future = pd.DataFrame(events_props_dict_future)\n",
    "future['Climate'] = 'Future'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8203f5",
   "metadata": {},
   "source": [
    "## Make a check on number of files (could shift this to the checking script)\n",
    "NB - the method of searching on part1 doesnt work, because the filename only represents on of the files that is represented by that event\n",
    "\n",
    "\n",
    "24529 is 19 * 1291 and is the number we expert with no part1s for one ensemble member.  \n",
    "For 12 ems it becomes 24529 * 12 = 294348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ec2bb",
   "metadata": {},
   "source": [
    "### Create one dataframe containing both present and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.concat([present, future])\n",
    "\n",
    "# Add D variable (day of year) and date\n",
    "df_long['D'] = (df_long['theta'] * 365.25) / (2 * np.pi)\n",
    "df_long['date'] = df_long.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "df_long['season'] = df_long['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1aa28",
   "metadata": {},
   "source": [
    "### Check the number of files for each duration\n",
    "NB: Number of files for 24h duration is longer due to compound events  \n",
    "Checked this by filtering only rows with part0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13853b0",
   "metadata": {},
   "source": [
    "### Remove entries which are less than 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4fd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_with_short_durations_kept = df_long.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd33065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long[df_long['duration'] >=1.5]\n",
    "present = present[present['duration'] >=1.5]\n",
    "future = future[future['duration'] >=1.5]\n",
    "# nan_rows = df_long[df_long['D50'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4ce11",
   "metadata": {},
   "source": [
    "### NIMROD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2bc859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home_dir +  f\"ProcessedData/AMAX_Events/NIMROD_30mins/event_props_dict.pickle\", 'rb') as handle:\n",
    "    events_props_dict_nimrod = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f603319",
   "metadata": {},
   "outputs": [],
   "source": [
    "nimrod = pd.DataFrame(events_props_dict_nimrod)\n",
    "# Add D variable (day of year) and date\n",
    "nimrod['D'] = (nimrod['theta'] * 365.25) / (2 * np.pi)\n",
    "nimrod['date'] = nimrod.apply(lambda row: date_from_D(row['D'], row['year']), axis=1)\n",
    "nimrod['season'] = nimrod['date'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4102701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24     19462\n",
      "0.5    19363\n",
      "12     14560\n",
      "6      11379\n",
      "2       8136\n",
      "1       7536\n",
      "3       6830\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Flatten the lists and count occurrences of each number\n",
    "all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# Show the result\n",
    "print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27805f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24     19462\n",
      "0.5    19363\n",
      "12     14560\n",
      "6      11379\n",
      "2       8136\n",
      "1       7536\n",
      "3       6830\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure that values are treated as lists. If any single numbers are not in a list, convert them to lists.\n",
    "nimrod['dur_for_which_this_is_amax'] = nimrod['dur_for_which_this_is_amax'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Flatten the lists and count occurrences of each number\n",
    "all_numbers = [num for sublist in nimrod['dur_for_which_this_is_amax'] for num in sublist]\n",
    "number_counts = pd.Series(all_numbers).value_counts()\n",
    "\n",
    "# Show the result\n",
    "print(number_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c62805e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nimrod.to_csv(home_dir + f\"ProcessedData/AMAX_Events/NIMROD_30mins/all_events_characteristics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c3f2d",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (no duplicates for durations)\n",
    "### Group by gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bae88465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df = grouped_by_gauge_allevents\n",
    "# group_by_columns = ['Climate', 'gauge_num']\n",
    "# sampling_duration = 'All'\n",
    "\n",
    "# group_by_columns_no_climate = [col for col in group_by_columns if col != 'Climate']\n",
    "# present_df = grouped_df[grouped_df['Climate'] == 'Present'].copy().rename(columns=lambda x: x + '_present' if x not in group_by_columns_no_climate else x)\n",
    "# future_df = grouped_df[grouped_df['Climate'] == 'Future'].copy().rename(columns=lambda x: x + '_future' if x not in group_by_columns_no_climate else x)\n",
    "\n",
    "# # Merge present and future data on common columns\n",
    "# merged_df = pd.merge(present_df, future_df, on=group_by_columns_no_climate, how='outer', suffixes=('_present', '_future'))\n",
    "# for metric in ['D_mean', 'R', 'D50_mean', 'D50_median', 'D50_P90', 'D50_P10', 'F2_percentage', 'B2_percentage', 'C_percentage', 'F1_percentage', 'B1_percentage']:\n",
    "#     merged_df[f'{metric}_diff'] = merged_df[f'{metric}_future'] - merged_df[f'{metric}_present']\n",
    "\n",
    "    \n",
    "# # Step 1: Define a circular difference function\n",
    "# def circular_day_difference(present_day, future_day):\n",
    "#     # Compute the circular difference\n",
    "#     circular_diff = ((future_day - present_day + 182.625) % 365.25) - 182.625\n",
    "#     return circular_diff\n",
    "\n",
    "# # Step 2: Apply the function to the DataFrame columns and create a new column\n",
    "# merged_df['D_mean_diff_new'] = merged_df.apply(\n",
    "#     lambda row: circular_day_difference(row['D_mean_future'], row['D_mean_present']), axis=1)\n",
    "# merged_df[['D_mean_diff', 'D_mean_diff_new']]\n",
    "# merged_df['dff'] = merged_df['D_mean_diff'] - merged_df['D_mean_diff_new']\n",
    "# merged_df['D_mean_diff_new'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c4341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num']\n",
    "grouped_by_gauge_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_allevents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521afbb8",
   "metadata": {},
   "source": [
    "### Group by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04471ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_columns = ['Climate', 'gauge_num', 'season']\n",
    "grouped_by_gauge_season_allevents = group_data_calc_means(df_long, 'D50_new', group_by_columns)\n",
    "grouped_by_gauge_season_allevents_changes = find_change_values_in_groups_new(grouped_by_gauge_season_allevents, group_by_columns, 'All')\n",
    "grouped_by_gauge_season_allevents_changes.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_allevents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dff98b",
   "metadata": {},
   "source": [
    "# Create grouped results, for all events (for each duration separately)\n",
    "### Group by gauge, climate and by season, gauge, climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_dur_per_climate_changes = []\n",
    "each_dur_per_climate_and_season_changes = []\n",
    "\n",
    "# For each duration in turn\n",
    "for duration in durations:\n",
    "    # Get data for just this duration\n",
    "    this_dur = df_long[df_long['dur_for_which_this_is_amax'].apply(\n",
    "        lambda x: isinstance(x, list) and str(duration) in x or x == str(duration))]\n",
    "    \n",
    "    # Summary of events at each gauge, for this duration, one for present, one for future\n",
    "    summary_per_climate = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num'])\n",
    "    # Summary of events at each gauge, for this duration, one for each season for present, one for each season for future\n",
    "    summary_per_climate_and_season = group_data_calc_means(this_dur, 'D50_new', ['Climate', 'gauge_num', 'season'])\n",
    "    # Reformat, so one row per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_changes = find_change_values_in_groups_new(summary_per_climate, ['Climate', 'gauge_num'], float(duration))\n",
    "    # Reformat, so four rows (each season) per gauge, with change between present and future in the columns\n",
    "    summary_per_climate_season_changes = find_change_values_in_groups_new(summary_per_climate_and_season, ['Climate', 'gauge_num', 'season'], float(duration))\n",
    "    \n",
    "    ## Add to lists\n",
    "    each_dur_per_climate_changes.append(summary_per_climate_changes)\n",
    "    each_dur_per_climate_and_season_changes.append(summary_per_climate_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32890f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total[['D_mean_diff_new', \"D_mean_diff\", 'dff']].sort_values(by='dff')[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat(each_dur_per_climate_changes)\n",
    "total_season = pd.concat(each_dur_per_climate_and_season_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1076036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_changes_bydur.csv\", index=False)\n",
    "total_season.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/grouped_by_gauge_season_changes_bydur.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8904609",
   "metadata": {},
   "source": [
    "### Save original data\n",
    "Don't do this higher up, because the json.dumps thing messes up the formatting for later stages of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "560d3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy and convert lists to JSON strings before saving - not doing this, messed up formatting a bit\n",
    "df_long['dur_for_which_this_is_amax'] = df_long['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics.csv\", index=False)\n",
    "df_long_with_short_durations_kept['dur_for_which_this_is_amax'] = df_long_with_short_durations_kept['dur_for_which_this_is_amax'].apply(json.dumps)\n",
    "df_long_with_short_durations_kept.to_csv(home_dir + f\"ProcessedData/AMAX_Events/UKCP18_30mins/all_events_characteristics_shortdurationskept.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
